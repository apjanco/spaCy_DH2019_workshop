{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction to \n",
    "<img src=\"https://miro.medium.com/max/1200/1*HTtQseukwrBiREJf8MSVcA.jpeg\" alt=\"Spacy Logo\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "- [Main Documentation Page](https://spacy.io/)  \n",
    "- [How to install spaCy](https://spacy.io/usage)\n",
    "- [spaCy 101, The most important concepts, explained in simple terms\n",
    "](https://spacy.io/usage/spacy-101)\n",
    "- [Free course- Advanced NLP with spaCy](https://course.spacy.io/)\n",
    "\n",
    "### Without spaCy, Python is able to process text as a sequence of characters (called a string).  We can slice a string, we can add strings, replace sections of a string and many other tasks.  \n",
    "\n",
    "See [w3schools string functions](https://www.w3schools.com/python/python_ref_string.asp)\n",
    "\n",
    "Common examples for working with strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting a slice, selecting part of the string from [begin : end]\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "print('the string \"{}\" has {} characters. Note that the index begins at 0.'.format(wilde, len(wilde)))\n",
    "\n",
    "[print(i, charachter) for i, charachter in enumerate(wilde)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('wilde[4:12] will start at position 4 and end at 12 ->', wilde[4:12])\n",
    "print('or read backwards from the end [-40: -32] ->', wilde[-40:-32])\n",
    "print('you can even mix forward and backward! wilde[-40:12]', wilde[-40:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find and replace\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "wilde.replace('yourself', 'a fish').replace('everyone', 'everything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split \n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "print(wilde.split()) # Split on empty spaces\n",
    "print(wilde.split(';'))\n",
    "print(wilde.split('y'))  #Note that the charachter or space used to split the string is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can join a list of strings\n",
    "print(' '.join(['Be', 'yourself;', 'everyone', 'else', 'is', 'already', 'taken.']))\n",
    "\n",
    "import random\n",
    "\n",
    "animals = ['fish', 'turtle', 'panther', 'parrot']\n",
    "adjective = ['scary', 'green', 'overweight', 'fluffy']\n",
    "print('Be a ' + ' '.join([random.choice(adjective), random.choice(animals)]) + ' everything else is taken.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy gives the machine an understanding of text, not just as a sequence of characters, but as natural language\n",
    "\n",
    "[A full list of base languages](https://github.com/explosion/spaCy/tree/master/spacy/lang)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "\n",
    "nlp = German()\n",
    "doc = nlp('Sei du selbst! Alle anderen sind bereits vergeben.')\n",
    "\n",
    "\n",
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "doc = nlp('Be yourself; everyone else is already taken.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The document object\n",
    "Once we have imported a base language class or language model and a text, spaCy will create what is called a document (doc) object.  \n",
    "The doc object typically contains:\n",
    "\n",
    "\n",
    "|   [attributes](https://spacy.io/api/doc#attributes) |   | \n",
    "|---|---|\n",
    "| tokens (individual parts of the text)  | doc[5]  |\n",
    "|  the text  | doc.text\n",
    "| the text split into sentences  | doc.sents |\n",
    "| entities detected in the text | doc.ents |\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/doc).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('**Note the difference between working with a slice of a doc object versus a Python string**')\n",
    "\n",
    "print(wilde[:3])\n",
    "print(doc[:3])\n",
    "\n",
    "print('**Also note how spaCy tokenization differs from Python split()**')\n",
    "print('[*] Python:')\n",
    "for token in wilde.split():\n",
    "    print(token)\n",
    "    \n",
    "print('------')    \n",
    "print('[*] spaCy:')\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The to_json() method is a useful way to view the information contained in the doc object\n",
    "\n",
    "doc = nlp('''spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "If you’re working with a lot of text, you’ll eventually want to know more about it. For example, what’s it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
    "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning..''')\n",
    "\n",
    "doc.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Tokens\n",
    "As you can see above, the doc contains a split of the text into tokens.  Each token object has [65 attributes](https://spacy.io/api/token#attributes) that can be used during analysis.  Common tasks include:\n",
    "- removing all punctuation from the text\n",
    "- counting root forms of the words (lemmata)\n",
    "- removing stopwords from the doc\n",
    "\n",
    "\n",
    "|   [attributes](https://spacy.io/api/token#attributes) |   | \n",
    "|---|---|\n",
    "| root form (lemma)  | token.lemma_  |\n",
    "| Named entity type  | token.ent_type_ |\n",
    "| token is punctuation  | token.is_punct |\n",
    "| part of speech | token.pos_ |\n",
    "| in stop words | token.is_stop |\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/token#_title).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\n",
    "         token.lemma_,\n",
    "         token.pos_,\n",
    "         token.dep_,\n",
    "         token.shape_,\n",
    "         token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function to make sense of linguistic terminology and abbreviations \n",
    "import spacy \n",
    "\n",
    "spacy.explain(\"PRON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will print all of the attributes of a token.  Change the index to inspect difference tokens in the doc.\n",
    "\n",
    "token_index = 0\n",
    "\n",
    "import inspect\n",
    "import random\n",
    "\n",
    "attributes = inspect.getmembers(doc[token_index], lambda a:not(inspect.isroutine(a)))\n",
    "output = ''\n",
    "for attribute in attributes:\n",
    "    output += 'token.' + attribute[0]\n",
    "    output +=  '   ' + str(eval('doc[{}].{}'.format(token_index, attribute[0]))) + '\\n'\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Spans\n",
    "When studying text, we are often interested in features that involve more than one token.  To do this, we can create a span.  For example, \"New York City\"\n",
    "\n",
    "Span [attributes](https://spacy.io/api/span#attributes)\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/span#_title). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I just got back from New York City.'\n",
    "nlp = English()\n",
    "doc = nlp(text)\n",
    "\n",
    "nyc = doc[5:8] \n",
    "\n",
    "print(\n",
    "    '[*] spaCy',\n",
    "    nyc.start,\n",
    "    nyc.end,\n",
    "    doc[nyc.start:nyc.end],\n",
    ")\n",
    "print(  \n",
    "    '[*] string',\n",
    "    nyc.start_char,\n",
    "    nyc.end_char,\n",
    "    text[nyc.start_char:nyc.end_char]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models \n",
    "\n",
    "The core spaCy models are statistical language models that utilize Matt Honibal's Thinc computational library.  If you look under the hood, you'll find LSTMs, convolutional layers and an opinionated selection of state of the art methods in natural language processing. Further details on the model architecture can be found [here](https://spacy.io/models#architecture).\n",
    "\n",
    "Here is a listing of the officially supported spaCy models: https://spacy.io/models\n",
    "There are currently models for :\n",
    "- English\n",
    "- German\n",
    "- French\n",
    "- Spanish\n",
    "- Portuguese\n",
    "- Italian\n",
    "- Dutch\n",
    "- Greek\n",
    "- Multi-language\n",
    "\n",
    "The spaCy documentation lists the features and capabilities of each model.  Keep in mind that there can be several models for a language.  Larger models are often slower and require more memory. In exchange, the larger models are often more accurate and have more features such as word vectors, dependency parsing and other pipelines.   If you're not using the more advanced features of a large model, then you would probably be better off using something small.  As a general rule, it's best to start small and then deliberately move up as needed. \n",
    "\n",
    "\n",
    "To add a spaCy supported model, simply type: \n",
    "`python -m spacy download <name of model>` `en_core_web_sm` for example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#English base language object\n",
    "#nlp = English()\n",
    "\n",
    "#English small language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "doc = nlp('Be yourself; everyone else is already taken.')\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a growing community of spaCy users.  There are dozens of spaCy-based projects in the [Universe](https://spacy.io/universe) as well as user-created language models.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Named Entity Recognition\n",
    "Most of the models in spaCy have an entity recognizer.  This is similar to identifying parts of speech in the text, but greatly expands what we can automatically identify.  The types of entities and categories will vary from model to model and should be in the model's documentation. For most languages, the categories are: \n",
    "\n",
    "|   [named entities](https://spacy.io/api/annotation#ner-wikipedia-scheme) |   | \n",
    "|---|---|\n",
    "| PER  | Named person or family  |\n",
    "| ORG  | Named corporate, governmental, or other organizational entitity. |\n",
    "| LOC  | Name of politically or grographically defined location (cities, provinces, countries, international regions, bodies of water, mountains).  |\n",
    "| MISC | Miscellaneous entities, e.g. events, nationalities, products or works of art. |\n",
    "\n",
    "[Here is a list of the categories in the spaCy small English model](https://spacy.io/api/annotation#named-entities)\n",
    "\n",
    "[Here is a useful web application that can be used to assess the categories available in various spaCy models](https://explosion.ai/demos/displacy-ent)\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/usage/linguistic-features#named-entities-101).\n",
    "\n",
    "--- \n",
    "\n",
    "H.G. Wells, *The Invisible Man* (1897)\n",
    "<img src=\"https://www.slashfilm.com/wp/wp-content/images/invisible-man-cast-new.jpg\" alt=\"invisible man photo\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "invisible_man = requests.get('http://www.gutenberg.org/cache/epub/5230/pg5230.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(invisible_man.text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of people that appear in the text \n",
    "import pandas as pd\n",
    "doc = nlp(invisible_man.text)\n",
    "person_list = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        person_list.append(ent.text.replace('\\r','').replace('\\n',''))\n",
    "\n",
    "df = pd.DataFrame(set(person_list)) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of places that appear in the text \n",
    "import pandas as pd\n",
    "doc = nlp(invisible_man.text)\n",
    "place_list = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'GPE':\n",
    "        place_list.append(ent.text)\n",
    "\n",
    "df = pd.DataFrame(set(place_list)) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "displacy.render(next(doc.sents), style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source https://github.com/pmbaumgartner/binder-notebooks/blob/master/holy-nlp.ipynb \n",
    "\n",
    "actors_and_actions = []\n",
    "\n",
    "def token_is_subject_with_action(token):\n",
    "    nsubj = token.dep_ == 'nsubj'\n",
    "    head_verb = token.head.pos_ == 'VERB'\n",
    "    person = token.ent_type_ == 'PERSON'\n",
    "    return nsubj and head_verb and person\n",
    "\n",
    "for token in doc:\n",
    "    if token_is_subject_with_action(token):\n",
    "        span = doc[token.head.left_edge.i:token.head.right_edge.i+1]\n",
    "        data = dict(name=token.orth_,\n",
    "                    span=span.text,\n",
    "                    verb=token.head.lower_,\n",
    "                    log_prob=token.head.prob,\n",
    "                    )\n",
    "        actors_and_actions.append(data)\n",
    "\n",
    "print(len(actors_and_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "action_df = pd.DataFrame(actors_and_actions)\n",
    "\n",
    "print('Unique Names:', action_df['name'].nunique())\n",
    "\n",
    "most_common = (action_df\n",
    "    .groupby(['name', 'verb'])\n",
    "    .size()\n",
    "    .groupby(level=0, group_keys=False)\n",
    "    .nlargest(1)\n",
    "    .rename('Count')\n",
    "    .reset_index(level=0)\n",
    "    .rename(columns={\n",
    "        'verb': 'Most Common'\n",
    "    })\n",
    ")\n",
    "\n",
    "# exclude log prob < -20, those indicate absence in the model vocabulary\n",
    "most_unique = (action_df[action_df['log_prob'] > -20]\n",
    "    .groupby(['name', 'verb'])['log_prob']\n",
    "    .min()\n",
    "    .groupby(level=0, group_keys=False)\n",
    "    .nsmallest(1)\n",
    "    .rename('Log Prob.')\n",
    "    .reset_index(level = 0)\n",
    "    .rename(columns={\n",
    "        'verb': 'Most Unique'\n",
    "    })\n",
    ")\n",
    "\n",
    "# SO groupby credit\n",
    "# https: //stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common.sort_values('Count', ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Stanfordnlp models ![](https://pbs.twimg.com/profile_images/897182721272799232/0CplRl36_400x400.jpg)\n",
    "\n",
    "[Documentation](https://stanfordnlp.github.io/stanfordnlp/installation_usage.html)\n",
    "\n",
    "```\n",
    "$ pip install stanfordnlp spacy-stanfordnlp\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import stanfordnlp\n",
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "\n",
    "\n",
    "Using the default treebank \"en_ewt\" for language \"en\".\n",
    "Would you like to download the models for: en_ewt now? (Y/n)\n",
    "y\n",
    "\n",
    "Default download directory: /home/ajanco/stanfordnlp_resources\n",
    "Hit enter to continue or type an alternate directory.\n",
    "\n",
    "\n",
    "Downloading models for: en_ewt\n",
    "Download location: /home/ajanco/stanfordnlp_resources/en_ewt_models.zip\n",
    "100%|██████████| 235M/235M [00:51<00:00, 4.92MB/s] \n",
    "\n",
    "Download complete.  Models saved to: /home/ajanco/stanfordnlp_resources/en_ewt_models.zip\n",
    "Extracting models file for: en_ewt\n",
    "Cleaning up...Done.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "from spacy_stanfordnlp import StanfordNLPLanguage\n",
    "\n",
    "snlp = stanfordnlp.Pipeline(lang=\"en\")\n",
    "nlp = StanfordNLPLanguage(snlp)\n",
    "\n",
    "doc = nlp('Be yourself; everyone else is already taken.')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://spacy.io/architecture-bcdfffe5c0b9f221a2f6607f96ca0e4a.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language object\n",
    "from spacy.lang.es import Spanish\n",
    "nlp = Spanish()\n",
    "\n",
    "# Doc object\n",
    "doc = nlp(\"La duda es uno de los nombres de la inteligencia.\")\n",
    "\n",
    "# Tokens\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    \n",
    "# Spans\n",
    "span = doc[0:2]\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "# python -m spacy download es_core_news_sm\n",
    "\n",
    "# Models\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"La duda es uno de los nombres de la inteligencia.\")\n",
    "\n",
    "# Part of speech \n",
    "for token in doc:\n",
    "    print(token.pos_)\n",
    "    \n",
    "# Entities \n",
    "for token in doc.ents:\n",
    "    print(token.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ines Montani has created a great streamlit visualization.  Here is a link to the [gist](https://gist.github.com/ines/b320cb8441b590eedf19137599ce6685) or a running copy [here](http://165.22.207.232:8501/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy22",
   "language": "python",
   "name": "spacy22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
