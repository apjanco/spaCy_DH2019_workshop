{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HE0lY59nRRJ9"
   },
   "source": [
    "![](https://www.packetpower.com/hs-fs/hubfs/Blog/acronyms.png?width=700&name=acronyms.png) \n",
    "\n",
    "As a useful case-study, I was asked to train a classifier that distinguishes between acronyms and initialisms. \n",
    "\n",
    "Acronyms are \"made up of parts of the phrase it stands for and pronounced as a word.\" Initialisms are \"an acronym that is pronounced as individual letters.\"  \n",
    "\n",
    "NASA vs DNA.\n",
    "\n",
    "\n",
    "### Step 1) Data\n",
    "Wikipedia has a page with 6800 labeled acronyms [here](https://en.wikipedia.org/wiki/Lists_of_acronyms).  I scraped the data using [Web Scraper](https://www.webscraper.io/) and created a csv with cleaned acronyms, the original Wikipedia description, a label and the CMU pronunciation.  Only 250 of the acronyms were in the CMU dictionary, so I trained a model to automatically generate 950 of them based on [this post](https://www.ryanepp.com/blog/predicting-english-pronunciations) by Ryan Epp.  Only later did I see Ryan's [Big-Phoney](https://www.ryanepp.com/#Big-Phoney) library that accomplishes this task with pre-trained models.   \n",
    "\n",
    "(a) = acronym, e.g.: SARS – (a) severe acute respiratory syndrome \n",
    "\n",
    "(i) = initialism, e.g.: CD – (i) compact disc\n",
    "\n",
    "(p) = pseudo-blend, e.g.: UNIFEM – (p) United Nations Development Fund for Women\n",
    "\n",
    "(s) = symbol (none of the above, representing and pronounced as something else; for example: MHz – Megahertz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iu_oTh9Pl9fS",
    "outputId": "29de3e2b-0b12-44c5-dd10-9af0c58c61a6"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/apjanco/spaCy_workshops/master/acronym_data.csv\"\n",
    ")\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# create a list of the labeled acronyms and initialisms. Thank you Wikipedia!\n",
    "acronyms = []\n",
    "initialisms = []\n",
    "pseudo_blend = []\n",
    "symbols = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df[\"label\"][i] == \"a\":\n",
    "        acronyms.append(df[\"acronym\"][i])\n",
    "    if df[\"label\"][i] == \"i\":\n",
    "        initialisms.append(str(df[\"acronym\"][i]))\n",
    "    if df[\"label\"][i] == \"p\":\n",
    "        pseudo_blend.append(str(df[\"acronym\"][i]))\n",
    "    # if df['label'][i] == 's':\n",
    "    #    symbols.append(str(df['acronym'][i]))\n",
    "print(\n",
    "    \"a=\", len(acronyms), \"i=\", len(initialisms), \"p=\", len(pseudo_blend),\n",
    ")  #'s=',len(symbols) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's establish a simple baseline. If we just check that a word is capitalized and longer than three charachters, is that an acronym?\n",
    "def is_acronym(word: str):\n",
    "    score = 0\n",
    "    if word.isupper():\n",
    "        score += 1\n",
    "    if len(word) >= 3:\n",
    "        score += 1\n",
    "    # TODO acronym resembles an existing real word\n",
    "\n",
    "    if score == 2:\n",
    "        return True\n",
    "    else:\n",
    "        # print(word)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for acronym in acronyms:\n",
    "    if is_acronym(acronym):\n",
    "        count += 1\n",
    "print(count / len(acronyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad.  There are zero acronyms shorter than three characters and we match 95% of our gold data. We're only matching when the entire word is in capitals.  Try adding or changing the rules to improve this.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, these rules are just as good at finding initialisms.  But how to differentiate them?\n",
    "count = 0\n",
    "for initialism in initialisms:\n",
    "    if is_acronym(initialism):\n",
    "        count += 1\n",
    "print(count / len(initialisms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps if we look at pronunciation there are good features to distinguish the two?\n",
    "from big_phoney import BigPhoney\n",
    "\n",
    "phoney = BigPhoney()\n",
    "acronym_syl = []\n",
    "initialism_syl = []\n",
    "for acronym in acronyms:\n",
    "    acronym_syl.append(len(phoney.phonize(acronym)))\n",
    "for initialism in initialisms:\n",
    "    initialism_syl.append(len(phoney.phonize(initialism)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I tried number of syllables, and length of pronunciation.  They were no help in distinguishing initialisms from other acronyms.\n",
    "# What would you try? \n",
    "\n",
    "def is_acronym(word: str):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    \"https://raw.githubusercontent.com/apjanco/spaCy_workshops/master/acronym_data.csv\"\n",
    ")\n",
    "df = pd.read_csv(url)\n",
    "df = df[df.label != \"s\"]\n",
    "\n",
    "\n",
    "def is_correct(label, gold_label):\n",
    "\n",
    "    if label == \"INITIALISM\":\n",
    "        if gold_label == \"i\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    if label == \"ACRONYM\":\n",
    "        if gold_label == \"a\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    if label == \"PSEUDO\":\n",
    "        if gold_label == \"p\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "results = {\n",
    "    \"initialisms\": {\"correct\": 0, \"total\": 0},\n",
    "    \"acronyms\": {\"correct\": 0, \"total\": 0},\n",
    "    \"pseudo\": {\"correct\": 0, \"total\": 0},\n",
    "}\n",
    "\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        if is_acronym(df[\"acronym\"].iloc[i]):\n",
    "            if df[\"label\"].iloc[i] == \"a\":\n",
    "                results[\"acronyms\"][\"correct\"] += 1\n",
    "            results[\"acronyms\"][\"total\"] += 1\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "        # print(ent, ent.label_, df['label'].iloc[i], correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "results[\"acronyms\"][\"correct\"] / results[\"acronyms\"][\"total\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ws4kXyU764fD"
   },
   "source": [
    "## Yikes! While we're able to find acronyms, we're not discerning between acronyms and initialisms. \n",
    "\n",
    "##  To train a spaCy model, we'll need some training texts so that the model can learn to distinguish between a and i in context. \n",
    "I first tried this dataset: https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv\n",
    "```python\n",
    "url=\"https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/test.csv\"\n",
    "test_data = pd.read_csv(url, header=None,names=['category', 'headline', 'text'])\n",
    "```\n",
    "But I could not use it because it has no examples of acronym, only initialism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRmTjmcNTM1r"
   },
   "outputs": [],
   "source": [
    "# This is a much larger dataset, the Blogger Corpus, with 681,288 blog posts collated from blogger.com. Each blog contains at least 200 occurrences of frequently used English words.\n",
    "#!wget http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip\n",
    "#!unzip blogs.zip\n",
    "#import os\n",
    "#files = os.listdir(\"./blogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) Create text training data.  I need to identify sentences in this large corpus that contain acronyms.  \n",
    "\n",
    "When creating training data, you can use either the Matcher or the PhraseMatcher.  The matcher will use Token-level attributes to search and identify what you're looking for. You need to write the specific rules for the match.  The PhraseMatcher takes a Doc object as input.  Given that we're basically searching for strings in our list of acronyms, we could also search with Python, but spaCy is faster and gives us the ability to easily get the sentence where the acronym appears.     \n",
    "\n",
    "Further details on Matcher and PhraseMatcher can be found in Chapter 4 of the [spaCy course](https://course.spacy.io/chapter4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Rule-based Matcher Explorer](https://explosion.ai/demos/matcher)\n",
    "![](data:image/jpeg;base64,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)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZVjEVke_iBp"
   },
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher and initialize it\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "\n",
    "a_patterns = list(nlp.pipe(acronyms))\n",
    "matcher.add(\"ACRONYM\", None, *a_patterns)\n",
    "\n",
    "i_patterns = list(nlp.pipe(initialisms))\n",
    "matcher.add(\"INITIALISM\", None, *i_patterns)\n",
    "\n",
    "p_patterns = list(nlp.pipe(pseudo_blend))\n",
    "matcher.add(\"PSEUDO\", None, *p_patterns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this cell, we search for acronyms in the text data.  We create a list of sentences with acronyms in them.\n",
    "This takes about four hours, so I'll save you the time.  You can load the text as a pickle below. \n",
    "```python\n",
    "from tqdm import tqdm\n",
    "TEXTS = []\n",
    "nlp.max_length = 2000000\n",
    "for file in tqdm(files):\n",
    "    with open(''.join(['./blogs/', file]),'r') as f:\n",
    "        try:\n",
    "            doc = nlp(str(f.read()))\n",
    "            matches = matcher(doc)\n",
    "            for match in matches:\n",
    "                sentence = doc[match[1]].sent.text  #Note that this is a Span object, so we need to get the text as a string \n",
    "                TEXTS.append(sentence)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "100%|██████████| 19320/19320 [4:08:17<00:00,  3.12it/s]  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "TEXTS = pickle.load(open(\"TEXTS.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "srIHe8jgGDhG",
    "outputId": "74b1a448-7503-45db-97ff-669a48bfd04b"
   },
   "source": [
    "```python\n",
    "TRAINING_DATA = []\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "   \n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [(doc[start:end], match_id) for match_id, start, end in matcher(doc)]\n",
    "    \n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span[0].start_char, span[0].end_char, doc.vocab.strings[span[1]]) for span in spans]\n",
    "\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "```\n",
    "This one takes some time as well, so we'll skip it for the moment, but please feel free to try it if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "TRAINING_DATA = pickle.load(open(\"TRAINING_DATA.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1GlqaXLFIK4G",
    "outputId": "1550a797-c304-486c-97ce-ea4c967ad290"
   },
   "outputs": [],
   "source": [
    "# Display the number of example sentences per category\n",
    "initialism_examples = 0\n",
    "acronym_examples = 0\n",
    "pseudo_examples = 0\n",
    "for i in range(len(TRAINING_DATA)):\n",
    "    text = TRAINING_DATA[i][0]\n",
    "    entities = TRAINING_DATA[i][1][\"entities\"]\n",
    "    for start, stop, label in entities:\n",
    "        if label == \"INITIALISM\":\n",
    "            initialism_examples += 1\n",
    "        if label == \"ACRONYM\":\n",
    "            acronym_examples += 1\n",
    "        if label == \"PSEUDO\":\n",
    "            pseudo_examples += 1\n",
    "print(\n",
    "    \" acronyms: \",\n",
    "    acronym_examples,\n",
    "    \"\\n\",\n",
    "    \"initialisms: \",\n",
    "    initialism_examples,\n",
    "    \"\\n\",\n",
    "    \"pseudo: \",\n",
    "    pseudo_examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, with our training data in hand, we can start experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xB5ihrTktsvf"
   },
   "source": [
    "## Experiment No. 1  \n",
    "In this experiment, we are going to train a classifier using the acronyms and labels from Wikipedia and the news sentences with acronyms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1m1z0tEnmjCs",
    "outputId": "62c2c023-bea0-43f5-f22b-7868f075aa00"
   },
   "source": [
    "\n",
    "```python\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# Start the training\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"INITIALISM\")\n",
    "ner.add_label(\"ACRONYM\")\n",
    "ner.add_label('PSEUDO')\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "\n",
    "    nlp.begin_training()\n",
    "    # Loop for 10 iterations\n",
    "    for itn in tqdm(range(10)):\n",
    "        # Shuffle the training data\n",
    "        random.shuffle(TRAINING_DATA)\n",
    "        losses = {}\n",
    "        \n",
    "        # Batch the examples and iterate over them\n",
    "        for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "            texts = [text for text, entities in batch]\n",
    "            annotations = [entities for text, entities in batch]\n",
    "            \n",
    "            # Update the model\n",
    "            nlp.update(texts, annotations, losses=losses)\n",
    "            #print(losses)\n",
    "    nlp.to_disk(\"/en_acronym_initialism\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the amount of data you use, training can take some time. spaCy is designed for quick experimentation. Try with the least training and data. See how the model learns.  With transfer learning, you'll always be surprised how quickly the model learns.  I have saved the model [here](https://haverford.box.com/s/d38m6t9h6nk6m8r96kqj08rd3f3r05ai)\n",
    "You can download it and install it with `pip install <the_filename_and_path>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "LxfFA6mctvHF",
    "outputId": "c77867c7-8cb2-4b7c-d2b6-3b96f56c1dad"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy import displacy\n",
    "\n",
    "data = TRAINING_DATA[random.choice(range(len(TRAINING_DATA)))]\n",
    "text = data[0]\n",
    "label = data[1][\"entities\"]\n",
    "nlp = spacy.load(\"en_acronym_initialism\")\n",
    "doc = nlp(text)\n",
    "\n",
    "colors = {\n",
    "    \"ACRONYM\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "    \"INITIALISM\": \"linear-gradient(90deg,#fc9ce7,  #aa9cfc)\",\n",
    "    \"PSEUDO\": \"linear-gradient(90deg,#fc9ce7,  #aa9cfc)\",\n",
    "}\n",
    "options = {\"ents\": [\"ACRONYM\", \"INITIALISM\", \"PSEUDO\"], \"colors\": colors}\n",
    "\n",
    "print(label)\n",
    "displacy.render(doc, style=\"ent\", jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    \"https://raw.githubusercontent.com/apjanco/spaCy_workshops/master/acronym_data.csv\"\n",
    ")\n",
    "df = pd.read_csv(url)\n",
    "df = df[df.label != \"s\"]\n",
    "\n",
    "nlp = spacy.load(\"en_acronym_initialism\")\n",
    "\n",
    "\n",
    "def is_correct(label, gold_label):\n",
    "\n",
    "    if label == \"INITIALISM\":\n",
    "        if gold_label == \"i\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    if label == \"ACRONYM\":\n",
    "        if gold_label == \"a\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    if label == \"PSEUDO\":\n",
    "        if gold_label == \"p\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "results = {\n",
    "    \"initialisms\": {\"correct\": 0, \"total\": 0},\n",
    "    \"acronyms\": {\"correct\": 0, \"total\": 0},\n",
    "    \"pseudo\": {\"correct\": 0, \"total\": 0},\n",
    "}\n",
    "\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        doc = nlp(df[\"acronym\"].iloc[i])\n",
    "        for ent in doc.ents:\n",
    "\n",
    "            if df[\"label\"].iloc[i] == \"i\":\n",
    "                correct = is_correct(ent.label_, df[\"label\"].iloc[i])\n",
    "                if correct:\n",
    "                    results[\"initialisms\"][\"correct\"] += 1\n",
    "                results[\"initialisms\"][\"total\"] += 1\n",
    "\n",
    "            if df[\"label\"].iloc[i] == \"a\":\n",
    "                correct = is_correct(ent.label_, df[\"label\"].iloc[i])\n",
    "                if correct:\n",
    "                    results[\"acronyms\"][\"correct\"] += 1\n",
    "                results[\"acronyms\"][\"total\"] += 1\n",
    "\n",
    "            if df[\"label\"].iloc[i] == \"p\":\n",
    "                correct = is_correct(ent.label_, df[\"label\"].iloc[i])\n",
    "                if correct:\n",
    "                    results[\"pseudo\"][\"correct\"] += 1\n",
    "                results[\"pseudo\"][\"total\"] += 1\n",
    "    except TypeError:\n",
    "        print(ent)\n",
    "        # print(ent, ent.label_, df['label'].iloc[i], correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    print(result, results[result][\"correct\"] / results[result][\"total\"])\n",
    "    for result in results\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "acronyms.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
