{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path, makedirs, listdir\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from lxml import etree\n",
    "import standoffconverter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import bodleian_downloader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why not just regular expressions?\n",
    "\n",
    "- Match on `Doc` objects, not just strings\n",
    "- Match on tokens and token attributes\n",
    "- Use the model's predictions\n",
    "- Example: \"duck\" (verb) vs. \"duck\" (noun)\n",
    "\n",
    "Notes: Compared to regular expressions, the matcher works with Doc and Token\n",
    "objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical\n",
    "attributes.\n",
    "\n",
    "You can even write rules that use the model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun.  \n",
    "[(source Ines Montani)](https://raw.githubusercontent.com/ines/spacy-course/master/slides/chapter1_03_rule-based-matching.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match patterns\n",
    "\n",
    "- Lists of dictionaries, one per token\n",
    "\n",
    "- Match exact token texts\n",
    "\n",
    "```python\n",
    "[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "```\n",
    "\n",
    "- Match lexical attributes\n",
    "\n",
    "```python\n",
    "[{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "```\n",
    "\n",
    "- Match any token attributes\n",
    "\n",
    "```python\n",
    "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]\n",
    "```\n",
    "\n",
    "Notes: Match patterns are lists of dictionaries. Each dictionary describes one\n",
    "token. The keys are the names of token attributes, mapped to their expected\n",
    "values.\n",
    "\n",
    "In this example, we're looking for two tokens with the text \"iPhone\" and \"X\".\n",
    "\n",
    "We can also match on other token attributes. Here, we're looking for two tokens\n",
    "whose lowercase forms equal \"iphone\" and \"x\".\n",
    "\n",
    "We can even write patterns using attributes predicted by the model. Here, we're\n",
    "matching a token with the lemma \"buy\", plus a noun. The lemma is the base form,\n",
    "so this pattern would match phrases like \"buying milk\" or \"bought flowers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"New iPhone X release date leaked.\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Rule-based Matcher Explorer](https://explosion.ai/demos/matcher)\n",
    "![](data:image/jpeg;base64,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)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically create a list of ministries in George Orwell's *Nineteen eighty-four*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "_1984 = requests.get('http://gutenberg.net.au/ebooks01/0100021.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Ministry\"},\n",
    "           {\"LOWER\": \"of\"},\n",
    "           {\"IS_ALPHA\": True},\n",
    "            ]\n",
    "matcher.add(\"ministryof\", None, pattern)\n",
    "\n",
    "doc = nlp(_1984.text)\n",
    "\n",
    "matches = matcher(doc)\n",
    "ministries = []\n",
    "for match_id, start, end in matches:\n",
    "    if str(doc[start:end]) not in ministries:\n",
    "        ministries.append(str(doc[start:end]))\n",
    "ministries    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Matcher for stage directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download Shakespeare plays that have annotations for stage directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standoffs = []\n",
    "\n",
    "for fn in tqdm(bodleian_downloader.get_file_descriptors(), desc=\"download and extract xml\"):\n",
    "\n",
    "    tree = etree.fromstring(open(fn, \"rb\").read())\n",
    "\n",
    "    so = standoffconverter.Converter.from_tree(tree)\n",
    "\n",
    "    standoffs.append(so)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(standoffs, random_state=4123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_true_stage_directions(standoffs):\n",
    "    stage_directions = []\n",
    "    indices = []\n",
    "    for standoff in standoffs:\n",
    "        indices.append([])\n",
    "        for annotation in standoff.collection:\n",
    "\n",
    "            if annotation.get_tag() == \"{http://www.tei-c.org/ns/1.0}stage\":\n",
    "                \n",
    "                anno_begin = annotation.get_begin()\n",
    "                anno_end = annotation.get_end()\n",
    "\n",
    "                stage_directions.append(standoff.plain[anno_begin:anno_end])\n",
    "                indices[-1].append((anno_begin,anno_end))\n",
    "    return stage_directions, indices\n",
    "                \n",
    "train_sd,_ = extract_true_stage_directions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating some example matchers and validate\n",
    "The validation function also shows you examples of fales matches (false negatives and false positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "pattern = [{'ORTH': 'Enter'},\n",
    "           {'IS_PUNCT': True}]\n",
    "\n",
    "matcher.add('ENTER', None, pattern)\n",
    "\n",
    "\n",
    "pattern = [{'ORTH': 'Enter'},\n",
    "           {'POS': 'PROPN'},\n",
    "           {'IS_PUNCT': True}]\n",
    "\n",
    "matcher.add('ENTER_SOMEONE', None, pattern)\n",
    "\n",
    "pattern = [{'ORTH': 'Exit'},\n",
    "           {'IS_PUNCT': True}]\n",
    "\n",
    "matcher.add('EXIT', None, pattern)\n",
    "\n",
    "pattern = [{'ORTH': 'Exit'},\n",
    "           {'POS': 'PROPN'},\n",
    "           {'IS_PUNCT': True}]\n",
    "\n",
    "matcher.add('EXIT_SOMEONE', None, pattern)\n",
    "\n",
    "\n",
    "def validate_stage_directions(matcher, data):\n",
    "\n",
    "    docs = [nlp(doc.plain) for doc in data]\n",
    "    \n",
    "    data_sd, data_indices = extract_true_stage_directions(data)\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    fn_examples = []\n",
    "    fp_examples = []\n",
    "    \n",
    "    tp_match_names = []\n",
    "    fp_match_names = []\n",
    "    \n",
    "    \n",
    "    for idoc, doc in tqdm(enumerate(docs), total=len(docs)):\n",
    "        \n",
    "        ground_truth = np.zeros(len(doc.text)).astype(bool)\n",
    "        prediction = np.zeros(len(doc.text)).astype(bool)\n",
    "        for begin, end in data_indices[idoc]:\n",
    "            ground_truth[begin:end] = True\n",
    "            \n",
    "        \n",
    "        matches = matcher(doc)\n",
    "        for match_id, begin, end in matches:\n",
    "            last_ind = max(end-1, begin)\n",
    "            char_begin = doc[begin].idx\n",
    "            char_end = doc[last_ind].idx + len(doc[last_ind])\n",
    "\n",
    "            prediction[char_begin:char_end] = True\n",
    "\n",
    "                \n",
    "            if ground_truth[char_begin:char_end].sum() < len(ground_truth[char_begin:char_end]):\n",
    "                fp_examples.append({\n",
    "                    \"phrase\": doc.text[char_begin:char_end],\n",
    "                    \"phrase_with_context\":doc.text[max(0, char_begin-25):min(len(doc.text),char_end+25)]\n",
    "                })\n",
    "                fp_match_names.append(\n",
    "                    nlp.vocab.strings[match_id]\n",
    "                )\n",
    "            else:\n",
    "                tp_match_names.append(\n",
    "                    nlp.vocab.strings[match_id]\n",
    "                )\n",
    "\n",
    "        for begin, end in data_indices[idoc]:\n",
    "            if prediction[begin:end].sum() == 0:#< len(prediction[begin:end]):\n",
    "                fn_examples.append({\n",
    "                    \"phrase\": doc.text[begin:end],\n",
    "                    \"phrase_with_context\":doc.text[max(0,begin-25):min(len(doc.text), end + 25)]\n",
    "                })\n",
    "\n",
    "        tp += np.logical_and(ground_truth, prediction).sum()\n",
    "        fn += np.logical_and(ground_truth, ~prediction).sum()\n",
    "        fp += np.logical_and(~ground_truth, prediction).sum()\n",
    "        tn += np.logical_and(~ground_truth, ~prediction).sum()\n",
    "\n",
    "    precision = tp /(tp + fp) if fp > 0 or tp > 0 else 0\n",
    "    recall = tp /(tp + fn) if tp > 0 or fn > 0 else 0\n",
    "    f1_score = 2*tp / (2*tp + fp + fn) if tp > 0 or fp > 0 or fn > 0 else 0\n",
    "    print(\"precision: {:.2f}, recall: {:.2f}, f1 {:.2f}\".format(precision, recall, f1_score))\n",
    "    \n",
    "    printmd(\"\\n **What has been identified. although it is not a stage direction? For example:**\")\n",
    "    if len(fp_examples) > 0:\n",
    "        for ex_dict in np.random.choice(fp_examples, 3):\n",
    "            for k,v in ex_dict.items():\n",
    "                printmd(\"*\" + k + \"*\")\n",
    "                print(\">\", \"\\\"\"+v+\"\\\"\")\n",
    "                \n",
    "    else:\n",
    "        print(\"0 examples.\")\n",
    "    printmd(\"**What has not been identified? For example:**\")\n",
    "    if len(fn_examples) > 0:\n",
    "        for ex_dict in np.random.choice(fn_examples, 3):\n",
    "            for k,v in ex_dict.items():\n",
    "                printmd(\"*\" + k + \"*\")\n",
    "                print(\">\", \"\\\"\"+v+\"\\\"\")\n",
    "    else:\n",
    "        print(\"0 examples.\")\n",
    "\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,6))\n",
    "    fig.suptitle('how often did a pattern apply?')\n",
    "    \n",
    "    keys, counts = np.unique(tp_match_names, return_counts=True)\n",
    "    ax1.bar(keys,counts)\n",
    "    ax1.set_title(\"correctly applied pattern\")\n",
    "    \n",
    "    keys, counts = np.unique(fp_match_names, return_counts=True)\n",
    "    ax2.bar(keys, counts)\n",
    "    ax2.set_title(\"incorrectly applied pattern\")\n",
    "    \n",
    "    \n",
    "validate_stage_directions(matcher, train[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_workshops_test",
   "language": "python",
   "name": "spacy_workshops_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
