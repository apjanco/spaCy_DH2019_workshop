{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](top.png)\n",
    "\n",
    "---\n",
    "\n",
    "[Prodigy Documentation](https://spacy.apjan.co/docs/)\n",
    "\n",
    "Basic NER with pre-trained spaCy models ([source](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The COST Action Distant Reading for European Literary History is issuing a Call for Applications for its third Training School, hosted by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the Centre for Digital Humanities – Eötvös Loránd University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Budapest\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    September 23 to 25\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", and co-located with the DH_Budapest 2019 conference \n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "import pickle\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "spec = {\"tei\":\"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "doc = nlp(\n",
    "    \"\"\"The COST Action Distant Reading for European Literary History is issuing a Call for Applications for its third Training School, hosted by the Centre for Digital Humanities – Eötvös Loránd University in Budapest from September 23 to 25, and co-located with the DH_Budapest 2019 conference \n",
    "\"\"\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here is a [simple web app](https://explosion.ai/demos/displacy-ent) that you can use to quickly evaluate how useful an existing model would be for your current text or project.  For many languages, there are several possible [models](https://spacy.io/usage/models#languages).  This tool can be used to evaluate the results from different models.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "*This works very well for many 20th and 21st century texts.  But what about early modern English?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ITEM because that the kings most deare \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Uncle\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", the king of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Denmarke\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Norway &amp; Sweveland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold &amp; great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Realme of England\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", by ye going in, entring &amp; passage of such forain &amp; strange persons into his realme of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Norwey &amp;\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " other dominions, streits, territories, jurisdictions &amp; places subdued and subject to him, specially into his \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Iles of Fynmarke\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and elswhere, aswell in their persons as their things and goods</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"ITEM because that the kings most deare Uncle, the king of Denmarke, Norway & Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring & passage of such forain & strange persons into his realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ✨Prodigy\n",
    "\n",
    "Prodigy is an annotation tool for active machine teaching.  Prodigy makes it possible to quickly experiment and improve models with relatively little data, time and typically only one annotator.  Prodigy will sort a model's results by certainty, and ask for human input where the results are most ambiguous. Prodigy stores these annotations and uses them to update the model.  Machine teaching can be used to: \n",
    "\n",
    "- improve an existing model for a specific context or set of documents\n",
    "- add a new entity or category to a text model \n",
    "- custom image categorization and object recognition\n",
    "\n",
    "*While spaCy is a free and open-source library, Prodigy is a commercial product that requires a license. Researchers at degree-granting academic institutions can obtain a free research license [here](https://prodi.gy/forms/research-license).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this example, our goal is to teach an existing English-language model to identify early modern place names.\n",
    "\n",
    "There are several approaches that we could take to this problem.  Different approaches can lend better or worse results and experimentation is an essential part of any machine learning project. \n",
    "\n",
    "### How can we teach a statistical language model that Sweveland is a place?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual annotation**\n",
    "\n",
    "Some researchers may prefer to add seed annotations using the `ner.manual` recipe. For certain personalities, myself included, this is actually kind of fun and a good way to think about the text and the goals of your experiment.  We'll discuss how Prodigy can be used as a manual annotation tool near the end of this session.\n",
    "\n",
    "If you only have a few seed terms you can also add them as an argument with `ner.teach en_core_web_lg ... --seeds \"digital,humanities,utrecht,netherlands,spacy,workshop\"`[(source)](https://support.prodi.gy/t/train-a-new-ner-entity-with-multi-word-tokens/227/3) \n",
    "\n",
    "In our current case, we're going to provide as many examples of place names as possible.  Note that with spaCy, we are working with place names as [patterns](https://spacy.apjan.co/docs/#match-patterns) so the model can learn that place names often have several parts.  \n",
    "\n",
    "**Seed term patterns** \n",
    "\n",
    "We're going to give the model as many examples of historic place names as we can to get the learning process started. To do this, I have chosen to mine a text that has already been markedup in TEI with early modern place names and is available from the Perseus Project.  We're going to use Richard Hakluyt's [*The Principal Navigations, Voyages, Traffiques, and Discoveries of the English Nation* (1599)](http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D1).  I have chosen an English-language example because we'll be annotating the text in this workshop and English is a working language of DH2019.  However, you can work with texts in [any language](https://spacy.io/usage/adding-languages) even those that do not have an existing spaCy language model ([colonial Zapotec](https://ticha.haverford.edu/) for example!).  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**New category or existing?** \n",
    "\n",
    "We will need to choose whether we want to add a new entity to the model (let's call it \"PLACE\") or improve the existing \"GPE\" (Geopolitical entity, i.e. countries, cities, states) for our historic places.  Either one works.  If we improve an existing entity, we retain the existing training and examples. A new entity makes it easier to distinguish between what we have taught the model and previous training.  This can also address potential bias in previous training data.  For that reason, we'll add \"PLACE\" as a new category.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the TEI files from Persius \n",
    "- We're going to extract a list of all the place names from the text to create a patterns JSONL file.\n",
    "- We'll also extract the raw text to create a set of training documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to download the table of contents and create a list of the 937 segments of the document. We will then get each page, remove the place names (`<name type=\"place\">Utrect</name>`) and add them to a places list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickles loaded\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "\n",
    "def tei_loader(url):\n",
    "    tei = urlopen(url).read()\n",
    "    return etree.XML(tei)\n",
    "\n",
    "table_of_contents_url = \"http://www.perseus.tufts.edu/hopper/xmltoc?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D1\"\n",
    "table_of_contents_xml = tei_loader(table_of_contents_url)\n",
    "\n",
    "if not os.path.exists('refs.pickle'):\n",
    "    chunks = table_of_contents_xml.xpath(\"//chunk[@ref]\")\n",
    "    refs = [chunk.get('ref') for chunk in chunks] \n",
    "    # an example ref 'Perseus%3Atext%3A1999.03.0070%3Anarrative%3D6'\n",
    "\n",
    "\n",
    "    places = []\n",
    "\n",
    "    for ref in refs:\n",
    "\n",
    "        url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + ref\n",
    "        try:\n",
    "            tei = tei_loader(url)\n",
    "\n",
    "            #get all <name type='place'> tags\n",
    "            for place in tei.findall(\".//name[@type='place']\", namespaces=spec):\n",
    "                places.append(place.text.replace('\\n',''))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "    pickle.dump(places, open('places.pickle', 'wb'))\n",
    "    pickle.dump(refs, open('refs.pickle', 'wb'))\n",
    "\n",
    "else:\n",
    "    places = pickle.load(open('places.pickle', 'rb'))\n",
    "    refs = pickle.load(open('refs.pickle', 'rb'))\n",
    "    print('pickles loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents:  937\n",
      "number of places found:  2279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'York'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('number of documents: ', len(refs))\n",
    "print('number of places found: ', len(set(places)))\n",
    "places[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a patterns.jsonl file with seed terms.  \n",
    "These terms provide examples that the model can use to learn the new category. It is good to use as many terms as are practical; preferably several hundred. Further examples of patterns files can be found [here](https://github.com/explosion/prodigy-recipes/tree/master/example-patterns). There is a very helpful tool for creating patterns that are relevant to your projects and texts [here](https://explosion.ai/demos/matcher).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "new_label = 'PLACE'\n",
    "\n",
    "if not os.path.exists('patterns.jsonl'):\n",
    "    with open('patterns.jsonl','w') as f:\n",
    "        for place in set(places):   # A set is used here to remove duplicate place names\n",
    "            \n",
    "            row = {}\n",
    "                \n",
    "            row['label'] = new_label\n",
    "            row['pattern'] = []\n",
    "            for token in place.split():\n",
    "                pattern = {}\n",
    "                pattern['lower'] = token.lower()\n",
    "                row['pattern'].append(pattern)\n",
    "        \n",
    "            f.write(str(json.dumps(row) +'\\n')) \n",
    "            \n",
    "        '''Polite intervention:  \n",
    "        Sweveland is not in our list of historic and beautiful places, but is essential to the narrative of this notebook.\n",
    "        To correct this error, the line below has been added, with appologies to our colleagues from Sweveland'''\n",
    "        row = {\"label\": \"PLACE\", \"pattern\": [{\"lower\": \"sweveland\"}]}\n",
    "        f.write(json.dumps(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"label\": \"PLACE\", \"pattern\": [{\"lower\": \"wardhouse\"}]}\n",
      "{\"label\": \"PLACE\", \"pattern\": [{\"lower\": \"sea\"}, {\"lower\": \"southwest\"}]}\n",
      "{\"label\": \"PLACE\", \"pattern\": [{\"lower\": \"silauria\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('patterns.jsonl','r') as f:\n",
    "    print(f.read()[:185])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file already exists. This process takes several minutes. To re-run, change process_anew to True. You can also adjust the percentage of the corpus to be gathered.\n"
     ]
    }
   ],
   "source": [
    "#Now to extract the full text\n",
    "percent_of_texts_to_process = 0.05 \n",
    "'''We are only using a tiny portion of the text for annotation.  \n",
    "Unlike many machine learning tasks, in this case, more text data for annotations does not necessarily\n",
    "improve learning. More text will dramatically increase Prodigy's memory usage, so keep that in mind.'''\n",
    "\n",
    "end_index = int(len(refs) * percent_of_texts_to_process)\n",
    "\n",
    "if os.path.exists(\"principal_navigations.txt\"):\n",
    "    print(\n",
    "        \"The file already exists. This process takes several minutes. To re-run, change process_anew to True. You can also adjust the percentage of the corpus to be gathered.\"\n",
    "    )\n",
    "    \n",
    "process_anew = False\n",
    "if process_anew:\n",
    "    txts = []\n",
    "    for ref in refs[:end_index]:\n",
    "\n",
    "        url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + ref\n",
    "        try:\n",
    "            tei = tei_loader(url)\n",
    "\n",
    "            new_txt = []\n",
    "            for body in tei.iter('body'):\n",
    "                new_txt.append(''.join(body.itertext()).strip().replace('\\n',''))\n",
    "                txts.append(''.join(new_txt))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "\n",
    "    full_text = [\n",
    "        txt.replace(\"        \", \" \").replace(\"   \", \" \").replace(\"  \", \" \")\n",
    "        for txt in txts\n",
    "    ]\n",
    "    \n",
    "    with open('principal_navigations.txt','w') as f:\n",
    "        f.write(str(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A branch of a Statute made in the eight yeere of Henry the sixt, for the trade to Norwey, Sweveland, Den marke, and Fynmarke. ITEM because that the kings most deare Uncle, the kingof Denmarke, Norway & Sweveland, as the same oursoveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils,hurts and damage which have late happened aswell tohim and his, as to other foraines and strangers, and alsofriends and speciall subjects of our said soveraigne Lordthe king of his Realme of England, by ye going in,entring & passage of such forain & strange pers\n"
     ]
    }
   ],
   "source": [
    "with open('principal_navigations.txt','r') as f: \n",
    "    print(f.read()[:600])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With patterns and text files created, we can now work with ✨Prodigy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dataset` command will create a database table to save your annotations. The default is sqlite, but you can connect to [MySQL or postgres](https://spacy.apjan.co/docs/#database-setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  ✨  Successfully added 'DH-Budapest' to database SQLite.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!prodigy dataset DH-Budapest \"A dataset for British historic places\" --author Andy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to delete a dataset:   \n",
    "\n",
    "`prodigy drop historic_places`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Plain text to TEI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the annotation application for a named entity recognition task, we use the `ner.teach` recipe. Similar [built-in recipes](https://spacy.apjan.co/docs/#built-in-recipes) are available for: \n",
    "- text categorization (textcat.teach) \n",
    "- part of speech tagging (pos.teach)  \n",
    "- vectors & terminology (terms.teach)\n",
    "- computer vision (image.manual)\n",
    "\n",
    "In the command below, we use the `ner.teach` recipe to annotate the `historic_places` dataset, using the `en_core_web_sm` model to add the new entity `PLACE` after loading the patterns in `patterns.jsonl`.  \n",
    "\n",
    "*Click on the stop button to intterupt the kernel when you're done.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 labels: PLACE\n",
      "\n",
      "  ✨  Starting the web server at http://localhost:8080 ...\n",
      "  Open the app in your browser and start annotating!\n",
      "\n",
      "Task queue depth is 1\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.teach DH-Budapest en_core_web_sm principal_navigations.txt --label PLACE --patterns patterns.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed adding annotations, the next step is to train the model.  \n",
    "\n",
    "In the command below, we use `ner.batch-train` to use the annotations in the `historic_places` dataset to train the `en_core_web_sm` model on the new entity `PLACE`.  We then save the updated model as `new_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!prodigy ner.batch-train DH-Budapest en_core_web_sm --label PLACE --output new_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think the model would benefit from more training you can run this process again to load and update `new_model`.  You can also add the `--n-iter` argument to specify the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 labels: PLACE\n",
      "\n",
      "Loaded model new_model\n",
      "Using 50% of accept/reject examples (14) for evaluation\n",
      "Using 100% of remaining examples (14) for training\n",
      "Dropout: 0.2  Batch size: 4  Iterations: 120  \n",
      "\n",
      "\n",
      "BEFORE      0.632           \n",
      "Correct     \u001b[38;5;77m12\u001b[0m\n",
      "Incorrect   \u001b[38;5;197m7\u001b[0m\n",
      "Entities    25              \n",
      "Unknown     12              \n",
      "\n",
      "#            LOSS         RIGHT        WRONG        ENTS         SKIP         ACCURACY  \n",
      "01           7.571        11           8            24           0            0.579     \n",
      "02           12.906       12           7            24           0            0.632     \n",
      "03           45.039       11           8            20           0            0.579     \n",
      "04           12.333       11           8            18           0            0.579     \n",
      "05           26.525       11           8            17           0            0.579     \n",
      "06           17.856       11           8            17           0            0.579     \n",
      "07           30.938       10           9            16           0            0.526     \n",
      "08           25.859       10           9            16           0            0.526     \n",
      "09           19.062       10           9            16           0            0.526     \n",
      "10           22.487       10           9            18           0            0.526     \n",
      "11           9.137        10           9            18           0            0.526     \n",
      "12           10.050       11           8            18           0            0.579     \n",
      "13           30.444       11           8            18           0            0.579     \n",
      "14           27.817       11           8            21           0            0.579     \n",
      "15           16.327       12           7            25           0            0.632     \n",
      "16           20.787       11           8            25           0            0.579     \n",
      "17           31.314       11           8            25           0            0.579     \n",
      "18           31.440       11           8            25           0            0.579     \n",
      "19           12.438       11           8            25           0            0.579     \n",
      "20           6.500        11           8            24           0            0.579     \n",
      "21           14.384       11           8            24           0            0.579     \n",
      "22           18.471       12           7            24           0            0.632     \n",
      "23           18.497       12           7            21           0            0.632     \n",
      "24           39.410       12           7            22           0            0.632     \n",
      "25           17.939       11           8            18           0            0.579     \n",
      "26           19.705       11           8            18           0            0.579     \n",
      "27           19.592       11           8            18           0            0.579     \n",
      "28           19.016       11           8            18           0            0.579     \n",
      "29           27.509       11           8            19           0            0.579     \n",
      "30           55.029       11           8            19           0            0.579     \n",
      "31           22.109       11           8            19           0            0.579     \n",
      "32           14.016       11           8            21           0            0.579     \n",
      "33           6.441        11           8            21           0            0.579     \n",
      "34           10.300       11           8            21           0            0.579     \n",
      "35           21.649       11           8            21           0            0.579     \n",
      "36           10.063       11           8            21           0            0.579     \n",
      "37           13.463       11           8            21           0            0.579     \n",
      "38           42.019       11           8            22           0            0.579     \n",
      "39           36.800       11           8            22           0            0.579     \n",
      "40           49.062       11           8            23           0            0.579     \n",
      "41           18.259       11           8            23           0            0.579     \n",
      "42           34.124       11           8            23           0            0.579     \n",
      "43           14.814       11           8            23           0            0.579     \n",
      "44           8.846        11           8            23           0            0.579     \n",
      "45           23.063       11           8            23           0            0.579     \n",
      "46           6.234        11           8            23           0            0.579     \n",
      "47           6.611        11           8            22           0            0.579     \n",
      "48           24.640       11           8            22           0            0.579     \n",
      "49           10.848       11           8            22           0            0.579     \n",
      "50           12.641       11           8            22           0            0.579     \n",
      "51           45.167       11           8            22           0            0.579     \n",
      "52           18.907       11           8            20           0            0.579     \n",
      "53           10.907       11           8            20           0            0.579     \n",
      "54           34.280       11           8            21           0            0.579     \n",
      "55           29.497       11           8            21           0            0.579     \n",
      "56           28.909       11           8            19           0            0.579     \n",
      "57           33.118       11           8            18           0            0.579     \n",
      "58           23.309       11           8            17           0            0.579     \n",
      "59           25.881       11           8            17           0            0.579     \n",
      "60           21.627       11           8            17           0            0.579     \n",
      "61           25.497       11           8            17           0            0.579     \n",
      "62           31.739       11           8            17           0            0.579     \n",
      "63           22.408       11           8            20           0            0.579     \n",
      "64           8.971        11           8            20           0            0.579     \n",
      "65           29.357       11           8            23           0            0.579     \n",
      "66           16.715       11           8            23           0            0.579     \n",
      "67           11.402       11           8            23           0            0.579     \n",
      "68           2.415        11           8            24           0            0.579     \n",
      "69           32.811       11           8            24           0            0.579     \n",
      "70           16.012       11           8            25           0            0.579     \n",
      "71           25.249       11           8            24           0            0.579     \n",
      "72           30.524       11           8            24           0            0.579     \n",
      "73           15.527       11           8            25           0            0.579     \n",
      "74           24.695       11           8            25           0            0.579     \n",
      "75           17.432       11           8            25           0            0.579     \n",
      "76           22.225       11           8            25           0            0.579     \n",
      "77           15.941       11           8            25           0            0.579     \n",
      "78           13.535       11           8            25           0            0.579     \n",
      "79           9.463        11           8            25           0            0.579     \n",
      "80           10.845       11           8            25           0            0.579     \n",
      "81           30.388       11           8            25           0            0.579     \n",
      "82           24.887       11           8            25           0            0.579     \n",
      "83           5.189        11           8            25           0            0.579     \n",
      "84           29.318       11           8            24           0            0.579     \n",
      "85           24.361       11           8            23           0            0.579     \n",
      "86           12.756       11           8            23           0            0.579     \n",
      "87           34.023       12           7            24           0            0.632     \n",
      "88           5.257        12           7            25           0            0.632     \n",
      "89           19.103       12           7            27           0            0.632     \n",
      "90           22.024       12           7            27           0            0.632     \n",
      "91           15.698       12           7            29           0            0.632     \n",
      "92           31.820       12           7            27           0            0.632     \n",
      "93           30.587       12           7            27           0            0.632     \n",
      "94           6.518        12           7            27           0            0.632     \n",
      "95           17.919       12           7            25           0            0.632     \n",
      "96           10.843       12           7            25           0            0.632     \n",
      "97           10.326       12           7            25           0            0.632     \n",
      "98           15.386       12           7            26           0            0.632     \n",
      "99           7.045        12           7            26           0            0.632     \n",
      "100          17.442       12           7            25           0            0.632     \n",
      "101          24.307       12           7            25           0            0.632     \n",
      "102          6.664        12           7            25           0            0.632     \n",
      "^C                                                                              \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/prodigy/__main__.py\", line 380, in <module>\n",
      "    controller = recipe(*args, use_plac=True)\n",
      "  File \"cython_src/prodigy/core.pyx\", line 212, in prodigy.core.recipe.recipe_decorator.recipe_proxy\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/plac_core.py\", line 328, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/plac_core.py\", line 207, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/prodigy/recipes/ner.py\", line 623, in batch_train\n",
      "    stats = model.evaluate(evals)\n",
      "  File \"cython_src/prodigy/models/ner.pyx\", line 460, in prodigy.models.ner.EntityRecognizer.evaluate\n",
      "  File \"cython_src/prodigy/models/ner.pyx\", line 462, in prodigy.models.ner.EntityRecognizer.evaluate\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/spacy/language.py\", line 688, in pipe\n",
      "    for doc, context in izip(docs, contexts):\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/spacy/language.py\", line 716, in pipe\n",
      "    for doc in docs:\n",
      "  File \"nn_parser.pyx\", line 221, in pipe\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/spacy/util.py\", line 457, in minibatch\n",
      "    batch = list(itertools.islice(items, int(batch_size)))\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/spacy/language.py\", line 903, in _pipe\n",
      "    for doc in docs:\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/spacy/language.py\", line 691, in <genexpr>\n",
      "    docs = (self.make_doc(text) for text in texts)\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/spacy/language.py\", line 406, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"tokenizer.pyx\", line 89, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"doc.pyx\", line 194, in spacy.tokens.doc.Doc.__init__\n",
      "  File \"doc.pyx\", line 73, in spacy.tokens.doc._get_chunker\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/spacy/util.py\", line 61, in get_lang_class\n",
      "    entry_point = get_entry_point(\"spacy_languages\", lang)\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/spacy/util.py\", line 260, in get_entry_point\n",
      "    for entry_point in pkg_resources.iter_entry_points(key):\n",
      "  File \"/home/ajanco/spacy/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 645, in iter_entry_points\n",
      "    entries = dist.get_entry_map(group)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.batch-train DH-Budapest new_model --output new_model --label PLACE --n-iter 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would our model improve with more data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy ner.train-curve DH-Budapest new_model --label PLACE --n-iter 10 --eval-split 0.2 --dropout 0.2  --n-samples 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load and see the results of our new model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ITEM because that the kings most deare Uncle, the king of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Denmarke\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Norway\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       " &amp; \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sweveland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold &amp; great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Realme of England\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", by ye going in, entring &amp; passage of such forain &amp; strange persons into his realme of Norwey &amp; other dominions, streits, territories, jurisdictions &amp; places subdued and subject to him, specially into his \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Iles of Fynmarke\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", and elswhere, aswell in their persons as their things and goods</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"new_model\")\n",
    "doc = nlp(\n",
    "    \"ITEM because that the kings most deare Uncle, the king of Denmarke, Norway & Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring & passage of such forain & strange persons into his realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not too bad.  What about a text that the model has never seen before? Let's try Çelebi Evliya's [Narrative of travels in Europe, Asia, and Africa](https://archive.org/details/narrativeoftrave01evli/page/n4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of the place entities were in the training data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The army marched from \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Konia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Kaiseria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Caesarea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       "), and thence to \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sivas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Pâshâ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Kazmaghan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", and halted under the walls of Eriviin in the year 1044 (1634).  \n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"new_model\")\n",
    "doc = nlp(\n",
    "    \"\"\"The army marched from Konia to Kaiseria (Caesarea), and thence to Sivas, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "counter = 0\n",
    "for ent in doc.ents:\n",
    "    if ent.text in places:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        counter += 1\n",
    "\n",
    "print(f\"{counter} of the place entities were in the training data\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mustafâ Pâshâ is a person, but the model has done as passable job. Let's use our model to automatically identify historical place names and produce a markedup TEI document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The army marched from <name type=\"place\">Konia</name> to <name type=\"place\">Kaiseria</name> (<name type=\"place\">Caesarea</name>), and thence to <name type=\"place\">Sivas</name>, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ <name type=\"place\">Pâshâ</name>, the emperor \\'s favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander - in - chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of <name type=\"place\">Kazmaghan</name>, and halted under the walls of Eriviin in the year 1044 (1634)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"The army marched from Konia to Kaiseria (Caesarea), and thence to Sivas, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).\"\"\"  \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('new_model')\n",
    "doc = nlp(text)\n",
    "text_list = [i.text for i in doc]\n",
    "\n",
    "for token in doc:\n",
    "    if token.ent_type_ == 'PLACE': \n",
    "        text_list[token.i] = '<name type=\"place\">' + text_list[token.i] + '</name>'\n",
    "\n",
    "punct = ['.',\"'\",',',')',':',';']\n",
    "\n",
    "text=''\n",
    "for i, token in enumerate(text_list):\n",
    "    try:\n",
    "        if text_list[i+1] in punct:\n",
    "            text += token\n",
    "\n",
    "        else: \n",
    "            text += token + ' '\n",
    "        \n",
    "    except IndexError:\n",
    "        pass\n",
    "   \n",
    "    \n",
    "text.replace('\\n','').replace('( ','(')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save text as tei \n",
    "filename = 'my_tei.xml'\n",
    "language = 'en'\n",
    "\n",
    "tei_string = f\"\"\"\n",
    "<TEI.2>\n",
    "  <text lang=\"{language}\">\n",
    "    <body>\n",
    "        <p>\n",
    "            {text}\n",
    "        </p>\n",
    "    </body>\n",
    "  </text>\n",
    "</TEI.2>\n",
    "\"\"\"\n",
    "doc = etree.fromstring(tei_string)\n",
    "tree = etree.ElementTree(doc)\n",
    "tree.write(f'{filename}', pretty_print=True, xml_declaration=False,   encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TEI.2>\n",
      "  <text lang=\"en\">\n",
      "    <body>\n",
      "        <p>\n",
      "            The army marched from <name type=\"place\">Konia</name> to <name type=\"place\">Kaiseria</name> ( <name type=\"place\">Caesarea</name>), and thence to <name type=\"place\">Sivas</name>, where the feast of the Korbân ( sacrifice) was celebrated. Here Mustafâ <name type=\"place\">Pâshâ</name>, the emperor 's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander - in - chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of <name type=\"place\">Kazmaghan</name>, and halted under the walls of Eriviin in the year 1044 ( 1634)\n",
      "        </p>\n",
      "    </body>\n",
      "  </text>\n",
      "</TEI.2>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('my_tei.xml','r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. TEI to TEI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!prodigy dataset tei_to_tei \"A dataset for British historic places for the TEI to TEI section\" --author Andy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below uses David Lassner's [standoff converter](https://github.com/millawell/standoffconverter).  The tree_to_standoff() function returns a tuple with the plain text at index 0, a list of dictionaries for each xml tag at 1. Each tag contains the 'begin' and 'end' index in the text, the 'tag' as well as a dictionary of attributes ['attrib'] and ['depth'] \n",
    "\n",
    "```json\n",
    "[{'begin': 0, 'tag': 'TEI.2', 'attrib': {}, 'depth': 0, 'end': 51499},  ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import srsly   #https://pypi.org/project/srsly/\n",
    "import standoffconverter\n",
    "\n",
    "url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + refs[2]\n",
    "print(url)\n",
    "\n",
    "tei = urlopen(url).read()\n",
    "tei = etree.XML(tei)\n",
    "\n",
    "jsonl = {}\n",
    "standoff = standoffconverter.tree_to_standoff(tei)\n",
    "jsonl[\"text\"] = standoff[0]\n",
    "with open('my_tei.jsonl', 'w') as outfile:  \n",
    "    json.dump(jsonl, outfile)\n",
    "\n",
    "show = False \n",
    "if show:\n",
    "    with open('my_tei.jsonl','r') as fr:\n",
    "            jsonl = fr.read()\n",
    "            print(json.loads(jsonl)['text'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ✨  Starting the web server at http://spacy.apjan.co:8080 ...\n",
      "  Open the app in your browser and start annotating!\n",
      "\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.teach tei_to_tei en_core_web_sm my_tei.jsonl --patterns patterns.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<TEI.2><text lang='en'><body><div1 type='narrative' org='uniform' sample='complete'>\\n<head>Another branch of a statute made in the tenth yeere of the reigne of <name type='pers'>Henry</name> the sixt concerning the state of  the <name>English Marchants</name> in the dominions of the king  of <PLACE><name>Denmarke</name></PLACE>. </head>\\n<p>ITEM because that our soveraigne Lord the king at the\\ngrievous complaint to him made in this Parliament by\\nthe commons of his <name reg='England' type='place'>realme of <PLACE>England</name></PLACE> being in this\\nParliament is informed, that many of his faithfull liege\\npeople be greatly impoverished, undone, & in point to\\nbe destroyed by the <name type='pers'>king of <PLACE>Denmarke</name></PLACE> & his lieges, which\\nbe of the amitie of the king our soveraigne Lord, because\\nthat they do daily take of his said faithful subjects their\\ngoods, so that they have taken of marchants of <name reg='  +York [-1.83,53.966] (inhabited place), York, England, United Kingdom, Europe ' type='place' key='tgn,7011995'>York\\n</name>\\nand <name reg='  +Kingston upon Thames [-0.3,51.416] (neighborhood), Greater London, England, United Kingdom, Europe ' type='place' key='tgn,7011969'>Kingston\\n</name> upon <name>Hul</name> goods & marchandises to the\\nvalour of v.M.li. within a yeere, and of other lieges &\\nmarchants of ye <PLACE><name reg='England' type='place'>Realme of England</name></PLACE> goods & cattals to\\nthe valour of xx.M.li. wherof they have no remedie of\\nthe said <name type='pers'>king of <PLACE>Denmarke</name></PLACE>, nor of none other, forasmuch\\nas none of them commeth within the <PLACE><name reg='England' type='place'>Realme of England</name></PLACE>,\\nnor nothing have in the same <PLACE><name reg='England' type='place'>Realme of England</name></PLACE>, & that\\nye goods be taken out of the same Realme: The king\\nwilling to provide remedy for his said liege people, hath\\nordeined & established, that if ye goods of any of ye\\nsaid his lieges be or shalbe taken by the said king of\\n<PLACE><name>Denmarke</name></PLACE> or any of his said lieges, the keeper of the\\nprivie seale for ye time being, shall have power to make\\nto ye partie grieved letters of request under the privie\\nseale, wtout any other pursuite to be made to any for\\nrestitution to be had of ye goods so taken & to be taken.\\nAnd if restitution be not made by such letters, the king\\nour soveraigne lord by the advise of his counsel shal\\nprovide to the partie grieved his covenable remedy,\\naccording as ye case requireth.\\n</p></div1></body></text></TEI.2>\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import standoffconverter\n",
    "\n",
    "url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + refs[1]\n",
    "print(url)\n",
    "\n",
    "tei = urlopen(url).read()\n",
    "tei = etree.XML(tei)\n",
    "\n",
    "jsonl = {}\n",
    "standoff = standoffconverter.tree_to_standoff(tei)\n",
    "text = standoff[0]\n",
    "nlp = spacy.load('new_model')\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "\n",
    "    new_tag = {}\n",
    "    new_tag['begin'] = ent.start_char\n",
    "    new_tag['end'] = ent.end_char\n",
    "    new_tag['tag'] = ent.label_  # change PERSON to a valid TEI label\n",
    "    new_tag['attrib'] = {}\n",
    "    new_tag['depth'] = 0\n",
    "    standoff[1].append(new_tag)\n",
    "\n",
    "    \n",
    "new = standoffconverter.standoff_to_xml(text, standoff[1])\n",
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prodigy as a manual annotation tool \n",
    "text > tei manual markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The army marched from Konia to Kaiseria (Caesarea), and thence to Sivas, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).\"\"\"  \n",
    "with open('text2tei.txt','w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  ✨  \u001b[38;5;197mERROR:\u001b[0m 'text2tei' already exists in database SQLite.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!prodigy dataset text2tei \"A dataset for British historic places with text input\" --author Andy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 18 labels from model: QUANTITY, TIME, PRODUCT, CARDINAL, LOC, ORG, WORK_OF_ART, ORDINAL, PERCENT, LAW, GPE, MONEY, LANGUAGE, NORP, FAC, DATE, PERSON, EVENT\n",
      "\n",
      "  ✨  Starting the web server at http://spacy.apjan.co:8080 ...\n",
      "  Open the app in your browser and start annotating!\n",
      "\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!prodigy ner.manual text2tei en_core_web_sm text2tei.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  ✨  Exported 1 annotations for 'text2tei' from database SQLite\r\n",
      "  /home/ajanco/spaCy_DH2019_workshop/unit3/text2tei.jsonl\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!prodigy db-out text2tei ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 22, 'end': 27, 'token_start': 4, 'token_end': 4, 'label': 'GPE'}, {'start': 31, 'end': 39, 'token_start': 6, 'token_end': 6, 'label': 'GPE'}, {'start': 66, 'end': 71, 'token_start': 14, 'token_end': 14, 'label': 'GPE'}, {'start': 83, 'end': 102, 'token_start': 18, 'token_end': 21, 'label': 'EVENT'}, {'start': 136, 'end': 149, 'token_start': 29, 'token_end': 30, 'label': 'PERSON'}, {'start': 282, 'end': 289, 'token_start': 59, 'token_end': 59, 'label': 'GPE'}, {'start': 325, 'end': 343, 'token_start': 67, 'token_end': 71, 'label': 'PERSON'}, {'start': 448, 'end': 457, 'token_start': 92, 'token_end': 92, 'label': 'GPE'}, {'start': 489, 'end': 496, 'token_start': 100, 'token_end': 100, 'label': 'GPE'}, {'start': 509, 'end': 513, 'token_start': 104, 'token_end': 104, 'label': 'DATE'}, {'start': 515, 'end': 519, 'token_start': 106, 'token_end': 106, 'label': 'DATE'}]\n"
     ]
    }
   ],
   "source": [
    "with open('text2tei.jsonl','r') as f:\n",
    "    jsonl = json.loads(f.read())\n",
    "    print(jsonl['spans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def jsonl_to_xml(jsonl):    \n",
    "    jsonl = json.loads(jsonl)\n",
    "    text = jsonl['text']\n",
    "    offset = 0\n",
    "    \n",
    "    for span in jsonl['spans']:\n",
    "\n",
    "        new_text = f\"<{span['label']}>\" + text[span[\"start\"] +offset : span[\"end\"] + offset] +  f\"</{span['label']}>\" \n",
    "        text = text[:span[\"start\"] + offset] + new_text + text[span[\"end\"] + offset:]\n",
    "        offset += len(new_text) - (span[\"end\"] - span[\"start\"])\n",
    "    \n",
    "    return text\n",
    "        \n",
    "\n",
    "with open('text2tei.jsonl','r') as f:\n",
    "    jsonl = f.read()\n",
    "    xml = jsonl_to_xml(jsonl)\n",
    "    print(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save text as tei \n",
    "filename = 'annotated_tei.xml'\n",
    "language = 'en'\n",
    "\n",
    "tei_string = f\"\"\"\n",
    "<TEI.2>\n",
    "  <text lang=\"{language}\">\n",
    "    <body>\n",
    "        <p>\n",
    "            {xml}\n",
    "        </p>\n",
    "    </body>\n",
    "  </text>\n",
    "</TEI.2>\n",
    "\"\"\"\n",
    "doc = etree.fromstring(tei_string)\n",
    "tree = etree.ElementTree(doc)\n",
    "tree.write(f'{filename}', pretty_print=True, xml_declaration=False,   encoding=\"utf-8\")\n",
    "\n",
    "with open(filename,'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For complex annotation tasks, see [brat](https://brat.nlplab.org/index.html), ([demo](http://weaver.nlplab.org/~brat/demo/latest/#/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for your attention!  How might you use spaCy and Prodigy in your research? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
