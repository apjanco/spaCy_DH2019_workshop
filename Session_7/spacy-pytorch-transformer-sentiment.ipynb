{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import thinc.extra.datasets\n",
    "import spacy\n",
    "import torch\n",
    "from spacy.util import minibatch\n",
    "import tqdm\n",
    "import unicodedata\n",
    "import wasabi\n",
    "from spacy_pytorch_transformers.util import cyclic_triangular_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentencizer', 'pytt_wordpiecer', 'pytt_tok2vec']\n",
      "Loaded model 'en_pytt_distilbertbaseuncased_lg'\n",
      "Loading IMDB data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ('POSITIVE', 'NEGATIVE')\n",
      "Positive label for evaluation: POSITIVE\n",
      "Using 100 training docs, 2000 evaluation\n",
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  5%|▌         | 1/20 [00:07<02:28,  7.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 2/20 [00:24<03:09, 10.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 3/20 [00:38<03:13, 11.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 4/20 [00:49<03:04, 11.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 5/20 [00:59<02:42, 10.81s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 6/20 [01:14<02:50, 12.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 7/20 [01:31<02:58, 13.74s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 8/20 [01:41<02:30, 12.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 9/20 [01:52<02:13, 12.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 10/20 [02:05<02:03, 12.32s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 11/20 [02:24<02:08, 14.27s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 12/20 [02:37<01:50, 13.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 13/20 [02:45<01:25, 12.22s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Best scoring checkpoints\u001b[0m\n",
      "Epoch   Step   Score \n",
      "--   ----   ------\n",
      "This movie was so poorly written and directed I fell asleep 30 minutes through the movie. The jokes in the movie are corny and even though the plot is interesting at some angles, it is too far fetched and at some points- ridiculous. If you are 11 or older you will overlook the writing in the movie and be disappointed, but if you are 10 or younger this is a film that will capture your attention and be amazed with all the stunts (which I might add are poorly done) and wish you were some warrior to. The casting in this movie wasn't very good, and the music was very disappointing because it was like they were trying to build up the tension but it didn't fit at all. On a scale of 1-10 (10 being excellent, 1 being horrible) the acting in this movie is a 4. Brenda Song is talented in comedy, but with this kind of movie, in some of the more serious scenes, her acting was laughable. When she made some of her \"fighting\" poses, I started laughing out loud. I think the worst thing about this movie is definitely the directing, for example, the part where her enemy turns out to be the person the evil villain is possesing, how her voice turns dark and evil, I think that was incredibly stupid, and how Wendy's (Brenda Song)teachers were all her teachers at school being possessed by monks, that was pretty ridiculous to. So to sumamrize it all, a disappointing movie, but okay if you're 10 or under. {'POSITIVE': 0.4631398022174835, 'NEGATIVE': 0.5368601679801941}\n",
      "Saved model to output\n",
      "Loading from output\n",
      "This movie was so poorly written and directed I fell asleep 30 minutes through the movie. The jokes in the movie are corny and even though the plot is interesting at some angles, it is too far fetched and at some points- ridiculous. If you are 11 or older you will overlook the writing in the movie and be disappointed, but if you are 10 or younger this is a film that will capture your attention and be amazed with all the stunts (which I might add are poorly done) and wish you were some warrior to. The casting in this movie wasn't very good, and the music was very disappointing because it was like they were trying to build up the tension but it didn't fit at all. On a scale of 1-10 (10 being excellent, 1 being horrible) the acting in this movie is a 4. Brenda Song is talented in comedy, but with this kind of movie, in some of the more serious scenes, her acting was laughable. When she made some of her \"fighting\" poses, I started laughing out loud. I think the worst thing about this movie is definitely the directing, for example, the part where her enemy turns out to be the person the evil villain is possesing, how her voice turns dark and evil, I think that was incredibly stupid, and how Wendy's (Brenda Song)teachers were all her teachers at school being possessed by monks, that was pretty ridiculous to. So to sumamrize it all, a disappointing movie, but okay if you're 10 or under. {'POSITIVE': 0.4631398022174835, 'NEGATIVE': 0.5368601679801941}\n"
     ]
    }
   ],
   "source": [
    "https://github.com/explosion/spacy-pytorch-transformers/blob/master/examples/train_textcat.py\n",
    "def main(\n",
    "    model,\n",
    "    input_dir=None,\n",
    "    output_dir=None,\n",
    "    n_iter=1,\n",
    "    n_texts=100,\n",
    "    batch_size=8,\n",
    "    learn_rate=2e-5,\n",
    "    max_wpb=1000,\n",
    "    use_test=False,\n",
    "    pos_label=None,\n",
    "):\n",
    "    spacy.util.fix_random_seed(0)\n",
    "    is_using_gpu = spacy.prefer_gpu()\n",
    "    if is_using_gpu:\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "\n",
    "    nlp = spacy.load(model)\n",
    "    print(nlp.pipe_names)\n",
    "    print(f\"Loaded model '{model}'\")\n",
    "    textcat = nlp.create_pipe(\n",
    "        \"pytt_textcat\", config={\"architecture\": \"softmax_last_hidden\", \"words_per_batch\": max_wpb}\n",
    "    )\n",
    "    if input_dir is not None:\n",
    "        train_texts, train_cats = read_inputs(input_dir / \"training.jsonl\")\n",
    "        eval_texts, eval_cats = read_inputs(input_dir / \"evaluation.jsonl\")\n",
    "        labels = set()\n",
    "        for cats in train_cats + eval_cats:\n",
    "            labels.update(cats)\n",
    "        # use the first label in the set as the positive label if one isn't\n",
    "        # provided\n",
    "        for label in sorted(labels):\n",
    "            if not pos_label:\n",
    "                pos_label = label\n",
    "            textcat.add_label(label)\n",
    "    else:\n",
    "        # add label to text classifier\n",
    "        textcat.add_label(\"POSITIVE\")\n",
    "        textcat.add_label(\"NEGATIVE\")\n",
    "        if not pos_label:\n",
    "            pos_label = \"POSITIVE\"\n",
    "        # load the IMDB dataset\n",
    "        print(\"Loading IMDB data...\")\n",
    "        if use_test:\n",
    "            (train_texts, train_cats), (\n",
    "                eval_texts,\n",
    "                eval_cats,\n",
    "            ) = load_data_for_final_test(limit=n_texts)\n",
    "        else:\n",
    "            (train_texts, train_cats), (eval_texts, eval_cats) = load_data(\n",
    "                limit=n_texts\n",
    "            )\n",
    "\n",
    "    print(\"Labels:\", textcat.labels)\n",
    "    print(\"Positive label for evaluation:\", pos_label)\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "    print(f\"Using {len(train_texts)} training docs, {len(eval_texts)} evaluation\")\n",
    "    split_training_by_sentence = False\n",
    "    if split_training_by_sentence:\n",
    "        # If we're using a model that averages over sentence predictions (we are),\n",
    "        # there are some advantages to just labelling each sentence as an example.\n",
    "        # It means we can mix the sentences into different batches, so we can make\n",
    "        # more frequent updates. It also changes the loss somewhat, in a way that's\n",
    "        # not obviously better -- but it does seem to work well.\n",
    "        train_texts, train_cats = make_sentence_examples(nlp, train_texts, train_cats)\n",
    "        print(f\"Extracted {len(train_texts)} training sents\")\n",
    "    # total_words = sum(len(text.split()) for text in train_texts)\n",
    "    train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
    "    # Initialize the TextCategorizer, and create an optimizer.\n",
    "    optimizer = nlp.resume_training()\n",
    "    optimizer.alpha = 0.001\n",
    "    optimizer.pytt_weight_decay = 0.005\n",
    "    optimizer.L2 = 0.0\n",
    "    learn_rates = cyclic_triangular_rate(\n",
    "        learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size\n",
    "    )\n",
    "    print(\"Training the model...\")\n",
    "    print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "\n",
    "    pbar = tqdm.tqdm(total=20, leave=False)\n",
    "    results = []\n",
    "    epoch = 0\n",
    "    step = 0\n",
    "    eval_every = 20\n",
    "    patience = 3\n",
    "    for _ in range(n_iter):\n",
    "        # Train and evaluate\n",
    "        losses = Counter()\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=batch_size)\n",
    "        for batch in batches:\n",
    "            optimizer.pytt_lr = next(learn_rates)\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.1, losses=losses)\n",
    "            pbar.update(1)\n",
    "            if step and (step % eval_every) == 0:\n",
    "                pbar.close()\n",
    "                with nlp.use_params(optimizer.averages):\n",
    "                    scores = evaluate(nlp, eval_texts, eval_cats, pos_label)\n",
    "                results.append((scores[\"textcat_f\"], step, epoch))\n",
    "                print(\n",
    "                    \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(\n",
    "                        losses[\"pytt_textcat\"],\n",
    "                        scores[\"textcat_p\"],\n",
    "                        scores[\"textcat_r\"],\n",
    "                        scores[\"textcat_f\"],\n",
    "                    )\n",
    "                )\n",
    "                pbar = tqdm.tqdm(total=eval_every, leave=False)\n",
    "            step += 1\n",
    "        epoch += 1\n",
    "        # Stop if no improvement in HP.patience checkpoints\n",
    "        if results:\n",
    "            best_score, best_step, best_epoch = max(results)\n",
    "            if ((step - best_step) // eval_every) >= patience:\n",
    "                break\n",
    "\n",
    "    msg = wasabi.Printer()\n",
    "    table_widths = [2, 4, 6]\n",
    "    msg.info(f\"Best scoring checkpoints\")\n",
    "    msg.row([\"Epoch\", \"Step\", \"Score\"], widths=table_widths)\n",
    "    msg.row([\"-\" * width for width in table_widths])\n",
    "    for score, step, epoch in sorted(results, reverse=True)[:10]:\n",
    "        msg.row([epoch, step, \"%.2f\" % (score * 100)], widths=table_widths)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = eval_texts[0]\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)\n",
    "\n",
    "    if output_dir is not None:\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        print(test_text, doc2.cats)\n",
    "\n",
    "\n",
    "def read_inputs(input_path):\n",
    "    texts = []\n",
    "    cats = []\n",
    "    with input_path.open(mode=\"r\") as file_:\n",
    "        for line in file_:\n",
    "            text, gold = json.loads(line)\n",
    "            text = preprocess_text(text)\n",
    "            texts.append(text)\n",
    "            cats.append(gold[\"cats\"])\n",
    "    return texts, cats\n",
    "\n",
    "\n",
    "def make_sentence_examples(nlp, texts, labels):\n",
    "    \"\"\"Treat each sentence of the document as an instance, using the doc labels.\"\"\"\n",
    "    sents = []\n",
    "    sent_cats = []\n",
    "    for text, cats in zip(texts, labels):\n",
    "        doc = nlp.make_doc(text)\n",
    "        doc = nlp.get_pipe(\"sentencizer\")(doc)\n",
    "        for sent in doc.sents:\n",
    "            sents.append(sent.text)\n",
    "            sent_cats.append(cats)\n",
    "    return sents, sent_cats\n",
    "\n",
    "\n",
    "white_re = re.compile(r\"\\s\\s+\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace(\"<s>\", \"<open-s-tag>\")\n",
    "    text = text.replace(\"</s>\", \"<close-s-tag>\")\n",
    "    text = white_re.sub(\" \", text).strip()\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_data(*, limit=0, dev_size=2000):\n",
    "    \"\"\"Load data from the IMDB dataset, splitting off a held-out set.\"\"\"\n",
    "    if limit != 0:\n",
    "        limit += dev_size\n",
    "    assert dev_size != 0\n",
    "    train_data, _ = thinc.extra.datasets.imdb(limit=limit)\n",
    "    assert len(train_data) > dev_size\n",
    "    random.shuffle(train_data)\n",
    "    dev_data = train_data[:dev_size]\n",
    "    train_data = train_data[dev_size:]\n",
    "    train_texts, train_labels = _prepare_partition(train_data, preprocess=False)\n",
    "    dev_texts, dev_labels = _prepare_partition(dev_data, preprocess=False)\n",
    "    return (train_texts, train_labels), (dev_texts, dev_labels)\n",
    "\n",
    "\n",
    "def load_data_for_final_test(*, limit=0):\n",
    "    print(\n",
    "        \"Warning: Using test data. You should use development data for most experiments.\"\n",
    "    )\n",
    "    train_data, test_data = thinc.extra.datasets.imdb()\n",
    "    random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    train_texts, train_labels = _prepare_partition(train_data)\n",
    "    test_texts, test_labels = _prepare_partition(test_data)\n",
    "    return (train_texts, train_labels), (test_texts, test_labels)\n",
    "\n",
    "\n",
    "def _prepare_partition(text_label_tuples, *, preprocess=False):\n",
    "    texts, labels = zip(*text_label_tuples)\n",
    "    if preprocess:\n",
    "        # Preprocessing can mask errors in our handling of noisy text, so\n",
    "        # we don't want to do it by default\n",
    "        texts = [preprocess_text(text) for text in texts]\n",
    "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
    "    return texts, cats\n",
    "\n",
    "\n",
    "def evaluate(nlp, texts, cats, pos_label):\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 0.0  # False positives\n",
    "    fn = 0.0  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    with tqdm.tqdm(total=total_words, leave=False) as pbar:\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=8)):\n",
    "            gold = cats[i]\n",
    "            for label, score in doc.cats.items():\n",
    "                if label not in gold:\n",
    "                    continue\n",
    "                if label != pos_label:\n",
    "                    continue\n",
    "                if score >= 0.5 and gold[label] >= 0.5:\n",
    "                    tp += 1.0\n",
    "                elif score >= 0.5 and gold[label] < 0.5:\n",
    "                    fp += 1.0\n",
    "                elif score < 0.5 and gold[label] < 0.5:\n",
    "                    tn += 1\n",
    "                elif score < 0.5 and gold[label] >= 0.5:\n",
    "                    fn += 1\n",
    "            pbar.update(len(doc.text.split()))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}\n",
    "\n",
    "main(\"en_pytt_distilbertbaseuncased_lg\", output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bodleian_downloader\n",
    "from lxml import etree\n",
    "import standoffconverter\n",
    "texts = []\n",
    "speakers = []\n",
    "\n",
    "for fn in bodleian_downloader.get_file_descriptors():\n",
    "\n",
    "    tree = etree.fromstring(open(fn, \"rb\").read())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for speech in tree.findall(\".//{http://www.tei-c.org/ns/1.0}sp\"):\n",
    "    speaker = speech.findall(\".//{http://www.tei-c.org/ns/1.0}speaker\")\n",
    "    \n",
    "    lines = speech.findall(\".//{http://www.tei-c.org/ns/1.0}l\")\n",
    "    if len(speaker) > 0:\n",
    "        for line in lines:\n",
    "            so = standoffconverter.Standoff()\n",
    "            so.from_lxml_tree(line)\n",
    "            \n",
    "            texts.append(so.plain)\n",
    "            speakers.append(speaker[0].text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i, doc in enumerate(nlp.pipe(texts, batch_size=8)):\n",
    "    for label, score in doc.cats.items():\n",
    "        if label == \"POSITIVE\":\n",
    "            scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "scores = np.array(scores)\n",
    "speakers = np.array(speakers)\n",
    "\n",
    "uspeakers = np.array(sorted(list(set(speakers))))\n",
    "mean_scores = np.empty(len(uspeakers))\n",
    "for ispeaker,uspeaker in enumerate(uspeakers):\n",
    "    mean_scores[ispeaker] = scores[speakers == uspeaker].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter(speakers)\n",
    "\n",
    "labels,_ = zip(*cnt.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(len(labels)), mean_scores[[uspeakers[i] in labels for i in range(len(uspeakers))]])\n",
    "plt.xlabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ar.', 'Art.', 'Arth.', 'Arthur.', 'Aus.', 'Aust.', 'Ba.', 'Bast.',\n",
       "       'Big.', 'Bla.', 'Blan.', 'Blanch.', 'Chat.', 'Cit.', 'Con.',\n",
       "       'Cons.', 'Const.', 'Daul.', 'Dol.', 'Dolph.', 'E. Har.', 'Ele.',\n",
       "       'Elea.', 'Elen.', 'Eli.', 'Elinor.', 'Eng.', 'Essex.', 'Exec.',\n",
       "       'F. Her.', 'Fra.', 'Fran.', 'France.', 'Gour.', 'H', 'Hen.', 'Hu.',\n",
       "       'Hub.', 'Hubert.', 'Io.', 'Ioh.', 'Iohn.', 'K. Io.', 'K. Iohn.',\n",
       "       'Kin.', 'King Iohn.', 'King.', 'Lady.', 'Lew.', 'Lewis.', 'Mel.',\n",
       "       'Mes.', 'Old Qu.', 'P.', 'Pan.', 'Pand.', 'Pem.', 'Pem. Big.',\n",
       "       'Pet.', 'Phi.', 'Phil.', 'Philip.', 'Qu.', 'Qu. Mo.', 'Que.',\n",
       "       'Queen.', 'Rob.', 'Robert.', 'Sal.'], dtype='<U10')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uspeakers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wh2",
   "language": "python",
   "name": "wh2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
