{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiGY3AW0lulV"
   },
   "source": [
    "\n",
    "## Introduction to \n",
    "<img width=500 src=\"https://miro.medium.com/max/1200/1*HTtQseukwrBiREJf8MSVcA.jpeg\" alt=\"Spacy Logo\"/>\n",
    "\n",
    "\n",
    "- [Main Documentation Page](https://spacy.io/)  \n",
    "- [How to install spaCy](https://spacy.io/usage)\n",
    "- [spaCy 101, The most important concepts, explained in simple terms\n",
    "](https://spacy.io/usage/spacy-101)\n",
    "- [Free course- Advanced NLP with spaCy](https://course.spacy.io/)\n",
    "\n",
    "- Please go ahead and form into groups of 2-4 people. Say hello and give a brief introduction.  I encourage you to talk and ask questions during the workshop. Listen to me with one ear while you work on code, tinker and make sense of things in ways that have relevance to you.  Please share your ideas and interupt me. Move around. The main goal is that you come away with a basic understanding of spaCy and how it might be useful in your projects.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRbX-RD9kb2H"
   },
   "source": [
    "# Strings \n",
    "\n",
    "### Without spaCy, Python is able to process text as a sequence of characters (called a string).  We can slice a string, we can add strings, replace sections of a string and many other tasks.  \n",
    "\n",
    "See [w3schools string functions](https://www.w3schools.com/python/python_ref_string.asp)\n",
    "\n",
    "Common examples for working with strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-RO_cNiluli"
   },
   "outputs": [],
   "source": [
    "#Selecting a slice, selecting part of the string from [begin : end]\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "print('the string \"{}\" has {} characters. Note that the index begins at 0.'.format(wilde, len(wilde)))\n",
    "\n",
    "[print(i, charachter) for i, charachter in enumerate(wilde)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sU5pM0AZlul_"
   },
   "outputs": [],
   "source": [
    "print('wilde[4:12] will start at position 4 and end at 12 ->', wilde[4:12])\n",
    "print('or read backwards from the end [-40: -32] ->', wilde[-40:-32])\n",
    "print('you can even mix forward and backward! wilde[-40:12]', wilde[-40:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_URSsHTlumU"
   },
   "outputs": [],
   "source": [
    "#Find and replace\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "wilde.replace('yourself', 'a fish').replace('everyone', 'everything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiH0FRmKlump"
   },
   "outputs": [],
   "source": [
    "#Split \n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "print(wilde.split()) # Split on empty spaces\n",
    "print(wilde.split(';'))\n",
    "print(wilde.split('y'))  #Note that the charachter or space used to split the string is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSnowJAdlum_"
   },
   "outputs": [],
   "source": [
    "# We can join a list of strings\n",
    "print(' '.join(['Be', 'yourself;', 'everyone', 'else', 'is', 'already', 'taken.']))\n",
    "\n",
    "import random\n",
    "\n",
    "animals = ['fish', 'turtle', 'panther', 'parrot']\n",
    "adjective = ['scary', 'green', 'overweight', 'fluffy']\n",
    "print('Be a ' + ' '.join([random.choice(adjective), random.choice(animals)]) + ' everything else is taken.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzUFZaXqlunS"
   },
   "source": [
    "# Base Language object\n",
    "### spaCy gives the machine an understanding of text, not just as a sequence of characters, but as natural language\n",
    "\n",
    "[A full list of base languages](https://github.com/explosion/spaCy/tree/master/spacy/lang)\n",
    "\n",
    "- stop words \n",
    "- lemmatization \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTah4hePlunZ"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "\n",
    "nlp = German()\n",
    "doc = nlp('Sei du selbst! Alle anderen sind bereits vergeben.')\n",
    "\n",
    "\n",
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "doc = nlp('Be yourself; everyone else is already taken.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jN_klNNxlunu"
   },
   "source": [
    "### The document object\n",
    "Once we have imported a base language class or language model and a text, spaCy will create what is called a document (doc) object.  \n",
    "The doc object typically contains:\n",
    "\n",
    "\n",
    "|   [attributes](https://spacy.io/api/doc#attributes) |   | \n",
    "|---|---|\n",
    "| tokens (individual parts of the text)  | doc[5]  |\n",
    "|  the text  | doc.text\n",
    "| the text split into sentences  | doc.sents |\n",
    "| entities detected in the text | doc.ents |\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/doc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uf5zn_Zlunz"
   },
   "outputs": [],
   "source": [
    "print('**Note the difference between working with a slice of a doc object versus a Python string**')\n",
    "\n",
    "print(wilde[:3])\n",
    "print(doc[:3])\n",
    "\n",
    "print('**Also note how spaCy tokenization differs from Python split()**')\n",
    "print('[*] Python:')\n",
    "for token in wilde.split():\n",
    "    print(token)\n",
    "    \n",
    "print('------')    \n",
    "print('[*] spaCy:')\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FuwTW47luoM"
   },
   "outputs": [],
   "source": [
    "# The to_json() method is a useful way to view the information contained in the doc object\n",
    "\n",
    "doc = nlp('Be yourself; everyone else is already taken.')\n",
    "doc.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nf1vUJKwluob"
   },
   "source": [
    "<img height=100 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Tokens\n",
    "As you can see above, the doc contains a split of the text into tokens.  Each token object has [65 attributes](https://spacy.io/api/token#attributes) that can be used during analysis.  Common tasks include:\n",
    "- removing all punctuation from the text\n",
    "- counting root forms of the words (lemmata)\n",
    "- removing stopwords from the doc\n",
    "\n",
    "\n",
    "|   [attributes](https://spacy.io/api/token#attributes) |   | \n",
    "|---|---|\n",
    "| root form (lemma)  | token.lemma_  |\n",
    "| Named entity type  | token.ent_type_ |\n",
    "| token is punctuation  | token.is_punct |\n",
    "| part of speech | token.pos_ |\n",
    "| in stop words | token.is_stop |\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/token#_title).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzPYwsyyluoi"
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\n",
    "         token.lemma_,\n",
    "         token.pos_,\n",
    "         token.dep_,\n",
    "         token.shape_,\n",
    "         token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAzfvNVAluot"
   },
   "outputs": [],
   "source": [
    "# Useful function to make sense of linguistic terminology and abbreviations \n",
    "import spacy \n",
    "\n",
    "spacy.explain(\"PRON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gg2_ssmulupl"
   },
   "source": [
    "<img height=100 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Spans\n",
    "When studying text, we are often interested in features that involve more than one token.  To do this, we can create a span.  For example, \"New York City\"\n",
    "\n",
    "Span [attributes](https://spacy.io/api/span#attributes)\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/span#_title). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8JT4DQQlups"
   },
   "outputs": [],
   "source": [
    "text = 'I just got back from New York City.'\n",
    "nlp = English()\n",
    "doc = nlp(text)\n",
    "\n",
    "nyc = doc[5:8] \n",
    "\n",
    "print(\n",
    "    '[*] spaCy',\n",
    "    nyc.start,\n",
    "    nyc.end,\n",
    "    doc[nyc.start:nyc.end],\n",
    ")\n",
    "print(  \n",
    "    '[*] string',\n",
    "    nyc.start_char,\n",
    "    nyc.end_char,\n",
    "    text[nyc.start_char:nyc.end_char]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgGYxqq7luqG"
   },
   "source": [
    "# Exercise: create individualized vocabularly lists \n",
    "At Haverford, we have an application called [the Bridge](https://bridge.haverford.edu/) that generates custom vocabulary lists for learning Latin and ancient Greek.  To do this, we create a list of words from texts that the student has already read and understood.  We then use the lemma of each word to compare the list of known words against words in a new text.  We can then identify which words will be new to the reader.\n",
    "\n",
    "<img height=\"400\" src='https://www.boxofficepro.com/wp-content/uploads/2020/02/emma-dom-E_FP_00001_rgb-scaled.jpg'>\n",
    "\n",
    "Let's say that I'm learning English and reading Jane Austen's *Emma* (1815).  I have just finished volume one and want to know what new words I will encounter when reading volume two.   \n",
    "\n",
    ">*Note* I am using Python sets to find the difference between the two volumes. I could also find the union, the intersection and other set operations.  For more on this topic, there is an excellent tutorial from [Real Python](https://realpython.com/python-sets/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQDdJTuiluqV"
   },
   "outputs": [],
   "source": [
    "# Here I use the requests library to get the texts from Project Gutenberg\n",
    "import requests \n",
    "emma = requests.get('http://www.gutenberg.org/files/158/158-0.txt')\n",
    "split = emma.text.find('VOLUME II')\n",
    "vol_1 = emma.text[:split]\n",
    "vol_2 = emma.text[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuE7n2TGluqu"
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.max_length = 1070000 # This is needed given the length of the text \n",
    "\n",
    "vol_1_doc = nlp(vol_1)\n",
    "\n",
    "# Create a set of words that are not punctuation or stop words\n",
    "vol_1_words = set([token.lemma_ for token in vol_1_doc if token.is_stop is False and token.is_punct is False])\n",
    "\n",
    "vol_2_doc = nlp(vol_2)\n",
    "vol_2_words = set([token.lemma_ for token in vol_2_doc if token.is_stop is False and token.is_punct is False])\n",
    "\n",
    "new_words = vol_2_words.difference(vol_1_words)\n",
    "len(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXE3B7znlurG"
   },
   "source": [
    "## Ouch, that's far too many words to learn!  Let's only count the 100 most frequent words and then create our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDCLoVJilurN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "from collections import Counter\n",
    "\n",
    "# Add an extension to our tokens called \"count\"\n",
    "Token.set_extension(\"count\", default=False, force=True)\n",
    "\n",
    "\n",
    "# Calculate the number of times that a lemma appears in the text\n",
    "counts = Counter([token.lemma_ for token in vol_1_doc if not token.is_punct and not token.is_stop]).most_common(100)\n",
    "counts = dict(counts)\n",
    "\n",
    "# Add the count to each token. \n",
    "vol_1_doc = nlp(vol_1)\n",
    "for token in vol_1_doc:\n",
    "    if token.lemma_ in counts.keys():\n",
    "        token._.count = counts[token.lemma_]\n",
    "\n",
    "# Repeat for the second text and find the difference \n",
    "counts = Counter([token.lemma_ for token in vol_2_doc if not token.is_punct and not token.is_stop]).most_common(100)\n",
    "counts = dict(counts)\n",
    "\n",
    "# These are clearly not words, let's get rid of them\n",
    "del counts['\\r\\n']\n",
    "del counts['\\r\\n\\r\\n']\n",
    "del counts[' ']\n",
    "\n",
    "\n",
    "vol_2_doc = nlp(vol_2)\n",
    "for token in vol_2_doc:\n",
    "    if token.lemma_ in counts.keys():\n",
    "        token._.count = counts[token.lemma_]\n",
    "\n",
    "# Now we find the difference between the most common words in the two texts        \n",
    "#set_vol1 = set([(token.lemma_, token._.count) for token in vol_1_doc if token._.count])\n",
    "#set_vol2 = set([(token.lemma_, token._.count) for token in vol_2_doc if token._.count])\n",
    "not_words = ['\\r\\n','\\r\\n\\r\\n',' ']\n",
    "set_vol1 = set([token.lemma_ for token in vol_1_doc if token._.count and token.lemma_ not in not_words])\n",
    "set_vol2 = set([token.lemma_ for token in vol_2_doc if token._.count and token.lemma_ not in not_words])\n",
    "\n",
    "difference = set_vol1.difference(set_vol2)\n",
    "difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylbG2bC5lur8"
   },
   "source": [
    "# Models \n",
    "\n",
    "What if we wanted to create a list of the 100 most freqent verbs or nouns in the text?  With the base Hungarian model, token.pos_ returns nothing. Also take a look at our lemmas. Are those really the root forms?  The basic Hungarian model simply does not know parts of speech or lemmata.  We need one that does. \n",
    "\n",
    "Here is a listing of the officially supported spaCy models: https://spacy.io/models\n",
    "There are currently models for :\n",
    "- English\n",
    "- German\n",
    "- French\n",
    "- Spanish\n",
    "- Portuguese\n",
    "- Italian\n",
    "- Dutch\n",
    "- Greek\n",
    "- Multi-language\n",
    "\n",
    "The spaCy documentation lists the features and capabilities of each model.  Keep in mind that there can be several models for a language.  Larger models are often slower and require more memory. In exchange, the larger models are often more accurate and have more features such as word vectors, dependency parsing and other pipelines.   If you're not using the more advanced features of a large model, then you would probably be better off using something small.  As a general rule, it's best to start small and then deliberately move up as needed. \n",
    "\n",
    "\n",
    "To add a spaCy supported model, simply type: \n",
    "`python -m spacy download <name of model>` `en_core_web_sm` for example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-W6T6yblusA"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#English base language object\n",
    "#nlp = English()\n",
    "\n",
    "#English small language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "doc = nlp('Be yourself; everyone else is already taken.')\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SghahbnvlutB"
   },
   "source": [
    "**Further Reading on Parts of Speech**  \n",
    "\n",
    "[Johnathan Reeve, Isolating Literary Style with Raymond Queneau\n",
    "](https://jonreeve.com/2019/09/exercises-in-style/) ([code notebook](https://gist.github.com/JonathanReeve/cacf9d874b405b621710a7436425af49))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SJn8NpNLlutI"
   },
   "source": [
    "<img height=100 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Named Entity Recognition\n",
    "Most of the models in spaCy have an entity recognizer.  This is similar to identifying parts of speech in the text, but greatly expands what we can automatically identify.  The types of entities and categories will vary from model to model and should be in the model's documentation. For most languages, the categories are: \n",
    "\n",
    "|   [named entities](https://spacy.io/api/annotation#ner-wikipedia-scheme) |   | \n",
    "|---|---|\n",
    "| PER  | Named person or family  |\n",
    "| ORG  | Named corporate, governmental, or other organizational entitity. |\n",
    "| LOC  | Name of politically or grographically defined location (cities, provinces, countries, international regions, bodies of water, mountains).  |\n",
    "| MISC | Miscellaneous entities, e.g. events, nationalities, products or works of art. |\n",
    "\n",
    "[Here is a list of the categories in the spaCy small English model](https://spacy.io/api/annotation#named-entities)\n",
    "\n",
    "[Here is a useful web application that can be used to assess the categories available in various spaCy models](https://explosion.ai/demos/displacy-ent)\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/usage/linguistic-features#named-entities-101).\n",
    "\n",
    "--- \n",
    "\n",
    "H.G. Wells, *The Invisible Man* (1897)\n",
    "<img src=\"https://www.slashfilm.com/wp/wp-content/images/invisible-man-cast-new.jpg\" alt=\"invisible man photo\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "st0OcOaelutR"
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "invisible_man = requests.get('http://www.gutenberg.org/cache/epub/5230/pg5230.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tggeYyBRluth"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "from IPython.display import HTML, IFrame\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(invisible_man.text[600:1500])\n",
    "\n",
    "HTML(displacy.render(doc, style=\"ent\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLbnq_FMlutq"
   },
   "outputs": [],
   "source": [
    "# list of people that appear in the text \n",
    "import pandas as pd\n",
    "doc = nlp(invisible_man.text)\n",
    "person_list = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        person_list.append(ent.text.replace('\\r','').replace('\\n',''))\n",
    "\n",
    "df = pd.DataFrame(set(person_list)) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IK_ZS2Blut5"
   },
   "outputs": [],
   "source": [
    "# list of places that appear in the text \n",
    "import pandas as pd\n",
    "doc = nlp(invisible_man.text)\n",
    "place_list = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'GPE':\n",
    "        place_list.append(ent.text)\n",
    "\n",
    "df = pd.DataFrame(set(place_list)) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7AzGBNbluuH"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "HTML(displacy.render(next(doc.sents), style=\"dep\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JYJabHcMluuP",
    "outputId": "54778efc-f6cf-4b6d-f495-d12dc71ce270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473\n"
     ]
    }
   ],
   "source": [
    "# Source https://github.com/pmbaumgartner/binder-notebooks/blob/master/holy-nlp.ipynb \n",
    "\n",
    "actors_and_actions = []\n",
    "\n",
    "def token_is_subject_with_action(token):\n",
    "    nsubj = token.dep_ == 'nsubj'\n",
    "    head_verb = token.head.pos_ == 'VERB'\n",
    "    person = token.ent_type_ == 'PERSON'\n",
    "    return nsubj and head_verb and person\n",
    "\n",
    "for token in doc:\n",
    "    if token_is_subject_with_action(token):\n",
    "        span = doc[token.head.left_edge.i:token.head.right_edge.i+1]\n",
    "        data = dict(name=token.orth_,\n",
    "                    span=span.text,\n",
    "                    verb=token.head.lower_,\n",
    "                    log_prob=token.head.prob,\n",
    "                    )\n",
    "        actors_and_actions.append(data)\n",
    "\n",
    "print(len(actors_and_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "h4ZRDOjGluuV",
    "outputId": "14f5f876-460f-469e-e7f3-093c95bb8b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Names: 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "action_df = pd.DataFrame(actors_and_actions)\n",
    "\n",
    "print('Unique Names:', action_df['name'].nunique())\n",
    "\n",
    "most_common = (action_df\n",
    "    .groupby(['name', 'verb'])\n",
    "    .size()\n",
    "    .groupby(level=0, group_keys=False)\n",
    "    .nlargest(1)\n",
    "    .rename('Count')\n",
    "    .reset_index(level=0)\n",
    "    .rename(columns={\n",
    "        'verb': 'Most Common'\n",
    "    })\n",
    ")\n",
    "\n",
    "# exclude log prob < -20, those indicate absence in the model vocabulary\n",
    "most_unique = (action_df[action_df['log_prob'] > -20]\n",
    "    .groupby(['name', 'verb'])['log_prob']\n",
    "    .min()\n",
    "    .groupby(level=0, group_keys=False)\n",
    "    .nsmallest(1)\n",
    "    .rename('Log Prob.')\n",
    "    .reset_index(level = 0)\n",
    "    .rename(columns={\n",
    "        'verb': 'Most Unique'\n",
    "    })\n",
    ")\n",
    "\n",
    "# SO groupby credit\n",
    "# https: //stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "aJ0E_AESluuk",
    "outputId": "28f37d43-8c86-4008-a48f-34cb91618bba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verb</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Kemp</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Marvel</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Hall</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Adye</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Henfrey</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Bunting</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Cuss</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Jaffers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Griffin</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>Lemme</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Wadgers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Teddy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>Huxter</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>works</th>\n",
       "      <td>tm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>Phipps</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  Count\n",
       "verb                 \n",
       "said      Kemp     68\n",
       "said    Marvel     50\n",
       "said      Hall     34\n",
       "said      Adye     19\n",
       "said   Henfrey     13\n",
       "said   Bunting     11\n",
       "said      Cuss     10\n",
       "said   Jaffers      9\n",
       "said   Griffin      4\n",
       "go       Lemme      3\n",
       "said   Wadgers      2\n",
       "said     Teddy      2\n",
       "said    Huxter      2\n",
       "works       tm      2\n",
       "had     Phipps      1"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common.sort_values('Count', ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hecr2eW_luu3"
   },
   "source": [
    "## Working with Stanfordnlp models ![](https://pbs.twimg.com/profile_images/897182721272799232/0CplRl36_400x400.jpg)\n",
    "\n",
    "[Documentation](https://stanfordnlp.github.io/stanfordnlp/installation_usage.html)\n",
    "\n",
    "```\n",
    "$ pip install stanfordnlp spacy-stanfordnlp\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTENNbhDluu5"
   },
   "source": [
    "```python\n",
    "import stanfordnlp\n",
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "\n",
    "\n",
    "Using the default treebank \"en_ewt\" for language \"en\".\n",
    "Would you like to download the models for: en_ewt now? (Y/n)\n",
    "y\n",
    "\n",
    "Default download directory: /home/ajanco/stanfordnlp_resources\n",
    "Hit enter to continue or type an alternate directory.\n",
    "\n",
    "\n",
    "Downloading models for: en_ewt\n",
    "Download location: /home/ajanco/stanfordnlp_resources/en_ewt_models.zip\n",
    "100%|██████████| 235M/235M [00:51<00:00, 4.92MB/s] \n",
    "\n",
    "Download complete.  Models saved to: /home/ajanco/stanfordnlp_resources/en_ewt_models.zip\n",
    "Extracting models file for: en_ewt\n",
    "Cleaning up...Done.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZX7-9OIsluu8"
   },
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "from spacy_stanfordnlp import StanfordNLPLanguage\n",
    "\n",
    "snlp = stanfordnlp.Pipeline(lang=\"en\")\n",
    "nlp = StanfordNLPLanguage(snlp)\n",
    "\n",
    "doc = nlp('Be yourself; everyone else is already taken.')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zq2yzyIqLR_O"
   },
   "source": [
    "# Transformer models \n",
    "[spaCy meets Transformers: Fine-tune BERT, XLNet and GPT-2](https://explosion.ai/blog/spacy-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "szga11k5LU9T",
    "outputId": "5b0d3084-01ad-4a29-cf15-47c0e3bc0027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_vectors_web_lg==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.1.0/en_vectors_web_lg-2.1.0.tar.gz (661.8MB)\n",
      "\u001b[K     |████████████████████████████████| 661.8MB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: en-vectors-web-lg\n",
      "  Building wheel for en-vectors-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-vectors-web-lg: filename=en_vectors_web_lg-2.1.0-cp36-none-any.whl size=663461747 sha256=29c0b957ba460d655f6326b68ae96f4fdb8d9597f3c631b0005fd2769663e6f5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-grylxdwg/wheels/ce/3e/83/59647d0b4584003cce18fb68ecda2866e7c7b2722c3ecaddaf\n",
      "Successfully built en-vectors-web-lg\n",
      "Installing collected packages: en-vectors-web-lg\n",
      "Successfully installed en-vectors-web-lg-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_vectors_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_vectors_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "VQumqKvOMVLW",
    "outputId": "6cd29afe-e9c7-4f08-a40e-b51df704502b"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-13115037de81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_vectors_web_lg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_vectors_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_vectors_web_lg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21Tqr1QpluvK"
   },
   "source": [
    "![](https://spacy.io/architecture-bcdfffe5c0b9f221a2f6607f96ca0e4a.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CISPd6VWCKiV"
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XShhpz-oluvN"
   },
   "outputs": [],
   "source": [
    "# Language object\n",
    "from spacy.lang.es import Spanish\n",
    "nlp = Spanish()\n",
    "\n",
    "# Doc object\n",
    "doc = nlp(\"La duda es uno de los nombres de la inteligencia.\")\n",
    "\n",
    "# Tokens\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    \n",
    "# Spans\n",
    "span = doc[0:2]\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swkJ8z7LluvY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DET\n",
      "NOUN\n",
      "AUX\n",
      "PRON\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "# python -m spacy download es_core_news_sm\n",
    "\n",
    "# Models\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"La duda es uno de los nombres de la inteligencia.\")\n",
    "\n",
    "# Part of speech \n",
    "for token in doc:\n",
    "    print(token.pos_)\n",
    "    \n",
    "# Entities \n",
    "for token in doc.ents:\n",
    "    print(token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Intro_to_spaCy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
