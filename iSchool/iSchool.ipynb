{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4AFAZ5KoMRnN"
   },
   "source": [
    "## Named Entity Linking with spaCy and TEI\n",
    "![](https://explosion.ai/blog/img/spacy-transformers.jpg)\n",
    "\n",
    "![](https://pbs.twimg.com/media/D0aHPzXWwAEgRwU?format=jpg&name=900x900)/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "iAHx4-ZcMdbK",
    "outputId": "4f1a635e-36cf-4382-8732-4f46f1c2bdc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/PW3RJM8tDGo\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f2fa3f86a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "# Youtube\n",
    "IFrame(\"https://www.youtube.com/embed/PW3RJM8tDGo\", 560, 315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1e5MZtJMmWY"
   },
   "source": [
    "### In this first example, our goal is to teach an existing English-language model to identify early modern place names.\n",
    "\n",
    "There are several approaches that we could take to this problem.  Different approaches can lend better or worse results and experimentation is an essential part of any machine learning project. \n",
    "\n",
    "#### How can we teach a statistical language model that Sweveland is a place? Where can I get data on early modern places? \n",
    "\n",
    "Richard Hakluyt's The Principal Navigations, Voyages, Traffiques, and Discoveries of the English Nation (1599)\n",
    "\n",
    "![](http://www.sequiturbooks.com/image/cache/Product%20Images/2015-12/The-Principal-1512150003/5ae35178-800x800.jpeg)\n",
    "\n",
    "--- \n",
    "\n",
    "### Download the TEI files from Persius \n",
    "- We're going to extract a list of all the place names from the text to create training data.\n",
    "- To make working with the TEI/XML easier, we're using a standoffconverter by David Lassner\n",
    "- The converter separates the text and annotations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gM3Gl_qhxd13",
    "outputId": "257d3d63-de16-44bf-afb8-e6ed3f204617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-12 07:27:57--  https://github.com/apjanco/spaCy_workshops/raw/master/Session_6/en_early_modern_places-2.1.0.tar.gz\n",
      "Resolving github.com (github.com)... 140.82.112.3\n",
      "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/apjanco/spaCy_workshops/master/Session_6/en_early_modern_places-2.1.0.tar.gz [following]\n",
      "--2021-02-12 07:27:57--  https://raw.githubusercontent.com/apjanco/spaCy_workshops/master/Session_6/en_early_modern_places-2.1.0.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11156243 (11M) [application/octet-stream]\n",
      "Saving to: ‘en_early_modern_places-2.1.0.tar.gz.3’\n",
      "\n",
      "en_early_modern_pla 100%[===================>]  10.64M  13.1MB/s    in 0.8s    \n",
      "\n",
      "2021-02-12 07:27:59 (13.1 MB/s) - ‘en_early_modern_places-2.1.0.tar.gz.3’ saved [11156243/11156243]\n",
      "\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Processing ./en_early_modern_places-2.1.0.tar.gz\n",
      "Collecting spacy>=2.1.0\n",
      "  Downloading spacy-3.0.1-cp38-cp38-manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.9 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 12.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "  Using cached packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
      "Collecting numpy>=1.15.0\n",
      "  Using cached numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp38-cp38-manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.0\n",
      "  Using cached thinc-8.0.1-cp38-cp38-manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp38-cp38-manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp38-cp38-manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting typer<0.4.0,>=0.3.0\n",
      "  Using cached typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.56.2-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 123 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp38-cp38-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
      "  Using cached spacy_legacy-3.0.1-py2.py3-none-any.whl (7.0 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.1\n",
      "  Using cached catalogue-2.0.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Using cached pydantic-1.7.3-cp38-cp38-manylinux2014_x86_64.whl (12.2 MB)\n",
      "Collecting pathy\n",
      "  Downloading pathy-0.3.6-py3-none-any.whl (36 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.0\n",
      "  Using cached srsly-2.4.0-cp38-cp38-manylinux2014_x86_64.whl (458 kB)\n",
      "Requirement already satisfied: setuptools in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from spacy>=2.1.0->en-early-modern-places==2.1.0) (50.3.0.post20201006)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB)\n",
      "Collecting pyparsing>=2.0.2\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Processing /home/ajanco/.cache/pip/wheels/11/73/9a/f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2/smart_open-3.0.0-py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.3-py2.py3-none-any.whl (137 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.1.0->en-early-modern-places==2.1.0) (2020.6.20)\n",
      "Building wheels for collected packages: en-early-modern-places\n",
      "  Building wheel for en-early-modern-places (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-early-modern-places: filename=en_early_modern_places-2.1.0-py3-none-any.whl size=11157798 sha256=52b51d3d4a5a5755fbbd9c4068252d07e6c201c827bb543aa0d8cb19c13a51e3\n",
      "  Stored in directory: /home/ajanco/.cache/pip/wheels/04/4e/b4/4b42b3dea42f89e4548ddbc37f59a8b649f54ff68935732eac\n",
      "Successfully built en-early-modern-places\n",
      "Installing collected packages: MarkupSafe, jinja2, pyparsing, packaging, numpy, cymem, murmurhash, preshed, wasabi, catalogue, srsly, blis, pydantic, thinc, click, typer, tqdm, spacy-legacy, urllib3, chardet, idna, requests, smart-open, pathy, spacy, en-early-modern-places\n",
      "Successfully installed MarkupSafe-1.1.1 blis-0.7.4 catalogue-2.0.1 chardet-4.0.0 click-7.1.2 cymem-2.0.5 en-early-modern-places-2.1.0 idna-2.10 jinja2-2.11.3 murmurhash-1.0.5 numpy-1.20.1 packaging-20.9 pathy-0.3.6 preshed-3.0.5 pydantic-1.7.3 pyparsing-2.4.7 requests-2.25.1 smart-open-3.0.0 spacy-3.0.1 spacy-legacy-3.0.1 srsly-2.4.0 thinc-8.0.1 tqdm-4.56.2 typer-0.3.2 urllib3-1.26.3 wasabi-0.8.2\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Collecting standoffconverter\n",
      "  Using cached standoffconverter-0.6.3-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: numpy in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from standoffconverter) (1.20.1)\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.6.2-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "Installing collected packages: lxml, standoffconverter\n",
      "Successfully installed lxml-4.6.2 standoffconverter-0.6.3\n",
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Collecting skosprovider_getty\n",
      "  Downloading skosprovider_getty-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting skosprovider>=0.7.0\n",
      "  Downloading skosprovider-0.7.1-py2.py3-none-any.whl (22 kB)\n",
      "Collecting rdflib\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from skosprovider_getty) (2.25.1)\n",
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hProcessing /home/ajanco/.cache/pip/wheels/54/ad/80/7f945177b0fa71d82335e5f0879c1d680d83184502bfce18a8/language_tags-1.0.0-py2.py3-none-any.whl\n",
      "Collecting pyld\n",
      "  Downloading PyLD-2.0.3.tar.gz (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 9.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting rfc3987\n",
      "  Using cached rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
      "Collecting isodate\n",
      "  Using cached isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: pyparsing in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from rdflib->skosprovider_getty) (2.4.7)\n",
      "Collecting six\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from requests->skosprovider_getty) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from requests->skosprovider_getty) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from requests->skosprovider_getty) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from requests->skosprovider_getty) (2020.6.20)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting cachetools\n",
      "  Using cached cachetools-4.2.1-py3-none-any.whl (12 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting frozendict\n",
      "  Downloading frozendict-1.2.tar.gz (2.6 kB)\n",
      "Requirement already satisfied: lxml in /home/ajanco/anaconda3/envs/gatsby/lib/python3.8/site-packages (from pyld->skosprovider>=0.7.0->skosprovider_getty) (4.6.2)\n",
      "Building wheels for collected packages: pyld, frozendict\n",
      "  Building wheel for pyld (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyld: filename=PyLD-2.0.3-py3-none-any.whl size=70851 sha256=e6dff288aa03a73004ad3b19e30ad147a6f603e24e9978bdc98111fd14c2e870\n",
      "  Stored in directory: /home/ajanco/.cache/pip/wheels/cb/de/17/ee5c1ed9e88ba7a3ed569f79dece719b7ed151db78565a3e06\n",
      "  Building wheel for frozendict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for frozendict: filename=frozendict-1.2-py3-none-any.whl size=3148 sha256=dde3ea86f5c3d02b47cae3b2912e06f718b3be98f7915af6d3b20508ac285c9a\n",
      "  Stored in directory: /home/ajanco/.cache/pip/wheels/9b/9b/56/5713233cf7226423ab6c58c08081551a301b5863e343ba053c\n",
      "Successfully built pyld frozendict\n",
      "Installing collected packages: six, webencodings, html5lib, language-tags, cachetools, frozendict, pyld, rfc3987, skosprovider, isodate, rdflib, skosprovider-getty\n",
      "Successfully installed cachetools-4.2.1 frozendict-1.2 html5lib-1.1 isodate-0.6.0 language-tags-1.0.0 pyld-2.0.3 rdflib-5.0.0 rfc3987-1.3.8 six-1.15.0 skosprovider-0.7.1 skosprovider-getty-0.5.1 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/apjanco/spaCy_workshops/raw/master/Session_6/en_early_modern_places-2.1.0.tar.gz\n",
    "!pip install en_early_modern_places-2.1.0.tar.gz\n",
    "!pip install standoffconverter\n",
    "!pip install skosprovider_getty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y712QWkgzN55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Restart the kernel \n",
    "import spacy\n",
    "nlp = spacy.load(\"en_early_modern_places\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "colab_type": "code",
    "id": "907d8TttzQrE",
    "outputId": "eb0ad4ac-a892-4349-da3f-f58af5fcc6f2"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E167] Unknown morphological feature: 'ConjType' (9141427322507498425). This can happen if the tagger was trained with a different set of morphological features. If you're using a pretrained model, make sure that your models are up to date:\npython -m spacy validate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-015892f50a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m doc = nlp(\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m\"ITEM because that the kings most deare Uncle, the king of Denmarke, Norway & Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring & passage of such forain & strange persons into his realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spacy22/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.set_annotations\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmorphology.pyx\u001b[0m in \u001b[0;36mspacy.morphology.Morphology.assign_tag_id\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mmorphology.pyx\u001b[0m in \u001b[0;36mspacy.morphology.Morphology.add\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E167] Unknown morphological feature: 'ConjType' (9141427322507498425). This can happen if the tagger was trained with a different set of morphological features. If you're using a pretrained model, make sure that your models are up to date:\npython -m spacy validate"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_early_modern_places\")\n",
    "\n",
    "doc = nlp(\n",
    "    \"ITEM because that the kings most deare Uncle, the king of Denmarke, Norway & Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring & passage of such forain & strange persons into his realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods\"\n",
    ")\n",
    "HTML(displacy.render(doc, style=\"ent\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kZMc90pQDQb"
   },
   "source": [
    "There's more information in the TEI than just the place names.  There is also an id number in many of the records for the Getty Thesaurus of Place Names (TGN). If we add an entity_linker pipeline to the model, we will get not only recognition of place, but also of a specific place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "7-gmvTjx4zFd",
    "outputId": "4cc19215-96d7-426c-deb0-db39c88c57b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmlParseEntityRef: no name, line 103, column 75 (<string>, line 103)\n",
      "xmlParseEntityRef: no name, line 199, column 94 (<string>, line 199)\n",
      "xmlParseEntityRef: no name, line 186, column 94 (<string>, line 186)\n",
      "xmlParseEntityRef: no name, line 803, column 109 (<string>, line 803)\n",
      "xmlParseEntityRef: no name, line 455, column 89 (<string>, line 455)\n",
      "xmlParseEntityRef: no name, line 441, column 89 (<string>, line 441)\n",
      "Unescaped '<' not allowed in attributes values, line 22, column 25 (<string>, line 22)\n",
      "xmlParseEntityRef: no name, line 49, column 152 (<string>, line 49)\n",
      "xmlParseEntityRef: no name, line 6, column 152 (<string>, line 6)\n",
      "xmlParseEntityRef: no name, line 4, column 111 (<string>, line 4)\n",
      "xmlParseEntityRef: no name, line 34, column 106 (<string>, line 34)\n",
      "xmlParseEntityRef: no name, line 3, column 149 (<string>, line 3)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle\n",
    "from collections import Counter\n",
    "spec = {\"tei\":\"http://www.tei-c.org/ns/1.0\"}\n",
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "from standoffconverter import Converter\n",
    "\n",
    "def tei_loader(url):\n",
    "    tei = urlopen(url).read()\n",
    "    return etree.XML(tei)\n",
    "\n",
    "table_of_contents_url = \"http://www.perseus.tufts.edu/hopper/xmltoc?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D1\"\n",
    "table_of_contents_xml = tei_loader(table_of_contents_url)\n",
    "\n",
    "\n",
    "chunks = table_of_contents_xml.xpath(\"//chunk[@ref]\")\n",
    "refs = [chunk.get('ref') for chunk in chunks] \n",
    "# an example ref 'Perseus%3Atext%3A1999.03.0070%3Anarrative%3D6'\n",
    "\n",
    "\n",
    "standoffs = []\n",
    "\n",
    "for ref in refs:\n",
    "    try:\n",
    "        url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + ref\n",
    "\n",
    "        tei = tei_loader(url)\n",
    "        so = Converter.from_tree(tei)\n",
    "        standoffs.append(so)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xBFUAOc5N22"
   },
   "outputs": [],
   "source": [
    "# Get the text from the TEI document and create training data\n",
    "import json\n",
    "places = []\n",
    "entities = []\n",
    "place_names = [] \n",
    "place_ids = []\n",
    "names = []\n",
    "ADD_NAMES = False # if True, the dataset will included all markedup names from the TEI\n",
    "ADD_PLACE = True\n",
    "for standoff in standoffs:\n",
    "    for annotation in json.loads(standoff.to_json()):\n",
    "        try:\n",
    "            if annotation['tag'] == 'name' and ADD_NAMES:\n",
    "                begin = annotation['begin']\n",
    "                end = annotation['end']\n",
    "                length = end-begin\n",
    "                sent = standoff.plain[begin-300:end+ 300]\n",
    "                assert len(sent) > 0\n",
    "                begin = 300\n",
    "                end = begin+length\n",
    "                if '\\n' in sent[begin:end]:\n",
    "                    end -= 1\n",
    "                place_names.append(sent[begin:end])\n",
    "                place = (sent, {'entities':[(begin,end,\"NAME\")]})\n",
    "                places.append(place)\n",
    "                \n",
    "            if annotation['attrib']['type'] == 'place' and ADD_PLACE:\n",
    "                begin = annotation['begin']\n",
    "                end = annotation['end']\n",
    "                length = end-begin\n",
    "                key = annotation['attrib']['key']\n",
    "                key = key.split(',')[1]\n",
    "                place_ids.append(key)\n",
    "                #modern_name = annotation['attrib']['reg']\n",
    "                sent = standoff.plain[begin-300:end+ 300]\n",
    "                assert len(sent) > 0\n",
    "                begin = 300\n",
    "                end = begin+length\n",
    "                if '\\n' in sent[begin:end]:\n",
    "                    end -= 1\n",
    "                place_names.append(sent[begin:end])\n",
    "                place = (sent, {'entities':[(begin,end,\"TGN\")]})\n",
    "                places.append(place)\n",
    "                \n",
    "                dict_1 = {(begin, end): {key: 1.0,}}\n",
    "                entities.append((sent, {\"links\": dict_1}))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "47tMnuUL5qnW",
    "outputId": "8a2418d1-bb26-43c3-bb1f-41d3cb601e32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('England', 797),\n",
       " ('America', 290),\n",
       " ('Guiana', 281),\n",
       " ('China', 255),\n",
       " ('Goa', 244),\n",
       " ('Pegu', 220),\n",
       " ('Russia', 212),\n",
       " ('Peru', 202),\n",
       " ('Mosco', 195),\n",
       " ('Russe', 193)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "place_counts = Counter(place_names)\n",
    "# place_counts['England'] == 797\n",
    "place_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agsGk6rq4wW_"
   },
   "outputs": [],
   "source": [
    "kb_entities = {}\n",
    "for entity in entities: \n",
    "    id = entity[1]['links']\n",
    "    start, end = list(id.keys())[0]\n",
    "    word = entity[0][start:end]\n",
    "    frequency = place_counts[word]\n",
    "    place_id = list(id[(start,end)].keys())\n",
    "    kb_entities[place_id[0]] = (word, frequency)\n",
    "\n",
    "    \n",
    "result = {}\n",
    "\n",
    "for key,value in kb_entities.items():\n",
    "    if value not in result.values():\n",
    "        result[key] = value\n",
    "\n",
    "kb_entities = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "dEmNhSYn6bjy",
    "outputId": "427ac240-540c-4afe-b5cc-99ac41c2226e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-23 13:16:02--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:16a6, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  11.3MB/s    in 59s     \n",
      "\n",
      "2020-02-23 13:17:02 (11.0 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n",
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "!unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZJOo0paNCuV"
   },
   "source": [
    "spaCy creates entity vectors and requires a model with vectors so let's add them.  Here we'll add Facebook's Fasttext vectors.  We could also easily add Stanford's Glove vectors with the large English model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "215438171bb34ebe86985971f5e225ca",
      "9b290c1bbc1a47e783fd93e67144e474",
      "45fa9eaa75ff4dc288e4979cfa1599fc",
      "9353da952cd94ea9b09925f4da0c4163",
      "2795f99bbaa049bfa1abfe628a4c40dd",
      "35bdd366937741f2bdb0c64a7b65552f",
      "d1304bd1255d49aeb70678d6e2d720fa",
      "a10a599e5b6f4310aac298df4ebb1d7a"
     ]
    },
    "colab_type": "code",
    "id": "x5xmf0IT6mZh",
    "outputId": "0efe5d69-7009-4458-a677-4f565348fddd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajanco/anaconda3/envs/spacy21/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8c05a87f40497db66d132a0f004cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999994.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "path_to_cc_XX_300_vec = \"wiki-news-300d-1M.vec\"\n",
    "\n",
    "nlp = spacy.load(\"en_early_modern_places\")\n",
    "\n",
    "with open(path_to_cc_XX_300_vec, 'rb') as file_:\n",
    "    header = file_.readline()\n",
    "    nr_row, nr_dim = header.split()\n",
    "    nlp.vocab.reset_vectors(width=int(nr_dim))\n",
    "    for line in tqdm(file_, total=999994):\n",
    "        line = line.rstrip().decode('utf8')\n",
    "        pieces = line.rsplit(' ', int(nr_dim))\n",
    "        word = pieces[0]\n",
    "        vector = np.asarray([float(v) for v in pieces[1:]], dtype='f')\n",
    "        nlp.vocab.set_vector(word, vector)  # add the vectors to the vocab\n",
    "\n",
    "nlp.to_disk('places_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "D9tnjeFbT5yj",
    "outputId": "83762a54-c0ea-46c7-c4f8-4d56b1d19ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'places_vectors'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajanco/anaconda3/envs/spacy21/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ajanco/anaconda3/envs/spacy21/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 nan\n",
      "Trained on 721 entities across 5 epochs\n",
      "Final loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajanco/anaconda3/envs/spacy21/lib/python3.7/runpy.py:193: UserWarning: [W017] Alias 'Bergen' already exists in the Knowledge base.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "721 kb entities: ['7011546', '1000144', '7015155', '1014466', '7014300', '7013071', '7010744', '2540217', 'Carthage', '1023771', '7014673', '7005903', '1121113', '1000090', '7016768', '1000056', '7017072', '2050214', '7011961', '7007652', '7004456', '1009213', '1007394', '7006190', '1127666', '7011374', '7010547', '1136549', '7011380', '1125922', '7005064', '7002759', '7007664', '7002354', '7009120', '1141024', '1055512', '7011385', '1002308', '7005554', '1020948', '7010028', '6006673', '4003876', '1130786', '7015528', '7002351', '7004109', '7000645', '7006278', '7016548', '7008157', '7009002', '4008282', '7000630', '7007127', '7005286', '7002444', '7004545', '6000442', '1045359', 'Placentia', '7013300', '1127524', '1130850', '7010874', '7005468', '7009090', '7006653', '7016845', '7003387', '7011024', '7012974', '1000160', '1062347', '7015156', '7011953', '1066243', '7012981', '1046884', '1020019', '1136825', '1000149', '7011173', '2226898', '7005019', '1001657', '1000047', '1121336', '7003085', '7008116', '1046908', '1124996', '7002842', '7016752', '1125234', '7001244', '1126144', '1038548', '7001810', '7002673', '7005801', '7004789', '7008209', '4002381', '7010743', '7010911', '7013008', '7008772', '1089785', '7002791', '2179499', '7008154', '7004100', '7016659', '1123650', '7005645', '2140424', '7014164', '1099922', '7007850', '7012209', '7002345', '1131414', '7008769', '7002861', '7006470', '7015391', '7003070', '1000180', '7001160', '7000084', '7007796', '1122702', '7008745', '1126760', '1007985', '7007959', '7005819', '7007975', '7003897', '1049813', '1083300', '7007483', '2645040', '7016631', '7011724', '1090310', '7011868', '7000989', '7010380', '1124903', '7001371', '7005428', '1017980', '7015447', '7002850', '1037369', '7002710', '1044888', '7007656', '1000074', '1021866', '7011174', '7010334', '2615756', '2773414', '7010249', '7004660', '7003870', '2131893', '1123466', '7005328', '1032038', '7013967', '7010992', '7002886', '1094266', '7013870', '7011362', '2208195', '1092429', '1061822', '7003695', '1032887', '6003016', '1038110', '1009021', '1021639', '7002681', '7000198', '7010979', '7008495', '1036431', '7007961', '7009907', '7011240', '7005371', '7013051', '1014078', '1034729', '7013345', '7004550', '7010956', '7001519', '7011320', '6000030', '7011969', '7002373', '7012606', '7018209', '7000261', 'Capua', '4007603', '1003757', '5003176', '7003833', '7014789', '7007426', '7006417', '7010321', '7014378', '7007641', '1000063', '7011614', '7011025', '1009589', '7002787', '7002052', '1000598', '7001315', '7012032', '7002987', '7002277', '1029272', '7011548', '1121214', '1023744', '4012700', '1000110', '1062614', '1129139', '7018159', '1126809', '1000004', '7011332', '1061747', '7017076', '7001164', '7010415', '1049207', '1046784', '7010978', '4004589', '4001969', '2118187', '6005523', 'Veii', '7011273', '2651104', '1012986', '7011091', '7016777', '2244180', '7005704', '7005074', '7010413', '1029317', '7001362', '1040400', '7006651', '7001460', '1006284', '7003254', '1061969', '7001215', '7011247', '7002849', '7017464', '1000112', '7001083', '7008139', '7010497', '1000173', '7001019', '7011420', '7005022', '7004549', '1123067', '1061954', '7004717', '4011756', '1024987', '1047542', '6005315', '1018948', '7018236', '1088304', '7001303', '1000062', '7015562', '7003704', '7004540', '2379727', '7001393', '1036541', '7013929', '1127477', '7009977', '2638158', '7012045', '1108814', '7008153', 'Rome', '7008741', '1123842', '7011807', '7001524', '1004427', '6004345', '7018923', '1007228', '1022063', '7002443', '7007649', '7017886', '7018161', '1044422', '7010869', '1062058', '2640408', '7001390', '7007157', '1044388', '7000231', '7004474', '1001409', '1130319', '7002470', '7011324', '7001410', '7003788', '7002638', '1000120', '7011907', '1124116', '1121151', '7004624', '7002338', '7007240', '7005233', '1000111', '7016567', '7003635', '7017965', '7011789', '7003122', '7016612', '7018905', '1046380', '1000181', '7004640', '1000576', '1042258', '7014825', '1031012', '2399218', '7006366', 'Florentia', '7006154', '2331687', '1000091', '7005258', '7014218', '1061781', '7002499', '1000140', '2346330', '7008211', '7013079', '7005685', '7012077', '7012806', '1123294', '1007109', '1003942', '1028790', '7000489', '7010799', '7007919', '7011198', '1000135', '1000059', '1007114', '7011359', '7005090', '2713297', '1015337', '7016716', '1019088', '7015931', '7010345', '1000172', '1043968', '7002835', '7018280', '7007590', '1047376', '1000142', '1127714', '7008596', '7005845', '7001369', '7007552', '7011805', '7002830', '1000088', '7012005', '1007210', '7004942', '1023429', '7011445', '7013645', '7016893', '1019762', '7010922', '7010011', '1009212', '7004641', '1131582', '7010804', '7012349', '7006667', '1000080', '7011987', '7001560', '7014579', '7005562', '1000054', '7003479', '1026748', '7012149', '7010720', '1048431', '7000457', '7005128', '7002847', '7005973', '7011071', '1127457', '7008165', '1064382', '2600042', '1030532', '7003975', '7009861', '7004771', '7007967', '2731762', '1091139', '1020513', '1063983', '7003462', '1136751', '7010824', '2766219', '7015484', '1000581', '1004441', '7014523', '7006453', '7010532', '7002261', '1062541', '7009395', '7012294', 'Brundusium', '7007856', '1121195', '1000167', '7008844', '6005374', '7008095', '7010734', '7005426', '7001865', '1001257', '7010662', '1129174', '7006413', '1062244', '1049680', '1000095', '1001424', '1049026', '1008753', '1065147', '2524925', '7004264', '7002813', '1000205', '7010221', '7002358', '7002784', '7015549', '1013871', '7002445', '7006809', '7011048', '7012813', '7008038', '7016833', '7006745', '7010986', '1036997', '7008008', '7018638', '1030082', '1124006', '1037904', '7010726', '2050948', '1024198', '2510647', '7006798', '7008771', '7003121', '1014294', '7001385', '7006656', '7013255', '7008765', '7014272', '7008468', '7011800', '2045107', '1049789', '2621282', '7012056', '1000003', '7007720', 'Cales', '1025084', '7005300', '7015451', '4004801', '1030388', '7005729', 'Mantua', '7001093', '7003836', '1000097', '2639930', '1017213', '7013346', '5003109', '1045246', '7010782', '1019516', '7008546', '7010640', '7001186', '1000069', '7016999', '1062407', '2424233', '2716479', '7006484', '7008510', '1000176', '1122114', '7017069', '7008910', '7011656', '7010817', '2276799', '7018224', '7002815', '1000118', '4001288', '1125419', '1029155', '2199209', '1128848', '1121852', '7001407', '1131280', '1017110', '7016816', '7006804', '7011781', '7016662', '7008653', '7002804', '7015350', '7013964', '7017998', '7012772', '7001800', '1132092', '7011995', '1009410', '1006597', '7001201', '1039426', '2682716', '7016693', '7004300', '7004394', '1012700', '1046218', '1049851', '1000051', '1127251', '1043116', '1043551', '7010886', '1000107', '1121447', '1001630', '7013687', '7012892', '2232721', '7006952', '7008180', '7010323', '2398880', '2338854', '7008360', '7012327', '6004687', '7002276', '7007746', '6005262', '7005556', '1000119', '7004548', '1122163', '1075032', '7000549', '7004068', '7012913', '7002326', '7001283', '7010707', '7010042', '1123923', '1062165', '7017236', '1126570', '2066659', '7009053', '7012914', '1008599', '7000442', '7002533', '1126030', '1091890', '7004089', '7006521', '1025086', '7006666', '7011444', '7004719', '7002817', '7005359', '7002435', '1031166', '7001388', '7011913', '7013701', '1064660', '1125225', '7011393', '1025776', '7001880', '1046376', '2694012', '7004334', '7008589', '1099876', '1136422', '7001313', 'Beneventum', '7013352', '1044997', '7006800', '7008042', '1123427', '7002862', '7002677', '7005560', '7006542', '1124748', '1049694', '1001925', '7008131', '7010948', '7002613', '7011301', '7013416', '7011304', '1014154', '7010719', '7012026', '1043838', '7008787', '7010427', '7010805', 'Genua', '7007470', '1000078', '7010539', '7018668', '1000158', '7003834', '1023908', '7001294', '7016619', '7003820', '1007724']\n",
      "719 kb aliases: ['Breme', 'Nova Zembla', 'Lituania', 'Flanders', 'Citta', 'Charing', 'Quito', 'Portsmouth', 'Lapland', 'Greenfield', 'Secota', 'Waltham', 'Thracia', 'Carthage', 'Varna', 'Denia', 'Ocombe', 'Benin', 'Coimbra', 'Neuburg', 'Tipton', 'Orkney', 'Ascalon', 'Negro', 'Arras', 'Lahor', 'Azores', 'Goletta', 'Poperinge', 'Negroponte', 'Atacama', 'Santo', 'Portugal', 'Thebes', 'Chepstow', 'Bonavista', 'Calabria', 'Patras', 'Brandon', 'Canne', 'Cartagena', 'Coquimbo', 'Italy', 'Magdeburg', 'Arabia', 'Dessie', 'Essex', 'Jordan', 'Malaca', 'Galatia', 'Quero', 'Dion', 'Zamora', 'Plymouth', 'Provence', 'Mansfield', 'Samos', 'Delos', 'Ponte', 'America', 'Harvey', 'Porto', 'Landsberg', 'Ireland', 'Tiber', 'Hellespont', 'Teneriffe', 'Coswig', 'Hopewell', 'Macao', 'Virginia', 'Malacca', 'Caorle', 'Juca', 'York', 'Almagro', 'Pasto', 'Dorsetshire', 'Placentia', 'Graciosa', 'Modon', 'Sian', 'Copiapo', 'Cuba', 'Laguna', 'Patan', 'Murr', 'Cappadocia', 'Rostock', 'Lemnos', 'Ramea', 'Cyprus', 'Bever', 'Mina', 'Tanais', 'Antioch', 'Constantina', 'Florida', 'Emden', 'Syria', 'Iberus', 'Mozambique', 'Castro', 'Cremia', 'Alva', 'Ethiopia', 'Stolp', 'Parenzo', 'Tana', 'Dabo', 'Granada', 'Sarmiento', 'Pico', 'Hainan', 'Brasilia', 'Newcastle', 'Ordas', 'Caria', 'Ferrara', 'Alger', 'Russe', 'Costa', 'Europe', 'Assa', 'Eltham', 'Kingston', 'Winterton', 'Northumberland', 'Algarve', 'Turchia', 'Bruton', 'Cali', 'Messina', 'Danubius', 'Cordova', 'Ceylon', 'Albania', 'Lubeck', 'The South', 'Cracovia', 'Isla', 'Attica', 'Nubia', 'Bara', 'Smerwick', 'Hama', 'Martinez', 'Cieza', 'Caucasus', 'Parthia', 'Neumark', 'Angra', 'Savona', 'Austria', 'Bugia', 'Nere', 'Hungary', 'Mundo', 'Vistula', 'Wismar', 'Galicia', 'Fowey', 'Buda', 'Kasan', 'Vilna', 'Labrador', 'Sandridge', 'Merida', 'Sandy Island', 'Lister', 'Finland', 'Westphalia', 'Greenwich', 'Lubec', 'Lunenburg', 'Bayon', 'Deptford', 'Sigillo', 'Kendal', 'Blanco', 'Japon', 'Sava', 'Santiago', 'Indus', 'Sodom', 'Bactria', 'Tarnow', 'Oxus', 'Dartmouth', 'Arica', 'Tauris', 'Saul', 'Nicaragua', 'Conde', 'Cumberland', 'Grigno', 'Alcantara', 'Asia', 'Phoenicia', 'Vera', 'Savoy', 'Amsterdam', 'Ghinea', 'Siam', 'Egerton', 'Russia', 'Torre', 'Florence', 'Zerbst', 'Conquerabia', 'Campeche', 'Novibazar', 'Romania', 'Yarmouth', 'Worcester', 'Milford', 'Dunkerk', 'Leigh', 'Germany', 'Hierro', 'Memel', 'Jericho', 'Ribadeo', 'Frio', 'Hans', 'Achim', 'Henauld', 'Assur', 'Milos', 'Silesia', 'Machico', 'Caiphas', 'Tigris', 'Paphos', 'Capua', 'Hage', 'Pina', 'Cherbourg', 'Yron', 'Vaga', 'Lambeth', 'Taunton', 'Vitte', 'Malta', 'Sebenico', 'Jerico', 'Limehouse', 'Holland', 'Norway', 'Corinth', 'Antwerp', 'Andros', 'Bourne', 'Mendoza', 'Whitehead', 'Agira', 'Augusta', 'Belgern', 'Devon', 'Ormus', 'Ivangorod', 'Esperanza', 'Hochelaga', 'Greece', 'Stamford', 'Volga', 'Dominica', 'Wittenberg', 'Cilicia', 'Morea', 'Tanger', 'Caffa', 'Sarmatia', 'Honan', 'Arra', 'Baye', 'Preston', 'Moncada', 'Euphrates', 'Veii', 'Israel', 'Samaria', 'Normandie', 'Judea', 'Corvo', 'Majorca', 'Bruno', 'Alpes', 'Hampshire', 'Arundel', 'Cumana', 'Antilles', 'Sesto', 'Antigua', 'Davis', 'Caen', 'Nicaria', 'Leon', 'Naumburg', 'Humber', 'Amida', 'Pescadores', 'S. Michael\\nArchange', 'Willis', 'Tyre', 'Vologda', 'Verzino', 'Fenton', 'Leiden', 'Rose', 'Bedford', 'Troy', 'Avellaneda', 'Elvas', 'Parma', 'Elbing', 'Salamanca', 'Harwich', 'Kara', 'Cascais', 'Famagusta', 'Latinum', 'Cuzco', 'Bristol', 'Sluys', 'Valona', 'Venta', 'Erle', 'Rome', 'Trent', 'Bermudas', 'Navarre', 'Este', 'Moluccas', 'Britain', 'Patmos', 'Peru', 'Jamaica', 'Burley', 'Christchurch', 'Bilbao', 'Tripoli', 'Georgia', 'Waverley', 'Lincolnshire', 'Helena', 'Dane', 'Bayona', 'Corfu', 'Marco', 'Castello', 'Elbe', 'Oviedo', 'Bonna', 'Reichenbach', 'Mano', 'Taylors', 'Corsica', 'Lissa', 'Lesina', 'Gravesend', 'Lepanto', 'Stora', 'Buena', 'St. Ives', 'Praia', 'Portland', 'Blankenberg', 'Saint Michael', 'Phenix', 'Inga', 'Reyes', 'Cephalonia', 'Argos', 'Whitehall', 'Sirvan', 'Ambrose', 'Nazareth', 'Celebes', 'Watchet', 'Gard', 'Florentia', 'Curzola', 'California', 'Antioche', 'London', 'Aragon', 'Totnes', 'Gaza', 'Constantine', 'Gillingham', 'The Island', 'Norton', 'Stafford', 'Brabant', 'Castile', 'Prussia', 'Mestre', 'Jemen', 'Jaffa', 'Pueblo', 'Tanda', 'Podolia', 'Dalmatia', 'Riga', 'Padstow', 'Delft', 'Dort', 'Calicut', 'Woolwich', 'Bosna', 'Jabel', 'Johnson', 'Nesse', 'Barca', 'Doro', 'Madagascar', 'Eisleben', 'Popayan', 'Gato', 'Naples', 'Baya', 'Bonaventure', 'Gilan', 'Cairo', 'Colombo', 'Pulo', 'Frankford', 'Roanoac', 'Bulgaria', 'Newfoundland', 'Middleburgh', 'Libya', 'Martins', 'Palestina', 'Barbuda', 'Soria', 'Pegu', 'Lydia', 'China', 'Mexico', 'Lewes', 'Lindsey', 'Anegada', 'Guiana', 'Roch', 'Thames', 'Muel', 'Biscay', 'Wales', 'Istria', 'Sonsonate', 'Northumbria', 'Arrecife', 'Curacao', 'Berg', 'Galway', 'Sion', 'Numidia', 'Chapman', 'Java', 'Guadalupe', 'Newland', 'Bohemia', 'Schiedam', 'Soca', 'Clifton', 'Spain', 'Falmouth', 'Bendzin', 'Castel', 'Scythia', 'Jena', 'Peniche', 'Zetland', 'Gran', 'Armenia', 'Aruba', 'Brundusium', 'Westminster', 'Gorlitz', 'Lisbon', 'Luna', 'Valladolid', 'Acari', 'Exeter', 'Northampton', 'Goa', 'Nairi', 'Triana', 'Hassan', 'Jerusalem', 'Athens', 'Gomera', 'Vigo', 'Matanzas', 'Fogo', 'Colmogro', 'Anderson', 'Fili', 'North sea', 'Philippinas', 'Bethleem', 'Arda', 'Ancre', 'Sicilia', 'Levant', 'Staden', 'Anjou', 'Borneo', 'Monte', 'Pola', 'Persia', 'Alicante', 'Cambridge', 'Piscaria', 'Alderney', 'Prut', 'Weymouth', 'Olinda', 'Coro', 'Causey', 'Newport', 'Cham', 'Ladrones', 'Mana', 'Pastrana', 'Lima', 'Russell', 'Hereford', 'Perm', 'Westside', 'Lech', 'Pontus', 'Lancaster', 'Yucatan', 'Manta', 'Cales', 'Ragusa', 'Siberia', 'Moore', 'Egypt', 'Congo', 'Nuremberg', 'Munster', 'Brava', 'Mantua', 'Sophia', 'Tunis', 'Mayo', 'Stratford', 'Madrid', 'Fleming', 'Stone', 'Lein', 'Calais', 'Belgio', 'Candia', 'Abydos', 'Paris', 'Chester', 'Canow', 'Lillo', 'Zanzibar', 'Mutare', 'Acapulco', 'White lake', 'Wiltshire', 'Cerigo', 'Lago', 'Grimston', 'Honduras', 'Cadiz', 'Barcelona', 'Cienaga', 'Kent', 'Zante', 'Lyon', 'Corna', 'Urie', 'Baltimore', 'Rone', 'Sampson', 'Caracas', 'Damas', 'Estremadura', 'Cortes', 'Minorca', 'Groenland', 'Newhaven', 'Solor', 'Norwich', 'Viana', 'Mesopotamia', 'Middleton', 'Mauritania', 'Alps', 'Rosa', 'Lucon', 'Apulia', 'Bere', 'Santos', 'Armine', 'Trinidad', 'Solis', 'Guarda', 'Macedonia', 'Cape Saint Vincent', 'Hord', 'Crawford', 'Zeland', 'Genoa', 'Richmond', 'Venezuela', 'Memphis', 'Parana', 'Standish', 'Pachuca', 'Eden', 'Poland', 'Smithfield', 'Babylonia', 'Canada', 'Gibraltar', 'Redonda', 'Angola', 'Mosco', 'Tulu', 'Sombrero', 'Lycia', 'Stevens', 'East sea', 'Ostend', 'Tucuman', 'Salina', 'Allen', 'Sala', 'Zara', 'Sestos', 'Timor', 'Saxon', 'Milan', 'Bizerta', 'Padua', 'Toledo', 'Niger', 'Guadalcanal', 'Brasil', 'Caplan', 'Telde', 'Toro', 'England', 'Sweden', 'Formentera', 'Olivet', 'Clifford', 'Havana', 'Boston', 'Bonito', 'Ceuta', 'Newton', 'Venice', 'Southside', 'Orwell', 'Sardinia', 'Ephesus', 'Austin', 'Tarento', 'Hampton', 'Landeck', 'Mona', 'Alexandretta', 'Sidon', 'Beneventum', 'Cochin', 'Ninive', 'Vago', 'Nicosia', 'Radcliffe', 'Pomerania', 'Cornwall', 'Livonia', 'Gallipoli', 'Scotland', 'Vermilion', 'Mors', 'Chari', 'Mecca', 'Southampton', 'Bona', 'Moldavia', 'Tumen', 'India', 'Cabo', 'Suez', 'Almeida', 'Phrygia', 'Guzman', 'Cassine', 'Somerset', 'Bethlehem', 'Waini', 'Genua', 'Morris', 'Flores', 'Leicester', 'Cannes', 'Bridgewater', 'Belgrade', 'Juba', 'Columbus', 'Vienna', 'Concepcion', 'Mito', 'Corno', 'Stockholm', 'Cayo', 'Valencia', 'Negrais', '']\n",
      "\n",
      "Saved KB to tgn_kb/kb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocab to tgn_kb/vocab\n"
     ]
    }
   ],
   "source": [
    "from spacy.vocab import Vocab\n",
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "from pathlib import Path\n",
    "\n",
    "from bin.wiki_entity_linking.train_descriptions import EntityEncoder\n",
    "\n",
    "\n",
    "ENTITIES = kb_entities # {\"Q2146908\": (\"American golfer\", 342), \"Q7381115\": (\"publisher\", 17)}\n",
    "\n",
    "INPUT_DIM = 300  # dimension of pretrained input vectors\n",
    "DESC_WIDTH = 64  # dimension of output entity vectors\n",
    "\n",
    "\n",
    "def main(model=None, output_dir=None, n_iter=50):\n",
    "    \"\"\"Load the model, create the KB and pretrain the entity encodings.\n",
    "    If an output_dir is provided, the KB will be stored there in a file 'kb'.\n",
    "    The updated vocab will also be written to a directory in the output_dir.\"\"\"\n",
    "\n",
    "    nlp = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "\n",
    "    # check the length of the nlp vectors\n",
    "    if \"vectors\" not in nlp.meta or not nlp.vocab.vectors.size:\n",
    "        raise ValueError(\n",
    "            \"The `nlp` object should have access to pretrained word vectors, \"\n",
    "            \" cf. https://spacy.io/usage/models#languages.\"\n",
    "        )\n",
    "\n",
    "    kb = KnowledgeBase(vocab=nlp.vocab,entity_vector_length=64)\n",
    "\n",
    "    # set up the data\n",
    "    entity_ids = []\n",
    "    descriptions = []\n",
    "    freqs = []\n",
    "    for key, value in ENTITIES.items():\n",
    "        desc, freq = value\n",
    "        entity_ids.append(key)\n",
    "        descriptions.append(desc)\n",
    "        freqs.append(freq)\n",
    "\n",
    "    # training entity description encodings\n",
    "    # this part can easily be replaced with a custom entity encoder\n",
    "    encoder = EntityEncoder(\n",
    "        nlp=nlp,\n",
    "        input_dim=INPUT_DIM,\n",
    "        desc_width=DESC_WIDTH,\n",
    "        #epochs=n_iter,\n",
    "    )\n",
    "    encoder.train(description_list=descriptions, to_print=True)\n",
    "\n",
    "    # get the pretrained entity vectors\n",
    "    embeddings = encoder.apply_encoder(descriptions)\n",
    "\n",
    "    # set the entities, can also be done by calling `kb.add_entity` for each entity\n",
    "    kb.set_entities(entity_list=entity_ids, freq_list=freqs, vector_list=embeddings)\n",
    "\n",
    "    # adding aliases, the entities need to be defined in the KB beforehand    \n",
    "    for key in kb_entities.keys():\n",
    "        kb.add_alias(\n",
    "            alias = kb_entities[key][0],\n",
    "            entities = [key],\n",
    "            probabilities=[1.0]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # test the trained model\n",
    "    print()\n",
    "    _print_kb(kb)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        kb_path = str(output_dir / \"kb\")\n",
    "        kb.dump(kb_path)\n",
    "        print()\n",
    "        print(\"Saved KB to\", kb_path)\n",
    "\n",
    "        vocab_path = output_dir / \"vocab\"\n",
    "        kb.vocab.to_disk(vocab_path)\n",
    "        print(\"Saved vocab to\", vocab_path)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def _print_kb(kb):\n",
    "    print(kb.get_size_entities(), \"kb entities:\", kb.get_entity_strings())\n",
    "    print(kb.get_size_aliases(), \"kb aliases:\", kb.get_alias_strings())\n",
    "\n",
    "\n",
    "main(model=\"places_vectors\",output_dir=\"./tgn_kb\",n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "6I-BoNK74oH9",
    "outputId": "4744beeb-bc5c-4dc8-aa1b-7e46bffcc7f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model with vocab from 'tgn_kb/vocab'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajanco/anaconda3/envs/spacy21/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ajanco/anaconda3/envs/spacy21/lib/python3.7/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 nan\n",
      "Trained on 721 entities across 5 epochs\n",
      "Final loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajanco/anaconda3/envs/spacy21/lib/python3.7/runpy.py:193: UserWarning: [W017] Alias 'Bergen' already exists in the Knowledge base.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 7003463 from training because it is not in the KB.\n",
      "Removed 1026647 from training because it is not in the KB.\n",
      "Removed 1026647 from training because it is not in the KB.\n",
      "Removed 7018004 from training because it is not in the KB.\n",
      "Removed 7018004 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7011375 from training because it is not in the KB.\n",
      "Removed 7011375 from training because it is not in the KB.\n",
      "Removed 7011375 from training because it is not in the KB.\n",
      "Removed 7011375 from training because it is not in the KB.\n",
      "Removed 7011375 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7011375 from training because it is not in the KB.\n",
      "Removed 7011375 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001532 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 1008469 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 1008469 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1008469 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1008469 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1008469 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7016796 from training because it is not in the KB.\n",
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7016796 from training because it is not in the KB.\n",
      "Removed 1091336 from training because it is not in the KB.\n",
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7016796 from training because it is not in the KB.\n",
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7016796 from training because it is not in the KB.\n",
      "Removed 1091336 from training because it is not in the KB.\n",
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7016796 from training because it is not in the KB.\n",
      "Removed 1091336 from training because it is not in the KB.\n",
      "Removed 7002805 from training because it is not in the KB.\n",
      "Removed 1007070 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 1030377 from training because it is not in the KB.\n",
      "Removed 1030377 from training because it is not in the KB.\n",
      "Removed 7000463 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7000463 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7000463 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 7001242 from training because it is not in the KB.\n",
      "Removed 1082660 from training because it is not in the KB.\n",
      "0 Losses {'entity_linker': nan}\n",
      "\n",
      "Saved model to tgn_kb_1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compatible with: spaCy v2.2.3\n",
    "Last tested with: v2.2.3\n",
    "https://spacy.io/usage/training#entity-linker\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from spacy.symbols import PERSON\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import minibatch, compounding\n",
    "from bin.wiki_entity_linking.train_descriptions import EntityEncoder\n",
    "\n",
    "INPUT_DIM = 300  # dimension of pretrained input vectors\n",
    "DESC_WIDTH = 64  # dimension of output entity vectors\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = places\n",
    "TRAIN_DATA_ENTS = entities\n",
    "ENTITIES = kb_entities\n",
    "n_iter=1\n",
    "kb_path=\"tgn_kb/kb\"\n",
    "vocab_path=\"tgn_kb/vocab\"\n",
    "output_dir=\"./tgn_kb_1\"\n",
    "\n",
    "\n",
    "\"\"\"Create a blank model with the specified vocab, set up the pipeline and train the entity linker.\n",
    "The `vocab` should be the one used during creation of the KB.\"\"\"\n",
    "vocab = Vocab().from_disk(vocab_path)\n",
    "# create blank Language class with correct vocab\n",
    "nlp = spacy.load('places_vectors')\n",
    "#nlp = spacy.blank(\"en\", vocab=vocab)\n",
    "nlp.vocab.vectors.name = \"spacy_pretrained_vectors\"\n",
    "print(\"Created blank 'en' model with vocab from '%s'\" % vocab_path)\n",
    "\n",
    "# Add a sentencizer component. Alternatively, add a dependency parser for higher accuracy.\n",
    "if not 'sentencizer' in nlp.pipe_names:\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "# Add a custom component to recognize \"Russ Cochran\" as an entity for the example training data.\n",
    "# Note that in a realistic application, an actual NER algorithm should be used instead.\n",
    "ruler = EntityRuler(nlp)\n",
    "patterns = []\n",
    "for text, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        start = ent[0]\n",
    "        end = ent[1]\n",
    "        label_ = ent[2]\n",
    "        word = text[start:end]\n",
    "        row = {\"label\":label_, \"pattern\":word}\n",
    "        patterns.append(row)    \n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler)\n",
    "    \n",
    "\n",
    "# Create the Entity Linker component and add it to the pipeline.\n",
    "if \"entity_linker\" not in nlp.pipe_names:\n",
    "    # use only the predicted EL score and not the prior probability (for demo purposes)\n",
    "    cfg = {\"incl_prior\": False}\n",
    "    entity_linker = nlp.create_pipe(\"entity_linker\", cfg)\n",
    "    kb = KnowledgeBase(vocab=nlp.vocab,entity_vector_length=64)\n",
    "\n",
    "    # set up the data\n",
    "    entity_ids = []\n",
    "    descriptions = []\n",
    "    freqs = []\n",
    "    for key, value in ENTITIES.items():\n",
    "        desc, freq = value\n",
    "        entity_ids.append(key)\n",
    "        descriptions.append(desc)\n",
    "        freqs.append(freq)\n",
    "\n",
    "    # training entity description encodings\n",
    "    # this part can easily be replaced with a custom entity encoder\n",
    "    encoder = EntityEncoder(\n",
    "        nlp=nlp,\n",
    "        input_dim=INPUT_DIM,\n",
    "        desc_width=DESC_WIDTH,\n",
    "        #epochs=n_iter,\n",
    "    )\n",
    "    encoder.train(description_list=descriptions, to_print=True)\n",
    "\n",
    "    # get the pretrained entity vectors\n",
    "    embeddings = encoder.apply_encoder(descriptions)\n",
    "\n",
    "    # set the entities, can also be done by calling `kb.add_entity` for each entity\n",
    "    kb.set_entities(entity_list=entity_ids, freq_list=freqs, vector_list=embeddings)\n",
    "\n",
    "    # adding aliases, the entities need to be defined in the KB beforehand    \n",
    "    for key in kb_entities.keys():\n",
    "        kb.add_alias(\n",
    "            alias = kb_entities[key][0],\n",
    "            entities = [key],\n",
    "            probabilities=[1.0]\n",
    "        )\n",
    "    \n",
    "    #print(\"Loaded Knowledge Base from '%s'\" % kb_path)\n",
    "    entity_linker.set_kb(kb)\n",
    "    nlp.add_pipe(entity_linker, last=True)\n",
    "\n",
    "# Convert the texts to docs to make sure we have doc.ents set for the training examples.\n",
    "# Also ensure that the annotated examples correspond to known identifiers in the knowlege base.\n",
    "kb_ids = nlp.get_pipe(\"entity_linker\").kb.get_entity_strings()\n",
    "TRAIN_DOCS = []\n",
    "for text, annotation in TRAIN_DATA_ENTS:\n",
    "    with nlp.disable_pipes(\"entity_linker\"):\n",
    "        doc = nlp(text)\n",
    "    annotation_clean = annotation\n",
    "    for offset, kb_id_dict in annotation[\"links\"].items():\n",
    "        new_dict = {}\n",
    "        for kb_id, value in kb_id_dict.items():\n",
    "            if kb_id in kb_ids:\n",
    "                new_dict[kb_id] = value\n",
    "            else:\n",
    "                print(\n",
    "                    \"Removed\", kb_id, \"from training because it is not in the KB.\"\n",
    "                )\n",
    "        annotation_clean[\"links\"][offset] = new_dict\n",
    "    TRAIN_DOCS.append((doc, annotation_clean))\n",
    "\n",
    "# get names of other pipes to disable them during training\n",
    "pipe_exceptions = [\"entity_linker\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train entity linker\n",
    "    # reset and initialize the weights randomly\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DOCS)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DOCS, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            try:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                    sgd=optimizer,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                pass\n",
    "        print(itn, \"Losses\", losses)\n",
    "\n",
    "\n",
    "\n",
    "# save model to output directory\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print()\n",
    "    print(\"Saved model to\", output_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "RWsx-EZ_9LF2",
    "outputId": "8e46193d-da29-4a63-a0fb-79dc66020dc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entities [('Flanders', 'PLACE', '7018236'), ('Portugal', 'PLACE', '1000090'), ('Chapter', 'PLACE', 'NIL'), ('Portugal', 'PLACE', '1000090'), ('England', 'PLACE', '7002445')]\n",
      "Tokens [('and', '', ''), ('brede', '', ''), ('.', '', ''), ('\\n', '', ''), ('What', '', ''), ('hath', '', ''), ('then', '', ''), ('Flanders', 'PLACE', '7018236'), (',', '', ''), ('bee', '', ''), ('Flemings', '', ''), ('lieffe', '', ''), ('or', '', ''), ('loth', '', ''), (',', '', ''), ('\\n', '', ''), ('But', '', ''), ('a', '', ''), ('little', '', ''), ('Mader', '', ''), ('and', '', ''), ('Flemish', '', ''), ('Cloth', '', ''), (':', '', ''), ('\\n', '', ''), ('By', '', ''), ('Drapering', '', ''), ('of', '', ''), ('our', '', ''), ('wooll', '', ''), ('in', '', ''), ('substance', '', ''), ('\\n', '', ''), ('Liven', '', ''), ('her', '', ''), ('commons', '', ''), (',', '', ''), ('this', '', ''), ('is', '', ''), ('her', '', ''), ('governance', '', ''), (',', '', ''), ('\\n', '', ''), ('Without', '', ''), ('wich', '', ''), ('they', '', ''), ('may', '', ''), ('not', '', ''), ('live', '', ''), ('at', '', ''), ('ease', '', ''), ('.', '', ''), ('\\n', '', ''), ('Thus', '', ''), ('must', '', ''), ('hem', '', ''), ('sterve', '', ''), (',', '', ''), ('or', '', ''), ('with', '', ''), ('us', '', ''), ('must', '', ''), ('have', '', ''), ('peace', '', ''), ('.', '', ''), ('\\n\\n\\n\\n', '', ''), ('Of', '', ''), ('the', '', ''), ('commodities', '', ''), ('of', '', ''), ('Portugal', 'PLACE', '1000090'), ('\\n', '', ''), ('.', '', ''), ('The', '', ''), ('second', '', ''), ('Chapter', 'PLACE', 'NIL'), ('.', '', ''), ('\\n\\n       ', '', ''), ('THE', '', ''), ('Marchandy', '', ''), ('also', '', ''), ('of', '', ''), ('Portugal', 'PLACE', '1000090'), ('\\n\\n       ', '', ''), ('By', '', ''), ('divers', '', ''), ('lands', '', ''), ('turne', '', ''), ('into', '', ''), ('sale', '', ''), ('.', '', ''), ('\\n       ', '', ''), ('Portugalers', '', ''), ('with', '', ''), ('us', '', ''), ('have', '', ''), ('trouth', '', ''), ('in', '', ''), ('hand', '', ''), (':', '', ''), ('\\n       ', '', ''), ('Whose', '', ''), ('Marchandy', '', ''), ('commeth', '', ''), ('much', '', ''), ('into', '', ''), ('England', 'PLACE', '7002445'), ('.', '', ''), ('\\n       ', '', ''), ('They', '', ''), ('ben', '', ''), ('our', '', ''), ('friends', '', ''), (',', '', ''), ('with', '', ''), ('their', '', ''), ('commodities', '', ''), (',', '', ''), ('\\n       ', '', ''), ('And', '', ''), ('wee', '', ''), ('English', '', ''), ('passen', '', ''), ('into', '', ''), ('their', '', ''), ('countr', '', '')]\n",
      "\n",
      "Entities [('Portugal', 'PLACE', '1000090'), ('Chapter', 'PLACE', 'NIL'), ('Portugal', 'PLACE', '1000090'), ('England', 'PLACE', '7002445')]\n",
      "Tokens [('th', '', ''), (',', '', ''), ('\\n', '', ''), ('But', '', ''), ('a', '', ''), ('little', '', ''), ('Mader', '', ''), ('and', '', ''), ('Flemish', '', ''), ('Cloth', '', ''), (':', '', ''), ('\\n', '', ''), ('By', '', ''), ('Drapering', '', ''), ('of', '', ''), ('our', '', ''), ('wooll', '', ''), ('in', '', ''), ('substance', '', ''), ('\\n', '', ''), ('Liven', '', ''), ('her', '', ''), ('commons', '', ''), (',', '', ''), ('this', '', ''), ('is', '', ''), ('her', '', ''), ('governance', '', ''), (',', '', ''), ('\\n', '', ''), ('Without', '', ''), ('wich', '', ''), ('they', '', ''), ('may', '', ''), ('not', '', ''), ('live', '', ''), ('at', '', ''), ('ease', '', ''), ('.', '', ''), ('\\n', '', ''), ('Thus', '', ''), ('must', '', ''), ('hem', '', ''), ('sterve', '', ''), (',', '', ''), ('or', '', ''), ('with', '', ''), ('us', '', ''), ('must', '', ''), ('have', '', ''), ('peace', '', ''), ('.', '', ''), ('\\n\\n\\n\\n', '', ''), ('Of', '', ''), ('the', '', ''), ('commodities', '', ''), ('of', '', ''), ('Portugal', 'PLACE', '1000090'), ('\\n', '', ''), ('.', '', ''), ('The', '', ''), ('second', '', ''), ('Chapter', 'PLACE', 'NIL'), ('.', '', ''), ('\\n\\n       ', '', ''), ('THE', '', ''), ('Marchandy', '', ''), ('also', '', ''), ('of', '', ''), ('Portugal', 'PLACE', '1000090'), ('\\n\\n       ', '', ''), ('By', '', ''), ('divers', '', ''), ('lands', '', ''), ('turne', '', ''), ('into', '', ''), ('sale', '', ''), ('.', '', ''), ('\\n       ', '', ''), ('Portugalers', '', ''), ('with', '', ''), ('us', '', ''), ('have', '', ''), ('trouth', '', ''), ('in', '', ''), ('hand', '', ''), (':', '', ''), ('\\n       ', '', ''), ('Whose', '', ''), ('Marchandy', '', ''), ('commeth', '', ''), ('much', '', ''), ('into', '', ''), ('England', 'PLACE', '7002445'), ('.', '', ''), ('\\n       ', '', ''), ('They', '', ''), ('ben', '', ''), ('our', '', ''), ('friends', '', ''), (',', '', ''), ('with', '', ''), ('their', '', ''), ('commodities', '', ''), (',', '', ''), ('\\n       ', '', ''), ('And', '', ''), ('wee', '', ''), ('English', '', ''), ('passen', '', ''), ('into', '', ''), ('their', '', ''), ('countrees', '', ''), ('.', '', ''), ('\\n       ', '', ''), ('Her', '', ''), ('land', '', ''), ('hath', '', ''), ('wine', '', ''), (',', '', ''), ('Osey', '', ''), (',', '', ''), ('Ware', '', ''), (',', '', ''), ('and', '', ''), ('Graine', '', ''), (',', '', ''), ('\\n     ', '', '')]\n",
      "\n",
      "Entities [('Townes', 'PLACE', 'NIL'), ('Plymouth', 'TGN', '7011301'), ('Fowey', 'TGN', '1029272')]\n",
      "Tokens [('ance', '', ''), (':', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('so', '', ''), ('they', '', ''), ('did', '', ''), ('withouten', '', ''), ('him', '', ''), ('that', '', ''), ('deede', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('But', '', ''), ('when', '', ''), ('the', '', ''), ('king', '', ''), ('anon', '', ''), ('had', '', ''), ('taken', '', ''), ('heede', '', ''), (':', '', ''), ('\\n   ', '', ''), ('Hee', '', ''), ('in', '', ''), ('his', '', ''), ('herte', '', ''), ('set', '', ''), ('a', '', ''), ('judgement', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Without', '', ''), ('calling', '', ''), ('of', '', ''), ('any', '', ''), ('Parliament', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Or', '', ''), ('greate', '', ''), ('tarry', '', ''), ('to', '', ''), ('take', '', ''), ('long', '', ''), ('advise', '', ''), ('\\n   ', '', ''), ('To', '', ''), ('fortifie', '', ''), ('anon', '', ''), ('he', '', ''), ('did', '', ''), ('devise', '', ''), ('\\n   ', '', ''), ('Of', '', ''), ('English', '', ''), ('Townes', 'PLACE', 'NIL'), ('three', '', ''), (',', '', ''), ('that', '', ''), ('is', '', ''), ('to', '', ''), ('say', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Dertmouth', '', ''), (',', '', ''), ('Plymouth', 'TGN', '7011301'), ('\\n', '', ''), (',', '', ''), ('the', '', ''), ('third', '', ''), ('it', '', ''), ('is', '', ''), ('Fowey', 'TGN', '1029272'), ('\\n', '', ''), (':', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('gave', '', ''), ('hem', '', ''), ('helpe', '', ''), ('and', '', ''), ('notable', '', ''), ('puisance', '', ''), ('\\n   ', '', ''), ('With', '', ''), ('insistence', '', ''), ('set', '', ''), ('them', '', ''), ('in', '', ''), ('governance', '', ''), ('\\n   ', '', ''), ('Upon', '', ''), ('pety', '', ''), ('Bretayne', '', ''), ('for', '', ''), ('to', '', ''), ('werre', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('Those', '', ''), ('good', '', ''), ('sea', '', ''), ('men', '', ''), ('would', '', ''), ('no', '', ''), ('more', '', ''), ('differre', '', ''), (',', '', ''), ('\\n   ', '', ''), ('But', '', ''), ('bete', '', ''), ('hem', '', ''), ('home', '', ''), ('and', '', ''), ('made', '', ''), ('they', '', ''), ('might', '', ''), ('not', '', ''), ('rowte', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Tooke', '', ''), ('prisoners', '', ''), (',', '', ''), ('and', '', ''), ('made', '', ''), ('them', '', ''), ('for', '', ''), ('to', '', ''), ('lowte', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', '')]\n",
      "\n",
      "Entities [('Townes', 'PLACE', 'NIL'), ('Plymouth', 'TGN', '7011301'), ('Fowey', 'TGN', '1029272')]\n",
      "Tokens [('thouten', '', ''), ('him', '', ''), ('that', '', ''), ('deede', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('But', '', ''), ('when', '', ''), ('the', '', ''), ('king', '', ''), ('anon', '', ''), ('had', '', ''), ('taken', '', ''), ('heede', '', ''), (':', '', ''), ('\\n   ', '', ''), ('Hee', '', ''), ('in', '', ''), ('his', '', ''), ('herte', '', ''), ('set', '', ''), ('a', '', ''), ('judgement', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Without', '', ''), ('calling', '', ''), ('of', '', ''), ('any', '', ''), ('Parliament', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Or', '', ''), ('greate', '', ''), ('tarry', '', ''), ('to', '', ''), ('take', '', ''), ('long', '', ''), ('advise', '', ''), ('\\n   ', '', ''), ('To', '', ''), ('fortifie', '', ''), ('anon', '', ''), ('he', '', ''), ('did', '', ''), ('devise', '', ''), ('\\n   ', '', ''), ('Of', '', ''), ('English', '', ''), ('Townes', 'PLACE', 'NIL'), ('three', '', ''), (',', '', ''), ('that', '', ''), ('is', '', ''), ('to', '', ''), ('say', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Dertmouth', '', ''), (',', '', ''), ('Plymouth', 'TGN', '7011301'), ('\\n', '', ''), (',', '', ''), ('the', '', ''), ('third', '', ''), ('it', '', ''), ('is', '', ''), ('Fowey', 'TGN', '1029272'), ('\\n', '', ''), (':', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('gave', '', ''), ('hem', '', ''), ('helpe', '', ''), ('and', '', ''), ('notable', '', ''), ('puisance', '', ''), ('\\n   ', '', ''), ('With', '', ''), ('insistence', '', ''), ('set', '', ''), ('them', '', ''), ('in', '', ''), ('governance', '', ''), ('\\n   ', '', ''), ('Upon', '', ''), ('pety', '', ''), ('Bretayne', '', ''), ('for', '', ''), ('to', '', ''), ('werre', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('Those', '', ''), ('good', '', ''), ('sea', '', ''), ('men', '', ''), ('would', '', ''), ('no', '', ''), ('more', '', ''), ('differre', '', ''), (',', '', ''), ('\\n   ', '', ''), ('But', '', ''), ('bete', '', ''), ('hem', '', ''), ('home', '', ''), ('and', '', ''), ('made', '', ''), ('they', '', ''), ('might', '', ''), ('not', '', ''), ('rowte', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Tooke', '', ''), ('prisoners', '', ''), (',', '', ''), ('and', '', ''), ('made', '', ''), ('them', '', ''), ('for', '', ''), ('to', '', ''), ('lowte', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('efte', '', ''), ('the', '', ''), ('Duke', '', ''), (',', '', ''), ('an', '', ''), ('ensamp', '', '')]\n",
      "\n",
      "Entities [('Marchants', 'PLACE', 'NIL'), ('Scotland', 'PLACE', '7002444'), ('Flanders', 'PLACE', '7018236'), ('Scotland', 'PLACE', '7002444'), ('Flanders', 'TGN', '7018236'), ('England', 'PLACE', '7002445')]\n",
      "Tokens [('tute', '', ''), ('for', '', ''), ('Lombards', '', ''), ('in', '', ''), ('this', '', ''), ('land', '', ''), (',', '', ''), ('\\n   ', '', ''), ('That', '', ''), ('they', '', ''), ('should', '', ''), ('in', '', ''), ('noe', '', ''), ('wise', '', ''), ('take', '', ''), ('on', '', ''), ('hande', '', ''), ('\\n   ', '', ''), ('Here', '', ''), ('to', '', ''), ('inhabite', '', ''), (',', '', ''), ('here', '', ''), ('to', '', ''), ('chardge', '', ''), ('and', '', ''), ('dischardge', '', ''), ('\\n   ', '', ''), ('But', '', ''), ('fortie', '', ''), ('dayes', '', ''), (',', '', ''), ('no', '', ''), ('more', '', ''), ('time', '', ''), ('had', '', ''), ('they', '', ''), ('large', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('This', '', ''), ('good', '', ''), ('king', '', ''), ('by', '', ''), ('witte', '', ''), ('of', '', ''), ('such', '', ''), ('appreiffe', '', ''), ('\\n   ', '', ''), ('Kept', '', ''), ('his', '', ''), ('Marchants', 'PLACE', 'NIL'), ('and', '', ''), ('the', '', ''), ('sea', '', ''), ('from', '', ''), ('mischiefe', '', ''), ('.', '', ''), ('\\n\\n\\n\\n', '', ''), ('Of', '', ''), ('the', '', ''), ('commodities', '', ''), ('of', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('and', '', ''), ('draping', '', ''), ('of', '', ''), ('hel', '', ''), ('\\n', '', ''), ('wolles', '', ''), ('in', '', ''), ('Flanders', 'PLACE', '7018236'), ('.', '', ''), ('The', '', ''), ('fourth', '', ''), ('Chapiter', '', ''), ('.', '', ''), ('\\n\\n   ', '', ''), ('MOREOVER', '', ''), ('of', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('the', '', ''), ('commodities', '', ''), ('\\n   ', '', ''), ('Are', '', ''), ('Felles', '', ''), (',', '', ''), ('Hides', '', ''), (',', '', ''), ('and', '', ''), ('of', '', ''), ('Wooll', '', ''), ('the', '', ''), ('Fleese', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('all', '', ''), ('these', '', ''), ('must', '', ''), ('passe', '', ''), ('by', '', ''), ('us', '', ''), ('away', '', ''), ('\\n   ', '', ''), ('Into', '', ''), ('Flanders', 'TGN', '7018236'), ('by', '', ''), ('England', 'PLACE', '7002445'), (',', '', ''), ('sooth', '', ''), ('to', '', ''), ('say', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('all', '', ''), ('her', '', ''), ('woolle', '', ''), ('was', '', ''), ('draped', '', ''), ('for', '', ''), ('to', '', ''), ('sell', '', ''), ('\\n   ', '', ''), ('In', '', ''), ('the', '', ''), ('Townes', '', ''), ('of', '', ''), ('Pop', '', '')]\n",
      "\n",
      "Entities [('Marchants', 'PLACE', 'NIL'), ('Scotland', 'PLACE', '7002444'), ('Flanders', 'PLACE', '7018236'), ('Scotland', 'PLACE', '7002444'), ('Flanders', 'TGN', '7018236'), ('England', 'PLACE', '7002445'), ('Poperinge', 'TGN', '7007975')]\n",
      "Tokens [('to', '', ''), ('inhabite', '', ''), (',', '', ''), ('here', '', ''), ('to', '', ''), ('chardge', '', ''), ('and', '', ''), ('dischardge', '', ''), ('\\n   ', '', ''), ('But', '', ''), ('fortie', '', ''), ('dayes', '', ''), (',', '', ''), ('no', '', ''), ('more', '', ''), ('time', '', ''), ('had', '', ''), ('they', '', ''), ('large', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('This', '', ''), ('good', '', ''), ('king', '', ''), ('by', '', ''), ('witte', '', ''), ('of', '', ''), ('such', '', ''), ('appreiffe', '', ''), ('\\n   ', '', ''), ('Kept', '', ''), ('his', '', ''), ('Marchants', 'PLACE', 'NIL'), ('and', '', ''), ('the', '', ''), ('sea', '', ''), ('from', '', ''), ('mischiefe', '', ''), ('.', '', ''), ('\\n\\n\\n\\n', '', ''), ('Of', '', ''), ('the', '', ''), ('commodities', '', ''), ('of', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('and', '', ''), ('draping', '', ''), ('of', '', ''), ('hel', '', ''), ('\\n', '', ''), ('wolles', '', ''), ('in', '', ''), ('Flanders', 'PLACE', '7018236'), ('.', '', ''), ('The', '', ''), ('fourth', '', ''), ('Chapiter', '', ''), ('.', '', ''), ('\\n\\n   ', '', ''), ('MOREOVER', '', ''), ('of', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('the', '', ''), ('commodities', '', ''), ('\\n   ', '', ''), ('Are', '', ''), ('Felles', '', ''), (',', '', ''), ('Hides', '', ''), (',', '', ''), ('and', '', ''), ('of', '', ''), ('Wooll', '', ''), ('the', '', ''), ('Fleese', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('all', '', ''), ('these', '', ''), ('must', '', ''), ('passe', '', ''), ('by', '', ''), ('us', '', ''), ('away', '', ''), ('\\n   ', '', ''), ('Into', '', ''), ('Flanders', 'TGN', '7018236'), ('by', '', ''), ('England', 'PLACE', '7002445'), (',', '', ''), ('sooth', '', ''), ('to', '', ''), ('say', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('all', '', ''), ('her', '', ''), ('woolle', '', ''), ('was', '', ''), ('draped', '', ''), ('for', '', ''), ('to', '', ''), ('sell', '', ''), ('\\n   ', '', ''), ('In', '', ''), ('the', '', ''), ('Townes', '', ''), ('of', '', ''), ('Poperinge', 'TGN', '7007975'), ('and', '', ''), ('of', '', ''), ('Bell', '', ''), (';', '', ''), ('\\n   ', '', ''), ('Which', '', ''), ('my', '', ''), ('Lord', '', ''), ('of', '', ''), ('Glocester', '', ''), ('with', '', ''), ('ire', '', ''), ('\\n   ', '', ''), ('For', '', ''), ('her', '', ''), ('falshed', '', ''), ('set', '', ''), ('upon', '', '')]\n",
      "\n",
      "Entities [('Flanders', 'PLACE', '7018236'), ('England', 'PLACE', '7002445'), ('Poperinge', 'TGN', '7007975'), ('Poperinge', 'TGN', '7007975'), ('Spaine', 'PLACE', 'NIL'), ('Scotland', 'PLACE', '7002444')]\n",
      "Tokens [(',', '', ''), ('and', '', ''), ('of', '', ''), ('Wooll', '', ''), ('the', '', ''), ('Fleese', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('all', '', ''), ('these', '', ''), ('must', '', ''), ('passe', '', ''), ('by', '', ''), ('us', '', ''), ('away', '', ''), ('\\n   ', '', ''), ('Into', '', ''), ('Flanders', 'PLACE', '7018236'), ('by', '', ''), ('England', 'PLACE', '7002445'), (',', '', ''), ('sooth', '', ''), ('to', '', ''), ('say', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('all', '', ''), ('her', '', ''), ('woolle', '', ''), ('was', '', ''), ('draped', '', ''), ('for', '', ''), ('to', '', ''), ('sell', '', ''), ('\\n   ', '', ''), ('In', '', ''), ('the', '', ''), ('Townes', '', ''), ('of', '', ''), ('Poperinge', 'TGN', '7007975'), ('and', '', ''), ('of', '', ''), ('Bell', '', ''), (';', '', ''), ('\\n   ', '', ''), ('Which', '', ''), ('my', '', ''), ('Lord', '', ''), ('of', '', ''), ('Glocester', '', ''), ('with', '', ''), ('ire', '', ''), ('\\n   ', '', ''), ('For', '', ''), ('her', '', ''), ('falshed', '', ''), ('set', '', ''), ('upon', '', ''), ('a', '', ''), ('fire', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('yet', '', ''), ('they', '', ''), ('of', '', ''), ('Bell', '', ''), ('and', '', ''), ('Poperinge', 'TGN', '7007975'), ('\\n\\n   ', '', ''), ('Could', '', ''), ('never', '', ''), ('drape', '', ''), ('her', '', ''), ('wooll', '', ''), ('for', '', ''), ('any', '', ''), ('thing', '', ''), (',', '', ''), ('\\n   ', '', ''), ('But', '', ''), ('if', '', ''), ('they', '', ''), ('had', '', ''), ('English', '', ''), ('woll', '', ''), ('withall', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('Our', '', ''), ('goodly', '', ''), ('wooll', '', ''), ('which', '', ''), ('is', '', ''), ('so', '', ''), ('generall', '', ''), ('\\n   ', '', ''), ('Needefull', '', ''), ('to', '', ''), ('them', '', ''), ('in', '', ''), ('Spaine', 'PLACE', 'NIL'), ('and', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('als', '', ''), (',', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('other', '', ''), ('costes', '', ''), (',', '', ''), ('this', '', ''), ('sentence', '', ''), ('is', '', ''), ('not', '', ''), ('false', '', ''), (':', '', ''), ('\\n   ', '', ''), ('Yee', '', ''), ('worthy', '', ''), ('Marchants', '', ''), ('I', '', ''), ('doe', '', ''), ('it', '', ''), ('upon', '', ''), ('you', '', ''), (',', '', ''), ('\\n   ', '', ''), ('I', '', ''), ('have', '', ''), ('this', '', ''), ('learned', '', ''), ('ye', '', ''), ('wot', '', '')]\n",
      "\n",
      "Entities [('Poperinge', 'TGN', '7007975'), ('Poperinge', 'PLACE', '7007975'), ('Spaine', 'PLACE', 'NIL'), ('Scotland', 'PLACE', '7002444'), ('Scotland', 'PLACE', '7002444'), ('Scots', 'PLACE', 'NIL'), ('Flande', 'PLACE', 'NIL')]\n",
      "Tokens [('Poperinge', 'TGN', '7007975'), ('and', '', ''), ('of', '', ''), ('Bell', '', ''), (';', '', ''), ('\\n   ', '', ''), ('Which', '', ''), ('my', '', ''), ('Lord', '', ''), ('of', '', ''), ('Glocester', '', ''), ('with', '', ''), ('ire', '', ''), ('\\n   ', '', ''), ('For', '', ''), ('her', '', ''), ('falshed', '', ''), ('set', '', ''), ('upon', '', ''), ('a', '', ''), ('fire', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('yet', '', ''), ('they', '', ''), ('of', '', ''), ('Bell', '', ''), ('and', '', ''), ('Poperinge', 'PLACE', '7007975'), ('\\n\\n   ', '', ''), ('Could', '', ''), ('never', '', ''), ('drape', '', ''), ('her', '', ''), ('wooll', '', ''), ('for', '', ''), ('any', '', ''), ('thing', '', ''), (',', '', ''), ('\\n   ', '', ''), ('But', '', ''), ('if', '', ''), ('they', '', ''), ('had', '', ''), ('English', '', ''), ('woll', '', ''), ('withall', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('Our', '', ''), ('goodly', '', ''), ('wooll', '', ''), ('which', '', ''), ('is', '', ''), ('so', '', ''), ('generall', '', ''), ('\\n   ', '', ''), ('Needefull', '', ''), ('to', '', ''), ('them', '', ''), ('in', '', ''), ('Spaine', 'PLACE', 'NIL'), ('and', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('als', '', ''), (',', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('other', '', ''), ('costes', '', ''), (',', '', ''), ('this', '', ''), ('sentence', '', ''), ('is', '', ''), ('not', '', ''), ('false', '', ''), (':', '', ''), ('\\n   ', '', ''), ('Yee', '', ''), ('worthy', '', ''), ('Marchants', '', ''), ('I', '', ''), ('doe', '', ''), ('it', '', ''), ('upon', '', ''), ('you', '', ''), (',', '', ''), ('\\n   ', '', ''), ('I', '', ''), ('have', '', ''), ('this', '', ''), ('learned', '', ''), ('ye', '', ''), ('wot', '', ''), ('well', '', ''), ('where', '', ''), ('and', '', ''), ('howe', '', ''), (':', '', ''), ('\\n   ', '', ''), ('Ye', '', ''), ('wotte', '', ''), ('the', '', ''), ('Staple', '', ''), ('of', '', ''), ('that', '', ''), ('Marchandie', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Of', '', ''), ('this', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('is', '', ''), ('Flaunders', '', ''), ('sekerly', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('the', '', ''), ('Scots', 'PLACE', 'NIL'), ('bene', '', ''), ('charged', '', ''), ('knowen', '', ''), ('at', '', ''), ('the', '', ''), ('eye', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Out', '', ''), ('of', '', ''), ('Flande', 'PLACE', 'NIL')]\n",
      "\n",
      "Entities [('Spaine', 'PLACE', 'NIL'), ('Scotland', 'PLACE', '7002444'), ('Scotland', 'PLACE', '7002444'), ('Scots', 'PLACE', 'NIL'), ('Flanders', 'PLACE', '7018236')]\n",
      "Tokens [('ll', '', ''), ('withall', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('Our', '', ''), ('goodly', '', ''), ('wooll', '', ''), ('which', '', ''), ('is', '', ''), ('so', '', ''), ('generall', '', ''), ('\\n   ', '', ''), ('Needefull', '', ''), ('to', '', ''), ('them', '', ''), ('in', '', ''), ('Spaine', 'PLACE', 'NIL'), ('and', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('als', '', ''), (',', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('other', '', ''), ('costes', '', ''), (',', '', ''), ('this', '', ''), ('sentence', '', ''), ('is', '', ''), ('not', '', ''), ('false', '', ''), (':', '', ''), ('\\n   ', '', ''), ('Yee', '', ''), ('worthy', '', ''), ('Marchants', '', ''), ('I', '', ''), ('doe', '', ''), ('it', '', ''), ('upon', '', ''), ('you', '', ''), (',', '', ''), ('\\n   ', '', ''), ('I', '', ''), ('have', '', ''), ('this', '', ''), ('learned', '', ''), ('ye', '', ''), ('wot', '', ''), ('well', '', ''), ('where', '', ''), ('and', '', ''), ('howe', '', ''), (':', '', ''), ('\\n   ', '', ''), ('Ye', '', ''), ('wotte', '', ''), ('the', '', ''), ('Staple', '', ''), ('of', '', ''), ('that', '', ''), ('Marchandie', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Of', '', ''), ('this', '', ''), ('Scotland', 'PLACE', '7002444'), ('\\n ', '', ''), ('is', '', ''), ('Flaunders', '', ''), ('sekerly', '', ''), ('.', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('the', '', ''), ('Scots', 'PLACE', 'NIL'), ('bene', '', ''), ('charged', '', ''), ('knowen', '', ''), ('at', '', ''), ('the', '', ''), ('eye', '', ''), (',', '', ''), ('\\n   ', '', ''), ('Out', '', ''), ('of', '', ''), ('Flanders', 'PLACE', '7018236'), ('with', '', ''), ('little', '', ''), ('Mercerie', '', ''), (',', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('great', '', ''), ('plentie', '', ''), ('of', '', ''), ('Haberdashers', '', ''), ('Ware', '', ''), (',', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('halfe', '', ''), ('her', '', ''), ('shippes', '', ''), ('with', '', ''), ('cart', '', ''), ('wheeles', '', ''), ('bare', '', ''), (',', '', ''), ('\\n   ', '', ''), ('And', '', ''), ('with', '', ''), ('Barrowes', '', ''), ('are', '', ''), ('laden', '', ''), ('as', '', ''), ('in', '', ''), ('substance', '', ''), (':', '', ''), ('\\n   ', '', ''), ('Thus', '', ''), ('most', '', ''), ('rude', '', ''), ('ware', '', ''), ('are', '', ''), ('in', '', ''), ('her', '', ''), ('chevesance', '', ''), ('.', '', ''), ('\\n ', '', '')]\n"
     ]
    }
   ],
   "source": [
    "for text, annotation in places[4:13]:\n",
    "    # apply the entity linker which will now make predictions for the 'Russ Cochran' entities\n",
    "    doc = nlp(text)\n",
    "    print()\n",
    "    print(\"Entities\", [(ent.text, ent.label_, ent.kb_id_) for ent in doc.ents])\n",
    "    print(\"Tokens\", [(t.text, t.ent_type_, t.ent_kb_id_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "WW1RtHwcdQM9",
    "outputId": "1f0b1129-5490-433a-8024-32376129398c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entities []\n"
     ]
    }
   ],
   "source": [
    "from skosprovider_getty.providers import TGNProvider\n",
    "aat = TGNProvider(metadata={'id': 'TGN'})\n",
    "def get_place_name(id:int) -> str:\n",
    "    place = aat.get_by_id(id)\n",
    "\n",
    "    print('Labels')\n",
    "    print('------')\n",
    "    for l in place.labels:\n",
    "       print(l.language + ': ' + l.label + ' [' + l.type + ']')\n",
    "\n",
    "    print('Notes')\n",
    "    print('-----')\n",
    "    for n in place.notes:\n",
    "        print(n.language + ': ' + n.note + ' [' + n.type + ']')\n",
    "for text, annotation in places[:1]:\n",
    "    doc = nlp(text)\n",
    "    print()\n",
    "    print(\"Entities\", [(get_place_name(ent.kb_id_)) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "colab_type": "code",
    "id": "MWAMyaaFxKaW",
    "outputId": "e21b2940-f207-4e11-9e93-ff2f14ee0222"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The army marched from \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Konia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Kaiseria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Caesarea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       "), and thence to \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sivas\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", where the feast of the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Korbân\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       " (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Kazmaghan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PLACE</span>\n",
       "</mark>\n",
       ", and halted under the walls of Eriviin in the year 1044 (1634).  \n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "doc = nlp(\n",
    "    \"\"\"The army marched from Konia to Kaiseria (Caesarea), and thence to Sivas, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).  \n",
    "\"\"\"\n",
    ")\n",
    "HTML(displacy.render(doc, style=\"ent\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wW2w-jcFGeuq"
   },
   "source": [
    "# Alternative approaches, Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "rYmBY1tuwz7I",
    "outputId": "da061fa1-9005-415a-a2f0-ad78b1405967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-23 17:11:51--  http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_500d.pkl.bz2\n",
      "Resolving wikipedia2vec.s3.amazonaws.com (wikipedia2vec.s3.amazonaws.com)... 52.219.0.197\n",
      "Connecting to wikipedia2vec.s3.amazonaws.com (wikipedia2vec.s3.amazonaws.com)|52.219.0.197|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17294111805 (16G) [application/x-bzip2]\n",
      "Saving to: ‘enwiki_20180420_500d.pkl.bz2.1’\n",
      "\n",
      "enwiki_20180420_500 100%[===================>]  16.11G  75.6MB/s    in 4m 30s  \n",
      "\n",
      "2020-02-23 17:16:21 (61.2 MB/s) - ‘enwiki_20180420_500d.pkl.bz2.1’ saved [17294111805/17294111805]\n",
      "\n",
      "\n",
      "bzip2: Control-C or similar caught, quitting.\n",
      "bzip2: Deleting output file enwiki_20180420_500d.pkl, if it exists.\n"
     ]
    }
   ],
   "source": [
    "if not Path('enwiki_20180420_500d.pkl.bz2').exists():\n",
    "    !wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_500d.pkl.bz2\n",
    "    !bzip2 -d enwiki_20180420_500d.pkl.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "u-9PDBBczIyk",
    "outputId": "77753bf8-542d-4d69-dab9-517f5b330209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16888804\n",
      "drwxr-xr-x 1 root root        4096 Feb 23 17:08 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 1 root root        4096 Feb 23 16:36 \u001b[01;34m..\u001b[0m/\n",
      "drwxr-xr-x 1 root root        4096 Feb 19 17:12 \u001b[01;34m.config\u001b[0m/\n",
      "-rw-r--r-- 1 root root 17294111805 May 17  2018 enwiki_20180420_500d.pkl.bz2\n",
      "drwxr-xr-x 1 root root        4096 Feb  5 18:37 \u001b[01;34msample_data\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 849
    },
    "colab_type": "code",
    "id": "YMu7Rw749Ffo",
    "outputId": "65f322c7-ca78-4f57-e296-29660eeaae53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia2vec\n",
      "  Using cached https://files.pythonhosted.org/packages/d8/88/751037c70ca86581d444824e66bb799ef9060339a1d5d1fc1804c422d7cc/wikipedia2vec-1.0.4.tar.gz\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from wikipedia2vec) (7.0)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from wikipedia2vec) (0.42.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from wikipedia2vec) (0.14.1)\n",
      "Requirement already satisfied: lmdb in /usr/local/lib/python3.6/dist-packages (from wikipedia2vec) (0.98)\n",
      "Collecting marisa-trie\n",
      "  Using cached https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz\n",
      "Collecting mwparserfromhell\n",
      "  Using cached https://files.pythonhosted.org/packages/23/03/4fb04da533c7e237c0104151c028d8bff856293d34e51d208c529696fb79/mwparserfromhell-0.5.4.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from wikipedia2vec) (1.17.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from wikipedia2vec) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from wikipedia2vec) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from wikipedia2vec) (4.28.1)\n",
      "Building wheels for collected packages: wikipedia2vec, marisa-trie, mwparserfromhell\n",
      "  Building wheel for wikipedia2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia2vec: filename=wikipedia2vec-1.0.4-cp36-cp36m-linux_x86_64.whl size=4581859 sha256=1d1e02205670bcd50042320c31979d1cf0281a716a88cca5a4e70770a76e601e\n",
      "  Stored in directory: /root/.cache/pip/wheels/16/e7/02/852c8ce366cc10adcf5d43c6471bbf926dd15c277578c13184\n",
      "  Building wheel for marisa-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp36-cp36m-linux_x86_64.whl size=862410 sha256=ee6fb7d9c0bcfa73e20d05fd32b7863a856f1350091027330902e059881caf48\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
      "  Building wheel for mwparserfromhell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mwparserfromhell: filename=mwparserfromhell-0.5.4-cp36-cp36m-linux_x86_64.whl size=183763 sha256=4246437e46fa57f8504b6191d68b949ddc1b85f06441ad6eadfb72eeb011480a\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/76/d5/7088b941df3b362c45dd7912dd05314bc034751ec9cbca9a75\n",
      "Successfully built wikipedia2vec marisa-trie mwparserfromhell\n",
      "Installing collected packages: marisa-trie, mwparserfromhell, wikipedia2vec\n",
      "Successfully installed marisa-trie-0.7.5 mwparserfromhell-0.5.4 wikipedia2vec-1.0.4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bbfd66184f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mwikipedia2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWikipedia2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwiki2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikipedia2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enwiki_20180420_500d.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwiki2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yoda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wikipedia2vec/wikipedia2vec.pyx\u001b[0m in \u001b[0;36mwikipedia2vec.wikipedia2vec.Wikipedia2Vec.load\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'enwiki_20180420_500d.pkl'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from wikipedia2vec import Wikipedia2Vec\n",
    "except ModuleNotFoundError:\n",
    "    !pip install wikipedia2vec\n",
    "    from wikipedia2vec import Wikipedia2Vec\n",
    "\n",
    "wiki2vec = Wikipedia2Vec.load('enwiki_20180420_500d.pkl')\n",
    "result = wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "hHb8tFtb7wEA",
    "outputId": "3215154b-3b4d-4dfd-d8ed-b4823fc23a52"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6459d04d738f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "v5pj7etX5SY4",
    "outputId": "29ab6b71-318d-4821-da7c-ba67fa6a9779"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b1f52d8635c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Entity'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# Load a text, identify entity spans, get wiki2vec entity \n",
    "for i in result: \n",
    "    if type(i[0]).__name__ == 'Entity':\n",
    "        confidence = i[1]\n",
    "        name = i[0].title \n",
    "        print(name, confidence) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cjs9b7afGFyu"
   },
   "source": [
    "# Another approach to the problem, SPARQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "VvxiEQTMER4q",
    "outputId": "5cf4aafa-9e4d-429a-e127-7bed0a9d774e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'URI': 'http://dbpedia.org/resource/Infant',\n",
       "  'offset': 0,\n",
       "  'percentageOfSecondRank': 0.00147352792162747,\n",
       "  'similarityScore': 0.9978001054278687,\n",
       "  'support': 3349,\n",
       "  'surfaceForm': 'Baby',\n",
       "  'types': ''},\n",
       " {'URI': 'http://dbpedia.org/resource/Yoda',\n",
       "  'offset': 5,\n",
       "  'percentageOfSecondRank': 1.225279889190865e-05,\n",
       "  'similarityScore': 0.9999754186031542,\n",
       "  'support': 840,\n",
       "  'surfaceForm': 'Yoda',\n",
       "  'types': 'Http://xmlns.com/foaf/0.1/Person,Wikidata:Q95074,Wikidata:Q5,Wikidata:Q24229398,Wikidata:Q215627,DUL:NaturalPerson,DUL:Agent,Schema:Person,DBpedia:Person,DBpedia:FictionalCharacter,DBpedia:Agent'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import spotlight\n",
    "except ModuleNotFoundError:\n",
    "    !pip install pyspotlight\n",
    "    import spotlight \n",
    "\n",
    "annotations = spotlight.annotate('https://api.dbpedia-spotlight.org/en/annotate', 'Baby Yoda is cute', confidence=0.4, support=20)\n",
    "annotations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdURB2HaF4fC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "iSchool.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "spaCy22",
   "language": "python",
   "name": "spacy22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "215438171bb34ebe86985971f5e225ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45fa9eaa75ff4dc288e4979cfa1599fc",
       "IPY_MODEL_9353da952cd94ea9b09925f4da0c4163"
      ],
      "layout": "IPY_MODEL_9b290c1bbc1a47e783fd93e67144e474"
     }
    },
    "2795f99bbaa049bfa1abfe628a4c40dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "35bdd366937741f2bdb0c64a7b65552f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45fa9eaa75ff4dc288e4979cfa1599fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35bdd366937741f2bdb0c64a7b65552f",
      "max": 999994,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2795f99bbaa049bfa1abfe628a4c40dd",
      "value": 999994
     }
    },
    "9353da952cd94ea9b09925f4da0c4163": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a10a599e5b6f4310aac298df4ebb1d7a",
      "placeholder": "​",
      "style": "IPY_MODEL_d1304bd1255d49aeb70678d6e2d720fa",
      "value": "100% 999994/999994 [02:12&lt;00:00, 7575.19it/s]"
     }
    },
    "9b290c1bbc1a47e783fd93e67144e474": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a10a599e5b6f4310aac298df4ebb1d7a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1304bd1255d49aeb70678d6e2d720fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
