{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Linking with spaCy and TEI\n",
    "![](https://explosion.ai/blog/img/spacy-transformers.jpg)\n",
    "\n",
    "![](https://pbs.twimg.com/media/D0aHPzXWwAEgRwU?format=jpg&name=900x900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/PW3RJM8tDGo\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1033efeb8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "# Youtube\n",
    "IFrame(\"https://www.youtube.com/embed/PW3RJM8tDGo\", 560, 315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The School of Information Sciences\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", also The iSchool at \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Illinois\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", is a graduate school at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the University of Illinois at Urbana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "–Champaign. Its Master of Science in Library and Information Science is currently accredited in full good standing by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the American Library Association\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") # Note that I use the medium model because entity linking requires a model with vectors\n",
    "doc = nlp(\n",
    "\"The School of Information Sciences, also The iSchool at Illinois, is a graduate school at the University of Illinois at Urbana–Champaign. Its Master of Science in Library and Information Science is currently accredited in full good standing by the American Library Association.\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem I: Domain\n",
    "*This works very well for many 20th and 21st century texts.  But what about early modern English?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ITEM because that the kings most \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    deare Uncle\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the king of Denmarke\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Norway &amp; Sweveland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", as the same our \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    soveraigne Lord\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " the king of his intimation hath understood, considering the manifold &amp; great losses, perils, hurts and damage which have late happened aswell to him and his, as to other \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    foraines\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " and strangers, and also friends and speciall subjects of our said \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    soveraigne Lord\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " the king of his \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Realme\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    England\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", by ye going in, entring &amp; passage of such forain &amp; strange persons into his realme of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Norwey &amp;\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " other dominions, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    streits\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", territories, jurisdictions &amp; places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"ITEM because that the kings most deare Uncle, the king of Denmarke, Norway & Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring & passage of such forain & strange persons into his realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"./out_of_domain.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this first example, our goal is to teach an existing English-language model to identify early modern place names.\n",
    "\n",
    "There are several approaches that we could take to this problem.  Different approaches can lend better or worse results and experimentation is an essential part of any machine learning project. \n",
    "\n",
    "#### How can we teach a statistical language model that Sweveland is a place? Where can I get data on early modern places? \n",
    "\n",
    "Richard Hakluyt's The Principal Navigations, Voyages, Traffiques, and Discoveries of the English Nation (1599)\n",
    "\n",
    "![](http://www.sequiturbooks.com/image/cache/Product%20Images/2015-12/The-Principal-1512150003/5ae35178-800x800.jpeg)\n",
    "\n",
    "--- \n",
    "\n",
    "### Download the TEI files from Persius \n",
    "- We're going to extract a list of all the place names from the text to create training data.\n",
    "- To make working with the TEI/XML easier, we're using a standoffconverter by David Lassner\n",
    "- The converter separates the text and annotations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmlParseEntityRef: no name, line 103, column 75 (<string>, line 103)\n",
      "xmlParseEntityRef: no name, line 199, column 94 (<string>, line 199)\n",
      "xmlParseEntityRef: no name, line 186, column 94 (<string>, line 186)\n",
      "xmlParseEntityRef: no name, line 803, column 109 (<string>, line 803)\n",
      "xmlParseEntityRef: no name, line 455, column 89 (<string>, line 455)\n",
      "xmlParseEntityRef: no name, line 441, column 89 (<string>, line 441)\n",
      "Unescaped '<' not allowed in attributes values, line 22, column 25 (<string>, line 22)\n",
      "xmlParseEntityRef: no name, line 49, column 152 (<string>, line 49)\n",
      "xmlParseEntityRef: no name, line 6, column 152 (<string>, line 6)\n",
      "xmlParseEntityRef: no name, line 4, column 111 (<string>, line 4)\n",
      "xmlParseEntityRef: no name, line 34, column 106 (<string>, line 34)\n",
      "xmlParseEntityRef: no name, line 3, column 149 (<string>, line 3)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle\n",
    "from collections import Counter\n",
    "spec = {\"tei\":\"http://www.tei-c.org/ns/1.0\"}\n",
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "from standoffconverter import Converter\n",
    "\n",
    "def tei_loader(url):\n",
    "    tei = urlopen(url).read()\n",
    "    return etree.XML(tei)\n",
    "\n",
    "table_of_contents_url = \"http://www.perseus.tufts.edu/hopper/xmltoc?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D1\"\n",
    "table_of_contents_xml = tei_loader(table_of_contents_url)\n",
    "\n",
    "\n",
    "chunks = table_of_contents_xml.xpath(\"//chunk[@ref]\")\n",
    "refs = [chunk.get('ref') for chunk in chunks] \n",
    "# an example ref 'Perseus%3Atext%3A1999.03.0070%3Anarrative%3D6'\n",
    "\n",
    "\n",
    "standoffs = []\n",
    "\n",
    "for ref in refs:\n",
    "    try:\n",
    "        url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + ref\n",
    "\n",
    "        tei = tei_loader(url)\n",
    "        so = Converter.from_tree(tei)\n",
    "        standoffs.append(so)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA branch of a Statute made in the eight yeere of Henry the sixt, for the trade to Norwey, Sweveland, Den marke, and Fynmarke. \\nITEM because that the kings most deare Uncle, the king\\nof Denmarke, Norway\\n & Sweveland, as the same our\\nsoveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils,\\nhurts and damage which have late happened aswell to\\nhim and his, as to other foraines and strangers, and also\\nfriends and speciall subjects of our said soveraigne Lord\\n\\n\\nthe king of his Realme of England, by ye going in,\\nentring & passage of such forain & strange persons into\\nhis realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him,\\nspecially into his Iles of Fynmarke, and elswhere, aswell\\nin their persons as their things and goods: for eschuing\\nof such losses, perils, hurts & damages, and that such\\nlike (which God forbid) should not hereafter happen: our\\nsaid soveraigne Lord the king hath ordeined and statuted,\\nthat all and singular strangers, aswell Englishmen and\\nothers willing to apply by Ship and come into his Realme\\nof Norwey and other dominions, streits, territories,\\njurisdictions, Isles & places aforesaid with their ships,\\nto the intent to get or have fish or any other Marchandises,\\nor goods, shall apply and come to his Towne of Northberne, where the said king of Denmarke hath specially\\nordained and stablished his staple for the concourses of\\nstrangers and specially of Englishmen, to the exercise\\nof such Marchandises: granting to the said Englishmen\\nthat they shall there injoy in and by all things the same\\nfavour, privileges and prerogatives which they of the\\nHans\\n did enjoy. Therefore our said soveraigne Lord\\nthe king willing the love, affinitie and amities to be\\nfirmely observed, which betwixt his said Uncle and his\\nnoble progenitors of good memory, their Realmes, lands,\\ndominions, streites, territories, jurisdictions and their\\nsaid places, and the same our soveraigne Lord the king\\n& his noble progenitours of famous memory, his great\\nmen, subjects, Realmes, lands & dominions hath bene\\nof old times hitherto continued, nor nothing by our said\\nsoveraigne Lord the king or his people to be attempted\\nor done whereby such amities by reason of any dissensions, enemities or discords might be broken: by the\\nadvise of the Lords spirituall & temporall & of the\\ncommons of his said Realme of England, assembled in\\nthis present Parliament, hath ordained, prohibiting that\\nnone of his liege people nor subjects of his Realme of\\nEngland by audacitie of their follie presume to enter the\\nRealmes, lands, dominions, straits, territories, jurisdictions & places of the said king of Denmarke against ye\\nordinance, prohibition & interdiction of ye same his Uncle\\nabove remembred, & in contempt of the same, upon paine\\nof forfeiture of all their moveable goods & imprisonment\\nof their persons at the kings will.\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's the text from the TEI document \n",
    "standoffs[0].plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"begin\": 0,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 0,\n",
      "    \"tag\": \"TEI.2\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 0,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {\n",
      "      \"lang\": \"en\"\n",
      "    },\n",
      "    \"depth\": 1,\n",
      "    \"tag\": \"text\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 0,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 2,\n",
      "    \"tag\": \"body\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 0,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"narrative\",\n",
      "      \"org\": \"uniform\",\n",
      "      \"sample\": \"complete\"\n",
      "    },\n",
      "    \"depth\": 3,\n",
      "    \"tag\": \"div1\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1,\n",
      "    \"end\": 127,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 4,\n",
      "    \"tag\": \"head\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 50,\n",
      "    \"end\": 55,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"pers\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 83,\n",
      "    \"end\": 89,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 91,\n",
      "    \"end\": 100,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 117,\n",
      "    \"end\": 125,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 128,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 4,\n",
      "    \"tag\": \"p\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 178,\n",
      "    \"end\": 194,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"pers\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 196,\n",
      "    \"end\": 203,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"  +Norway [10,62] (nation), Europe \",\n",
      "      \"type\": \"place\",\n",
      "      \"key\": \"tgn,1000088\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 206,\n",
      "    \"end\": 215,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 511,\n",
      "    \"end\": 511,\n",
      "    \"attrib\": {\n",
      "      \"n\": \"173\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"pb\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 528,\n",
      "    \"end\": 545,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"England\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 633,\n",
      "    \"end\": 639,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 751,\n",
      "    \"end\": 767,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"Fynmarke\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1057,\n",
      "    \"end\": 1067,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1122,\n",
      "    \"end\": 1138,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"Norwey\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1287,\n",
      "    \"end\": 1299,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1339,\n",
      "    \"end\": 1358,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"Northberne\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1375,\n",
      "    \"end\": 1391,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"pers\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1491,\n",
      "    \"end\": 1501,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1527,\n",
      "    \"end\": 1539,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1562,\n",
      "    \"end\": 1572,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1685,\n",
      "    \"end\": 1690,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"  +Hans (inhabited place), Marne, Champagne-Ardenne, France, Europe \",\n",
      "      \"type\": \"place\",\n",
      "      \"key\": \"tgn,4004801\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 2410,\n",
      "    \"end\": 2427,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"England\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 2544,\n",
      "    \"end\": 2561,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"England\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 2696,\n",
      "    \"end\": 2712,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"pers\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 2932,\n",
      "    \"end\": 2932,\n",
      "    \"attrib\": {\n",
      "      \"n\": \"174\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"pb\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Here are the annotations \n",
    "import json\n",
    "print(json.dumps(json.loads(standoffs[0].to_json()), indent=2)) # or just standoffs[0].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text from the TEI document and create training data\n",
    "import json\n",
    "places = []\n",
    "entities = []\n",
    "place_names = [] \n",
    "place_ids = []\n",
    "names = []\n",
    "ADD_NAMES = False # if True, the dataset will included all markedup names from the TEI\n",
    "ADD_PLACE = True\n",
    "for standoff in standoffs:\n",
    "    for annotation in json.loads(standoff.to_json()):\n",
    "        try:\n",
    "            if annotation['tag'] == 'name' and ADD_NAMES:\n",
    "                begin = annotation['begin']\n",
    "                end = annotation['end']\n",
    "                length = end-begin\n",
    "                sent = standoff.plain[begin-300:end+ 300]\n",
    "                assert len(sent) > 0\n",
    "                begin = 300\n",
    "                end = begin+length\n",
    "                if '\\n' in sent[begin:end]:\n",
    "                    end -= 1\n",
    "                place_names.append(sent[begin:end])\n",
    "                place = (sent, {'entities':[(begin,end,\"NAME\")]})\n",
    "                places.append(place)\n",
    "                \n",
    "            if annotation['attrib']['type'] == 'place' and ADD_PLACE:\n",
    "                begin = annotation['begin']\n",
    "                end = annotation['end']\n",
    "                length = end-begin\n",
    "                key = annotation['attrib']['key']\n",
    "                key = key.split(',')[1]\n",
    "                place_ids.append(key)\n",
    "                #modern_name = annotation['attrib']['reg']\n",
    "                sent = standoff.plain[begin-300:end+ 300]\n",
    "                assert len(sent) > 0\n",
    "                begin = 300\n",
    "                end = begin+length\n",
    "                if '\\n' in sent[begin:end]:\n",
    "                    end -= 1\n",
    "                place_names.append(sent[begin:end])\n",
    "                place = (sent, {'entities':[(begin,end,\"TGN\")]})\n",
    "                places.append(place)\n",
    "                \n",
    "                dict_1 = {(begin, end): {key: 1.0,}}\n",
    "                entities.append((sent, {\"links\": dict_1}))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Experiment with place names that are labeled as name, but not type:place\n",
    "names= []\n",
    "for standoff in standoffs: \n",
    "    for annotation in json.loads(standoff.to_json()):\n",
    "        if annotation['tag'] == 'name':\n",
    "            begin = annotation['begin']\n",
    "            end = annotation['end']\n",
    "            word = standoff.plain[begin:end]\n",
    "            names.append(word)\n",
    "names = set(names)\n",
    "df = pd.DataFrame(names, columns =['name'])\n",
    "df.to_csv('names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Molgomsey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Helike Kirke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Canaria Ilands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parthions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>captaine Venner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name\n",
       "0        Molgomsey\n",
       "1     Helike Kirke\n",
       "2   Canaria Ilands\n",
       "3        Parthions\n",
       "4  captaine Venner"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the documentation for training the named entity recognizer. Note the format expected for training data:  https://spacy.io/usage/training#ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 90052 places\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(' and goods: for eschuing\\nof such losses, perils, hurts & damages, and that such\\nlike (which God forbid) should not hereafter happen: our\\nsaid soveraigne Lord the king hath ordeined and statuted,\\nthat all and singular strangers, aswell Englishmen and\\nothers willing to apply by Ship and come into his Realme\\nof Norwey and other dominions, streits, territories,\\njurisdictions, Isles & places aforesaid with their ships,\\nto the intent to get or have fish or any other Marchandises,\\nor goods, shall apply and come to his Towne of Northberne, where the said king of Denmarke hath specially\\nordained and stablished his sta',\n",
       " {'entities': [(300, 315, 'NAME')]})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('found',len(places),'places')\n",
    "places[4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the format for entity-linker training data: https://spacy.io/usage/training#entity-linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 10000 entities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('and brede.\\nWhat hath then Flanders, bee Flemings lieffe or loth,\\nBut a little Mader and Flemish Cloth:\\nBy Drapering of our wooll in substance\\nLiven her commons, this is her governance,\\nWithout wich they may not live at ease.\\nThus must hem sterve, or with us must have peace.\\n\\n\\n\\nOf the commodities of Portugal\\n. The second Chapter.\\n\\n       THE Marchandy also of Portugal\\n\\n       By divers lands turne into sale.\\n       Portugalers with us have trouth in hand:\\n       Whose Marchandy commeth much into England.\\n       They ben our friends, with their commodities,\\n       And wee English passen into their countr',\n",
       " {'links': {(300, 308): {'1000090': 1.0}}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('found',len(entities),'entities')\n",
    "entities[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('England', 2189),\n",
       " ('English', 1131),\n",
       " ('Spaniards', 1004),\n",
       " ('', 738),\n",
       " ('Spaine', 665),\n",
       " ('Countrey', 654),\n",
       " ('Indians', 618),\n",
       " ('Iland', 615),\n",
       " ('America', 580),\n",
       " ('Guiana', 564)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "place_counts = Counter(place_names)\n",
    "# place_counts['England'] == 797\n",
    "place_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick digression, what do we get with our TGN number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels\n",
      "------\n",
      "en: Kingston upon Thames [prefLabel]\n",
      "und: Moreford [altLabel]\n",
      "en: Kingston [altLabel]\n",
      "und: Cyningestum [altLabel]\n",
      "Notes\n",
      "-----\n",
      "en: Residential suburb of London; fomerly county town of Surrey, until absorbed by Greater London; 11 Saxon kings crowned here; after London Bridge, first bridge above River Thames built here ca. 1750; once center for brewing, tanning & river barge traffic. [scopeNote]\n"
     ]
    }
   ],
   "source": [
    "from skosprovider_getty.providers import TGNProvider\n",
    "aat = TGNProvider(metadata={'id': 'TGN'})\n",
    "def get_place_name(id:int) -> str:\n",
    "    place = aat.get_by_id(id)\n",
    "\n",
    "    print('Labels')\n",
    "    print('------')\n",
    "    for l in place.labels:\n",
    "       print(l.language + ': ' + l.label + ' [' + l.type + ']')\n",
    "\n",
    "    print('Notes')\n",
    "    print('-----')\n",
    "    for n in place.notes:\n",
    "        print(n.language + ': ' + n.note + ' [' + n.type + ']')\n",
    "    \n",
    "links = entities[2][1]['links']\n",
    "for id in links.keys():\n",
    "    tgn_id = list(links[id].keys())[0]\n",
    "    get_place_name(tgn_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the making an early modern place name model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_md'\n",
      "['tagger', 'parser', 'ner', 'entity_ruler']\n",
      "['ner', 'entity_ruler']\n",
      "Losses 0 {'ner': 1293.541628498584}\n",
      "Losses 1 {'ner': 1245.728833436966}\n",
      "Losses 2 {'ner': 1274.3573137521744}\n",
      "Losses 3 {'ner': 1171.7467963695526}\n",
      "Losses 4 {'ner': 1176.1816000938416}\n",
      "Losses 5 {'ner': 1194.9532996416092}\n",
      "Losses 6 {'ner': 1144.7757304906845}\n",
      "Losses 7 {'ner': 1133.6882915496826}\n",
      "Losses 8 {'ner': 1087.0501599013805}\n",
      "Losses 9 {'ner': 1070.9576173126698}\n",
      "Saved model to tgn\n",
      "Loading from tgn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ing your selfe to be</br>changed in your opinion, for all the false reports which</br></br></br>they may raise against him, nor to doubt that wee will</br>not accomplish at large all that he shall promise you on</br>our behalfe. Our Lord keepe and preserve your right</br>high and mightie person. Written in our royall Court</br>at \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Greenwich\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</br> the 20. of \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    July 1587\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ndoza sonne to the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Marques\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " of Cannette determining to</br>discover the sayd Streights from the South sea, sent from</br>Chili two ships under the conduct of a captaine called</br>Latherelio : but the danger to seeke these Streights by the</br>South sea is more then by the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    North sea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NAME</span>\n",
       "</mark>\n",
       "</br>, because all the</br>stormes of the \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    North sea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NAME</span>\n",
       "</mark>\n",
       "</br> come from the land, but in</br>the South sea all the windes and stormes come off the sea,</br>and force the ships to run upon the leeshore, insomuch</br>that the sayd \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " ships were cast away in fiftie degrees.\n",
       "   The seeking of these Streights of Magellan is so dangerous, and the voyage so troublesome, that i</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### from https://spacy.io/usage/training#ner ###\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    ruler = EntityRuler(nlp)\n",
    "    patterns = []\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            start = ent[0]\n",
    "            end = ent[1]\n",
    "            label_ = ent[2]\n",
    "            word = text[start:end]\n",
    "            row = {\"label\":label_, \"pattern\":word}\n",
    "            patterns.append(row)    \n",
    "    ruler.add_patterns(patterns)\n",
    "    nlp.add_pipe(ruler)\n",
    "    print(nlp.pipe_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\", \"entity_ruler\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        print(nlp.pipe_names)\n",
    "\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", itn, losses)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA[:2]:\n",
    "            doc = nlp2(text)\n",
    "            displacy.render(doc, style=\"ent\")\n",
    "            #print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            #print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "TRAIN_DATA = places[:10]\n",
    "main(model=\"en_core_web_md\",output_dir=\"./tgn\",n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Did it work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ITEM because that the kings most deare Uncle, the king of Denmarke, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Norway &amp; Sweveland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold &amp; great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring &amp; passage of such forain &amp; strange persons into his realme of Norwey &amp; other dominions, streits, territories, jurisdictions &amp; places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"./tgn\")\n",
    "\n",
    "doc = nlp(\n",
    "    \"ITEM because that the kings most deare Uncle, the king of Denmarke, Norway & Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring & passage of such forain & strange persons into his realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The army marched from Konia to \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kaiseria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (Caesarea), and thence to Sivas, where the feast of the \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Korbân\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    second\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two thousand\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).  \n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on early modern text not in the training data\n",
    "doc = nlp(\n",
    "    \"\"\"The army marched from Konia to Kaiseria (Caesarea), and thence to Sivas, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).  \n",
    "\"\"\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's time to train the entity linker! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "place_counts['Fynmarke'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with NER: Yep, it's a PERSON, but *which* person? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipedia2vec import Wikipedia2Vec\n",
    "wiki2vec = Wikipedia2Vec.load(MODEL_FILE)\n",
    "wiki2vec.most_similar(wiki2vec.get_word('yoda'), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_entities = {}\n",
    "for entity in entities: \n",
    "    id = entity[1]['links']\n",
    "    start, end = list(id.keys())[0]\n",
    "    word = entity[0][start:end]\n",
    "    frequency = place_counts[word]\n",
    "    place_id = list(id[(start,end)].keys())\n",
    "    kb_entities[place_id[0]] = (word, frequency)\n",
    "\n",
    "    \n",
    "#{\"Q2146908\": (\"American golfer\", 342), \"Q7381115\": (\"publisher\", 17)}\n",
    "result = {}\n",
    "\n",
    "for key,value in kb_entities.items():\n",
    "    if value not in result.values():\n",
    "        result[key] = value\n",
    "\n",
    "kb_entities = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in kb_entities.keys():\n",
    "    kb.add_alias(\n",
    "        alias = kb_entities[key][0],\n",
    "        entities = [key],\n",
    "        probabilities=[1.0]\n",
    "    )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    kb.add_alias(\n",
    "        alias=\"Russ Cochran\",\n",
    "        entities=[\"Q2146908\", \"Q7381115\"],\n",
    "        probabilities=[0.24, 0.7],  # the sum of these probabilities should not exceed 1\n",
    "    )\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "from pathlib import Path\n",
    "\n",
    "from bin.wiki_entity_linking.train_descriptions import EntityEncoder\n",
    "\n",
    "\n",
    "ENTITIES = kb_entities # {\"Q2146908\": (\"American golfer\", 342), \"Q7381115\": (\"publisher\", 17)}\n",
    "\n",
    "INPUT_DIM = 300  # dimension of pretrained input vectors\n",
    "DESC_WIDTH = 64  # dimension of output entity vectors\n",
    "\n",
    "\n",
    "def main(model=None, output_dir=None, n_iter=50):\n",
    "    \"\"\"Load the model, create the KB and pretrain the entity encodings.\n",
    "    If an output_dir is provided, the KB will be stored there in a file 'kb'.\n",
    "    The updated vocab will also be written to a directory in the output_dir.\"\"\"\n",
    "\n",
    "    nlp = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "\n",
    "    # check the length of the nlp vectors\n",
    "    if \"vectors\" not in nlp.meta or not nlp.vocab.vectors.size:\n",
    "        raise ValueError(\n",
    "            \"The `nlp` object should have access to pretrained word vectors, \"\n",
    "            \" cf. https://spacy.io/usage/models#languages.\"\n",
    "        )\n",
    "\n",
    "    kb = KnowledgeBase(vocab=nlp.vocab)\n",
    "\n",
    "    # set up the data\n",
    "    entity_ids = []\n",
    "    descriptions = []\n",
    "    freqs = []\n",
    "    for key, value in ENTITIES.items():\n",
    "        desc, freq = value\n",
    "        entity_ids.append(key)\n",
    "        descriptions.append(desc)\n",
    "        freqs.append(freq)\n",
    "\n",
    "    # training entity description encodings\n",
    "    # this part can easily be replaced with a custom entity encoder\n",
    "    encoder = EntityEncoder(\n",
    "        nlp=nlp,\n",
    "        input_dim=INPUT_DIM,\n",
    "        desc_width=DESC_WIDTH,\n",
    "        epochs=n_iter,\n",
    "    )\n",
    "    encoder.train(description_list=descriptions, to_print=True)\n",
    "\n",
    "    # get the pretrained entity vectors\n",
    "    embeddings = encoder.apply_encoder(descriptions)\n",
    "\n",
    "    # set the entities, can also be done by calling `kb.add_entity` for each entity\n",
    "    kb.set_entities(entity_list=entity_ids, freq_list=freqs, vector_list=embeddings)\n",
    "\n",
    "    # adding aliases, the entities need to be defined in the KB beforehand    \n",
    "    for key in kb_entities.keys():\n",
    "        kb.add_alias(\n",
    "            alias = kb_entities[key][0],\n",
    "            entities = [key],\n",
    "            probabilities=[1.0]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # test the trained model\n",
    "    print()\n",
    "    _print_kb(kb)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        kb_path = str(output_dir / \"kb\")\n",
    "        kb.dump(kb_path)\n",
    "        print()\n",
    "        print(\"Saved KB to\", kb_path)\n",
    "\n",
    "        vocab_path = output_dir / \"vocab\"\n",
    "        kb.vocab.to_disk(vocab_path)\n",
    "        print(\"Saved vocab to\", vocab_path)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def _print_kb(kb):\n",
    "    print(kb.get_size_entities(), \"kb entities:\", kb.get_entity_strings())\n",
    "    print(kb.get_size_aliases(), \"kb aliases:\", kb.get_alias_strings())\n",
    "\n",
    "\n",
    "main(model=\"./tgn\",output_dir=\"./tgn_kb\",n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compatible with: spaCy v2.2.3\n",
    "Last tested with: v2.2.3\n",
    "https://spacy.io/usage/training#entity-linker\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from spacy.symbols import PERSON\n",
    "from spacy.vocab import Vocab\n",
    "\n",
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# training data\n",
    "TRAIN_DATA = entities\n",
    "\n",
    "\n",
    "\n",
    "def main(kb_path, vocab_path=None, output_dir=None, n_iter=50):\n",
    "    \"\"\"Create a blank model with the specified vocab, set up the pipeline and train the entity linker.\n",
    "    The `vocab` should be the one used during creation of the KB.\"\"\"\n",
    "    vocab = Vocab().from_disk(vocab_path)\n",
    "    # create blank Language class with correct vocab\n",
    "    nlp = spacy.blank(\"en\", vocab=vocab)\n",
    "    nlp.vocab.vectors.name = \"spacy_pretrained_vectors\"\n",
    "    print(\"Created blank 'en' model with vocab from '%s'\" % vocab_path)\n",
    "\n",
    "    # Add a sentencizer component. Alternatively, add a dependency parser for higher accuracy.\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "    # Add a custom component to recognize \"Russ Cochran\" as an entity for the example training data.\n",
    "    # Note that in a realistic application, an actual NER algorithm should be used instead.\n",
    "    ruler = EntityRuler(nlp)\n",
    "    patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"russ\"}, {\"LOWER\": \"cochran\"}]}]\n",
    "    ruler.add_patterns(patterns)\n",
    "    nlp.add_pipe(ruler)\n",
    "\n",
    "    # Create the Entity Linker component and add it to the pipeline.\n",
    "    if \"entity_linker\" not in nlp.pipe_names:\n",
    "        # use only the predicted EL score and not the prior probability (for demo purposes)\n",
    "        cfg = {\"incl_prior\": False}\n",
    "        entity_linker = nlp.create_pipe(\"entity_linker\", cfg)\n",
    "        #kb = KnowledgeBase(vocab=nlp.vocab)\n",
    "        #kb.load_bulk(kb_path)\n",
    "        #print(\"Loaded Knowledge Base from '%s'\" % kb_path)\n",
    "        entity_linker.set_kb(kb)\n",
    "        nlp.add_pipe(entity_linker, last=True)\n",
    "\n",
    "    # Convert the texts to docs to make sure we have doc.ents set for the training examples.\n",
    "    # Also ensure that the annotated examples correspond to known identifiers in the knowlege base.\n",
    "    kb_ids = nlp.get_pipe(\"entity_linker\").kb.get_entity_strings()\n",
    "    TRAIN_DOCS = []\n",
    "    for text, annotation in TRAIN_DATA:\n",
    "        with nlp.disable_pipes(\"entity_linker\"):\n",
    "            doc = nlp(text)\n",
    "        annotation_clean = annotation\n",
    "        for offset, kb_id_dict in annotation[\"links\"].items():\n",
    "            new_dict = {}\n",
    "            for kb_id, value in kb_id_dict.items():\n",
    "                if kb_id in kb_ids:\n",
    "                    new_dict[kb_id] = value\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Removed\", kb_id, \"from training because it is not in the KB.\"\n",
    "                    )\n",
    "            annotation_clean[\"links\"][offset] = new_dict\n",
    "        TRAIN_DOCS.append((doc, annotation_clean))\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"entity_linker\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train entity linker\n",
    "        # reset and initialize the weights randomly\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DOCS)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DOCS, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                    sgd=optimizer,\n",
    "                )\n",
    "            print(itn, \"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    _apply_model(nlp)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print()\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        _apply_model(nlp2)\n",
    "\n",
    "\n",
    "def _apply_model(nlp):\n",
    "    for text, annotation in TRAIN_DATA:\n",
    "        # apply the entity linker which will now make predictions for the 'Russ Cochran' entities\n",
    "        doc = nlp(text)\n",
    "        print()\n",
    "        print(\"Entities\", [(ent.text, ent.label_, ent.kb_id_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_kb_id_) for t in doc])\n",
    "\n",
    "\n",
    "main(kb_path=\"/Users/ds/projects/spaCy_workshops/iSchool/tgn_kb\", vocab_path=\"/Users/ds/projects/spaCy_workshops/iSchool/tgn_kb/vocab\",output_dir=\"./tgn_kb_1\", n_iter=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab().from_disk(\"./tgn_kb/vocab\")\n",
    "kb = KnowledgeBase(vocab=nlp.vocab)\n",
    "kb.load_bulk(\"/Users/ds/projects/spaCy_workshops/iSchool/tgn_kb/kb\")\n",
    "kb_ids = nlp.get_pipe(\"entity_linker\").kb.get_entity_strings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
