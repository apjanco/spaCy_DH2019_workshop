{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Linking with spaCy and TEI\n",
    "![](https://explosion.ai/blog/img/spacy-transformers.jpg)\n",
    "\n",
    "![](https://pbs.twimg.com/media/D0aHPzXWwAEgRwU?format=jpg&name=900x900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The School of Information Sciences\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", also The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    iSchool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " at Illinois, is a graduate school at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the University of Illinois at Urbana\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "–\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Champaign\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". Its Master of Science in Library and Information Science is currently accredited in full good standing by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the American Library Association\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\n",
    "\"The School of Information Sciences, also The iSchool at Illinois, is a graduate school at the University of Illinois at Urbana–Champaign. Its Master of Science in Library and Information Science is currently accredited in full good standing by the American Library Association.\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "*This works very well for many 20th and 21st century texts.  But what about early modern English?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ITEM because that the kings most \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    deare Uncle\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", the king of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Denmarke\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Norway &amp; Sweveland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold &amp; great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    soveraigne Lord the king of his Realme of England\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", by ye going in, entring &amp; passage of such forain &amp; strange persons into his realme of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Norwey &amp; other dominions\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", streits, territories, jurisdictions &amp; places subdued and subject to him, specially into his \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Iles\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " of Fynmarke, and elswhere, aswell in their persons as their things and goods</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"ITEM because that the kings most deare Uncle, the king of Denmarke, Norway & Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring & passage of such forain & strange persons into his realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"./out_of_domain.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this example, our goal is to teach an existing English-language model to identify early modern place names.\n",
    "\n",
    "There are several approaches that we could take to this problem.  Different approaches can lend better or worse results and experimentation is an essential part of any machine learning project. \n",
    "\n",
    "#### How can we teach a statistical language model that Sweveland is a place?\n",
    "\n",
    "Richard Hakluyt's The Principal Navigations, Voyages, Traffiques, and Discoveries of the English Nation (1599)\n",
    "\n",
    "![](http://www.sequiturbooks.com/image/cache/Product%20Images/2015-12/The-Principal-1512150003/5ae35178-800x800.jpeg)\n",
    "\n",
    "--- \n",
    "\n",
    "### Download the TEI files from Persius \n",
    "- We're going to extract a list of all the place names from the text to create training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xmlParseEntityRef: no name, line 103, column 75 (<string>, line 103)\n",
      "xmlParseEntityRef: no name, line 199, column 94 (<string>, line 199)\n",
      "xmlParseEntityRef: no name, line 186, column 94 (<string>, line 186)\n",
      "xmlParseEntityRef: no name, line 803, column 109 (<string>, line 803)\n",
      "xmlParseEntityRef: no name, line 455, column 89 (<string>, line 455)\n",
      "xmlParseEntityRef: no name, line 441, column 89 (<string>, line 441)\n",
      "Unescaped '<' not allowed in attributes values, line 22, column 25 (<string>, line 22)\n",
      "xmlParseEntityRef: no name, line 49, column 152 (<string>, line 49)\n",
      "xmlParseEntityRef: no name, line 6, column 152 (<string>, line 6)\n",
      "xmlParseEntityRef: no name, line 4, column 111 (<string>, line 4)\n",
      "xmlParseEntityRef: no name, line 34, column 106 (<string>, line 34)\n",
      "xmlParseEntityRef: no name, line 3, column 149 (<string>, line 3)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pickle\n",
    "from collections import Counter\n",
    "spec = {\"tei\":\"http://www.tei-c.org/ns/1.0\"}\n",
    "from urllib.request import urlopen\n",
    "from lxml import etree\n",
    "from standoffconverter import Converter\n",
    "\n",
    "def tei_loader(url):\n",
    "    tei = urlopen(url).read()\n",
    "    return etree.XML(tei)\n",
    "\n",
    "table_of_contents_url = \"http://www.perseus.tufts.edu/hopper/xmltoc?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D1\"\n",
    "table_of_contents_xml = tei_loader(table_of_contents_url)\n",
    "\n",
    "\n",
    "chunks = table_of_contents_xml.xpath(\"//chunk[@ref]\")\n",
    "refs = [chunk.get('ref') for chunk in chunks] \n",
    "# an example ref 'Perseus%3Atext%3A1999.03.0070%3Anarrative%3D6'\n",
    "\n",
    "\n",
    "standoffs = []\n",
    "\n",
    "for ref in refs:\n",
    "    try:\n",
    "        url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + ref\n",
    "\n",
    "        tei = tei_loader(url)\n",
    "        so = Converter.from_tree(tei)\n",
    "        standoffs.append(so)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA branch of a Statute made in the eight yeere of Henry the sixt, for the trade to Norwey, Sweveland, Den marke, and Fynmarke. \\nITEM because that the kings most deare Uncle, the king\\nof Denmarke, Norway\\n & Sweveland, as the same our\\nsoveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils,\\nhurts and damage which have late happened aswell to\\nhim and his, as to other foraines and strangers, and also\\nfriends and speciall subjects of our said soveraigne Lord\\n\\n\\nthe king of his Realme of England, by ye going in,\\nentring & passage of such forain & strange persons into\\nhis realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him,\\nspecially into his Iles of Fynmarke, and elswhere, aswell\\nin their persons as their things and goods: for eschuing\\nof such losses, perils, hurts & damages, and that such\\nlike (which God forbid) should not hereafter happen: our\\nsaid soveraigne Lord the king hath ordeined and statuted,\\nthat all and singular strangers, aswell Englishmen and\\nothers willing to apply by Ship and come into his Realme\\nof Norwey and other dominions, streits, territories,\\njurisdictions, Isles & places aforesaid with their ships,\\nto the intent to get or have fish or any other Marchandises,\\nor goods, shall apply and come to his Towne of Northberne, where the said king of Denmarke hath specially\\nordained and stablished his staple for the concourses of\\nstrangers and specially of Englishmen, to the exercise\\nof such Marchandises: granting to the said Englishmen\\nthat they shall there injoy in and by all things the same\\nfavour, privileges and prerogatives which they of the\\nHans\\n did enjoy. Therefore our said soveraigne Lord\\nthe king willing the love, affinitie and amities to be\\nfirmely observed, which betwixt his said Uncle and his\\nnoble progenitors of good memory, their Realmes, lands,\\ndominions, streites, territories, jurisdictions and their\\nsaid places, and the same our soveraigne Lord the king\\n& his noble progenitours of famous memory, his great\\nmen, subjects, Realmes, lands & dominions hath bene\\nof old times hitherto continued, nor nothing by our said\\nsoveraigne Lord the king or his people to be attempted\\nor done whereby such amities by reason of any dissensions, enemities or discords might be broken: by the\\nadvise of the Lords spirituall & temporall & of the\\ncommons of his said Realme of England, assembled in\\nthis present Parliament, hath ordained, prohibiting that\\nnone of his liege people nor subjects of his Realme of\\nEngland by audacitie of their follie presume to enter the\\nRealmes, lands, dominions, straits, territories, jurisdictions & places of the said king of Denmarke against ye\\nordinance, prohibition & interdiction of ye same his Uncle\\nabove remembred, & in contempt of the same, upon paine\\nof forfeiture of all their moveable goods & imprisonment\\nof their persons at the kings will.\\n\\n\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's the text from the TEI document \n",
    "standoffs[0].plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"begin\": 0,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 0,\n",
      "    \"tag\": \"TEI.2\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 0,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {\n",
      "      \"lang\": \"en\"\n",
      "    },\n",
      "    \"depth\": 1,\n",
      "    \"tag\": \"text\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 0,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 2,\n",
      "    \"tag\": \"body\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 0,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"narrative\",\n",
      "      \"org\": \"uniform\",\n",
      "      \"sample\": \"complete\"\n",
      "    },\n",
      "    \"depth\": 3,\n",
      "    \"tag\": \"div1\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1,\n",
      "    \"end\": 127,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 4,\n",
      "    \"tag\": \"head\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 50,\n",
      "    \"end\": 55,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"pers\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 83,\n",
      "    \"end\": 89,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 91,\n",
      "    \"end\": 100,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 117,\n",
      "    \"end\": 125,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 128,\n",
      "    \"end\": 2933,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 4,\n",
      "    \"tag\": \"p\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 178,\n",
      "    \"end\": 194,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"pers\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 196,\n",
      "    \"end\": 203,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"  +Norway [10,62] (nation), Europe \",\n",
      "      \"type\": \"place\",\n",
      "      \"key\": \"tgn,1000088\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 206,\n",
      "    \"end\": 215,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 511,\n",
      "    \"end\": 511,\n",
      "    \"attrib\": {\n",
      "      \"n\": \"173\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"pb\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 528,\n",
      "    \"end\": 545,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"England\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 633,\n",
      "    \"end\": 639,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 751,\n",
      "    \"end\": 767,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"Fynmarke\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1057,\n",
      "    \"end\": 1067,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1122,\n",
      "    \"end\": 1138,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"Norwey\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1287,\n",
      "    \"end\": 1299,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1339,\n",
      "    \"end\": 1358,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"Northberne\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1375,\n",
      "    \"end\": 1391,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"pers\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1491,\n",
      "    \"end\": 1501,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1527,\n",
      "    \"end\": 1539,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1562,\n",
      "    \"end\": 1572,\n",
      "    \"attrib\": {},\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 1685,\n",
      "    \"end\": 1690,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"  +Hans (inhabited place), Marne, Champagne-Ardenne, France, Europe \",\n",
      "      \"type\": \"place\",\n",
      "      \"key\": \"tgn,4004801\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 2410,\n",
      "    \"end\": 2427,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"England\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 2544,\n",
      "    \"end\": 2561,\n",
      "    \"attrib\": {\n",
      "      \"reg\": \"England\",\n",
      "      \"type\": \"place\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 2696,\n",
      "    \"end\": 2712,\n",
      "    \"attrib\": {\n",
      "      \"type\": \"pers\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"name\"\n",
      "  },\n",
      "  {\n",
      "    \"begin\": 2932,\n",
      "    \"end\": 2932,\n",
      "    \"attrib\": {\n",
      "      \"n\": \"174\"\n",
      "    },\n",
      "    \"depth\": 5,\n",
      "    \"tag\": \"pb\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Here are the annotations \n",
    "print(json.dumps(json.loads(standoffs[0].to_json()), indent=2)) # or just standoffs[0].to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text from the TEI document and create training data\n",
    "import json\n",
    "places = []\n",
    "entities = []\n",
    "for standoff in standoffs:\n",
    "    for annotation in json.loads(standoff.to_json()):\n",
    "        try:\n",
    "            if annotation['attrib']['type'] == 'place':\n",
    "                begin = annotation['begin']\n",
    "                end = annotation['end']\n",
    "                length = end-begin\n",
    "                key = annotation['attrib']['key']\n",
    "                key = key.split(',')[1]\n",
    "                #modern_name = annotation['attrib']['reg']\n",
    "                sent = standoff.plain[begin-300:end+ 300]\n",
    "                assert len(sent) > 0\n",
    "                begin = 300\n",
    "                end = begin+length\n",
    "                place = (sent, {'entities':[(begin,end,\"GPE\")]})\n",
    "                places.append(place)\n",
    "                \n",
    "                dict_1 = {(begin, end): {key: 1.0,}}\n",
    "                entities.append((sent, {\"links\": dict_1}))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the documentation for training the named entity recognizer. Note the format expected for training data:  https://spacy.io/usage/training#ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 10000 places\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('and brede.\\nWhat hath then Flanders, bee Flemings lieffe or loth,\\nBut a little Mader and Flemish Cloth:\\nBy Drapering of our wooll in substance\\nLiven her commons, this is her governance,\\nWithout wich they may not live at ease.\\nThus must hem sterve, or with us must have peace.\\n\\n\\n\\nOf the commodities of Portugal\\n. The second Chapter.\\n\\n       THE Marchandy also of Portugal\\n\\n       By divers lands turne into sale.\\n       Portugalers with us have trouth in hand:\\n       Whose Marchandy commeth much into England.\\n       They ben our friends, with their commodities,\\n       And wee English passen into their countr',\n",
       " {'entities': [(300, 309, 'GPE')]})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('found',len(places),'places')\n",
    "places[4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the format for entity-linker training data: https://spacy.io/usage/training#entity-linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 10000 entities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('and brede.\\nWhat hath then Flanders, bee Flemings lieffe or loth,\\nBut a little Mader and Flemish Cloth:\\nBy Drapering of our wooll in substance\\nLiven her commons, this is her governance,\\nWithout wich they may not live at ease.\\nThus must hem sterve, or with us must have peace.\\n\\n\\n\\nOf the commodities of Portugal\\n. The second Chapter.\\n\\n       THE Marchandy also of Portugal\\n\\n       By divers lands turne into sale.\\n       Portugalers with us have trouth in hand:\\n       Whose Marchandy commeth much into England.\\n       They ben our friends, with their commodities,\\n       And wee English passen into their countr',\n",
       " {'links': {(300, 309): {'1000090': 1.0}}})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('found',len(entities),'entities')\n",
    "entities[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick digression, what do we get with our TGN number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels\n",
      "------\n",
      "en: Kingston upon Thames [prefLabel]\n",
      "und: Cyningestum [altLabel]\n",
      "en: Kingston [altLabel]\n",
      "und: Moreford [altLabel]\n",
      "Notes\n",
      "-----\n",
      "en: Residential suburb of London; fomerly county town of Surrey, until absorbed by Greater London; 11 Saxon kings crowned here; after London Bridge, first bridge above River Thames built here ca. 1750; once center for brewing, tanning & river barge traffic. [scopeNote]\n"
     ]
    }
   ],
   "source": [
    "from skosprovider_getty.providers import TGNProvider\n",
    "aat = TGNProvider(metadata={'id': 'TGN'})\n",
    "def get_place_name(id:int) -> str:\n",
    "    place = aat.get_by_id(id)\n",
    "\n",
    "    print('Labels')\n",
    "    print('------')\n",
    "    for l in place.labels:\n",
    "       print(l.language + ': ' + l.label + ' [' + l.type + ']')\n",
    "\n",
    "    print('Notes')\n",
    "    print('-----')\n",
    "    for n in place.notes:\n",
    "        print(n.language + ': ' + n.note + ' [' + n.type + ']')\n",
    "    \n",
    "links = entities[2][1]['links']\n",
    "for id in links.keys():\n",
    "    tgn_id = list(links[id].keys())[0]\n",
    "    get_place_name(tgn_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the making an early modern place name model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe('ner')\n",
    "ner.add_label(\"TGN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-34ee4596a404>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# batch of annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# dropout - make it harder to memorise data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 )\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Losses\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spacy22/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update.finish_parser_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.make_updates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.precompute_hiddens.begin_update.backward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.precompute_hiddens._nonlinearity.backprop_nonlinearity\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mops.pyx\u001b[0m in \u001b[0;36mthinc.neural.ops.NumpyOps.asarray\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spacy22/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"Convert the input to an array.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "TRAIN_DATA = places\n",
    "n_iter = 1\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "\n",
    "    for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "    # Loop for 10 iterations\n",
    "    \n",
    "    nlp.to_disk(\"./tgn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_lg'\n",
      "Losses {'ner': 24174.444000461794}\n",
      "Losses {'ner': 20239.468402190556}\n",
      "Losses {'ner': 19364.363913456233}\n",
      "Losses {'ner': 18863.677987238767}\n",
      "Losses {'ner': 18814.93196198962}\n"
     ]
    }
   ],
   "source": [
    "### from https://spacy.io/usage/training#ner ###\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", itn, losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "TRAIN_DATA = places\n",
    "main(model=\"en_core_web_lg\",output_dir=\"./tgn\",n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Did it work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajanco/anaconda3/envs/spacy22/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">ITEM because that the kings most deare Uncle, the king of Denmarke, Norway &amp; Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold &amp; great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring &amp; passage of such forain &amp; strange persons into his realme of Norwey &amp; other dominions, streits, territories, jurisdictions &amp; places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"./tgn\")\n",
    "\n",
    "doc = nlp(\n",
    "    \"ITEM because that the kings most deare Uncle, the king of Denmarke, Norway & Sweveland, as the same our soveraigne Lord the king of his intimation hath understood, considering the manifold & great losses, perils, hurts and damage which have late happened aswell to him and his, as to other foraines and strangers, and also friends and speciall subjects of our said soveraigne Lord the king of his Realme of England, by ye going in, entring & passage of such forain & strange persons into his realme of Norwey & other dominions, streits, territories, jurisdictions & places subdued and subject to him, specially into his Iles of Fynmarke, and elswhere, aswell in their persons as their things and goods\"\n",
    ")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now it's time to train the entity linker! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "import spacy\n",
    "from spacy.kb import KnowledgeBase\n",
    "\n",
    "from bin.wiki_entity_linking.train_descriptions import EntityEncoder\n",
    "\n",
    "\n",
    "INPUT_DIM = 300  # dimension of pretrained input vectors\n",
    "DESC_WIDTH = 64  # dimension of output entity vectors\n",
    "n_iter=50\n",
    "\n",
    "nlp = spacy.load(\"./ner_model\") \n",
    "\n",
    "\n",
    "# check the length of the nlp vectors\n",
    "if \"vectors\" not in nlp.meta or not nlp.vocab.vectors.size:\n",
    "    raise ValueError(\n",
    "        \"The `nlp` object should have access to pretrained word vectors, \"\n",
    "        \" cf. https://spacy.io/usage/models#languages.\"\n",
    "    )\n",
    "\n",
    "kb = KnowledgeBase(vocab=nlp.vocab)\n",
    "\n",
    "# set up the data\n",
    "entity_ids = []\n",
    "descriptions = []\n",
    "freqs = []\n",
    "for key, value in entities.items():\n",
    "    desc, freq = value\n",
    "    entity_ids.append(key)\n",
    "    descriptions.append(desc)\n",
    "    freqs.append(freq)\n",
    "\n",
    "# training entity description encodings\n",
    "# this part can easily be replaced with a custom entity encoder\n",
    "encoder = EntityEncoder(\n",
    "    nlp=nlp,\n",
    "    input_dim=INPUT_DIM,\n",
    "    desc_width=DESC_WIDTH,\n",
    "    epochs=n_iter,\n",
    ")\n",
    "encoder.train(description_list=descriptions, to_print=True)\n",
    "\n",
    "# get the pretrained entity vectors\n",
    "embeddings = encoder.apply_encoder(descriptions)\n",
    "\n",
    "# set the entities, can also be done by calling `kb.add_entity` for each entity\n",
    "kb.set_entities(entity_list=entity_ids, freq_list=freqs, vector_list=embeddings)\n",
    "\n",
    "# adding aliases, the entities need to be defined in the KB beforehand\n",
    "kb.add_alias(\n",
    "    alias=\"Russ Cochran\",\n",
    "    entities=[\"Q2146908\", \"Q7381115\"],\n",
    "    probabilities=[0.24, 0.7],  # the sum of these probabilities should not exceed 1\n",
    ")\n",
    "\n",
    "# test the trained model\n",
    "print()\n",
    "_print_kb(kb)\n",
    "\n",
    "# save model to output directory\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    kb_path = str(output_dir / \"kb\")\n",
    "    kb.dump(kb_path)\n",
    "    print()\n",
    "    print(\"Saved KB to\", kb_path)\n",
    "\n",
    "    vocab_path = output_dir / \"vocab\"\n",
    "    kb.vocab.to_disk(vocab_path)\n",
    "    print(\"Saved vocab to\", vocab_path)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # test the saved model\n",
    "    # always reload a knowledge base with the same vocab instance!\n",
    "    print(\"Loading vocab from\", vocab_path)\n",
    "    print(\"Loading KB from\", kb_path)\n",
    "    vocab2 = Vocab().from_disk(vocab_path)\n",
    "    kb2 = KnowledgeBase(vocab=vocab2)\n",
    "    kb2.load_bulk(kb_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy22",
   "language": "python",
   "name": "spacy22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
