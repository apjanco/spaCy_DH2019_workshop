{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Processing with spaCy\n",
    "\n",
    "## Installation and Basic Use\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Installation\n",
    "2. Basic uses\n",
    "3. Project: Creating a vocabulary list from words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Installing spaCy\n",
    "2. Installing models\n",
    "3. Loading spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy\n",
    "\n",
    "You should have Python 3 installed. Dependencies for spaCy may take a while to install.\n",
    "\n",
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models\n",
    "\n",
    "SpaCy relies on installed language models. Before we do anything, we have to install at least one language model. We will install the medium English (91MB) and small German models (10MB).\n",
    "\n",
    "python -m spacy download en_core_web_md\n\n",
    "python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading spaCy\n",
    "\n",
    "spaCy is now ready to load in Python. Let's load the English model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Uses\n",
    "\n",
    "1. Loading a text\n",
    "2. Lemma, parts of speech and stopwords\n",
    "3. Assessing similarity\n",
    "4. Working with entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a text\n",
    "\n",
    "Feeling festive, I chose two short sections of *A Christmas Carol* by Charles Dickens to analyze with spaCy. We will open them and process them with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookLoc1 = 'xmascarol1.txt'\n",
    "bookLoc2 = 'xmascarol2.txt'\n",
    "\n",
    "with open(bookLoc1, 'r') as f:\n",
    "    text1 = f.read()\n",
    "    \n",
    "with open(bookLoc2, 'r') as f:\n",
    "    text2 = f.read()\n",
    "    \n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Lemma, parts of speech and stopwords\n",
    "\n",
    "You can iterate through a spaCy document object for its tokens. The token objects allow identification of things like the word lemma, its part of speech, its shape and whether it is a common word (a stopword). Let's take the first part of the text and interate through the tokens for:\n",
    "\n",
    "1. lemma (stemmed word)\n",
    "2. part of speech (e.g., noun)\n",
    "3. syntactic dependecy (what it relates to in the sentence)\n",
    "4. shape (capitalization)\n",
    "5. stopword (whether it is a common word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(token.text,\n",
    "         token.lemma_,\n",
    "         token.pos_,\n",
    "         token.dep_,\n",
    "         token.shape_,\n",
    "         token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linguistic Similarity\n",
    "\n",
    "What if we want to see if texts are similar to one another? Out of the box, spaCy can compare texts based on linguistic features. Let's take the two parts of *A Christmas Carol*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, these two short texts are pretty similar. Let's load part of a series of stories that Dickens published anonymously as Boz and is compiled as *Sketches by Boz, Illustrative of Every-Day Life and Every-Day People* and test for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bozLoc = 'sketchesboz.txt'\n",
    "    \n",
    "with open(bozLoc,'r',encoding='utf8') as f:\n",
    "    bozText = f.read()\n",
    "\n",
    "bozDoc = nlp(bozText)\n",
    "\n",
    "doc1.similarity(bozDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only slightly less similar than the two parts of *A Christmas Carol*. Let's try Dickens against parts of Joyce's *Ulysses*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulyssesLoc = 'ulysses.txt'\n",
    "\n",
    "with open(ulyssesLoc, 'r',encoding='utf8') as f:\n",
    "    ulyssesText = f.read()\n",
    "\n",
    "ulyssesDoc = nlp(ulyssesText)\n",
    "\n",
    "bozDoc.similarity(ulyssesDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less similar although also very similar. What if we try against something very different, [an article about the NBA finals from fivethirtyeight.com](https://fivethirtyeight.com/features/warriors-raptors-game-2-nba-finals/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basketballLoc = 'basketball.txt'\n",
    "\n",
    "with open(basketballLoc, 'r',encoding='utf8') as f:\n",
    "    basketballText = f.read()\n",
    "\n",
    "basketballDoc = nlp(basketballText)\n",
    "\n",
    "bozDoc.similarity(basketballDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less similar still, although still pretty similar. A different model or a trained model might give more differentiating results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Entities\n",
    "\n",
    "A useful feature in spaCy is its ability to distinguish entities. Entities are properly named people or places. Out of the box, spaCy does a pretty good job of finding names, institutions and locations. We can iterate through the entities in the text just like tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in basketballDoc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems with this index, of course. \"Six dimes\" is slang for six assists, for example. If you were interested in making a gazeteer (an place index), it would be easy to do by only printing those entities that have geographical qualities. These are \"geographical or political entities\" (GPE) or \"locations\" (LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in basketballDoc.ents:\n",
    "    if ent.label_ in ['GPE','LOC']:\n",
    "        print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Creating a List of Words for Language Study\n",
    "\n",
    "1. Eliminating unwanted tokens\n",
    "2. Creating an updating set\n",
    "\n",
    "Let's say you are teaching an ESL class for beginners. There are many texts you could assign, but how can you make a vocabulary list? And once you have a list for one text, how can you make sure the words do not repeat?\n",
    "\n",
    "Let's make a set of the words we need to learn and that we have already learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2learn = set()\n",
    "learnedWords = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminating unwanted tokens\n",
    "\n",
    "We need to iterate through the list of tokens from our document and make sure that easy words or non-words do not end up in the list. The first thing to do is to eliminate tokens that should not be in the list. These include punctuation (PUNCT), proper nouns (PROPN), spaces (SPACE), numbers (NUM) and symbols (SYM). Simply test to see if the tokens part of speech is one of these parts. We also should test to see if the token is a stopword so that overly simple words do not end up on the list.\n",
    "\n",
    "Here we iterate through the first part of *A Christmas Carol* and add the lemma and its part of speech to our learning list. Doing so will mean that strings that could represent different parts of speech (e.g., \"run\") will both end up in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    if token.pos_ not in ['PUNCT','PROPN','SPACE','NUM','SYM'] and not token.is_stop:\n",
    "        words2learn.add((token.lemma_,token.pos_))\n",
    "\n",
    "for word in words2learn:\n",
    "    print(word[0],word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it might make sense to write up this part of the code as a function so we can use it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usableWords(doc):\n",
    "    w2l = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['PUNCT','PROPN','SPACE','NUM','SYM'] and not token.is_stop:\n",
    "            w2l.add((token.lemma_,token.pos_))\n",
    "    return w2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume the student learned the words from the first part of *A Christmas Carol* and wants to learn new words from the second part. We can put the learned words in that set and figure out what words need to be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnedWords = learnedWords|words2learn\n",
    "\n",
    "newWords = usableWords(doc2)\n",
    "\n",
    "words2learn.clear()\n",
    "\n",
    "print(newWords-learnedWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a set that updates\n",
    "\n",
    "It might be useful to take a corpus of texts. There is a site called ESL Fast that has short, easy texts. Using the  urllib module and the HTML parsing module BeautifulSoup, we can download these texts and make lists from them. If you need to download BeautifulSoup, do so in your terminal:\n",
    "\n",
    "pip install beautifulsoup4\n",
    "\n",
    "Let's import these modules and download the first fourteen \"supereasy\" texts on the site. If you don't know how to use BeautifulSoup, you might look at [The Programming Historian's lesson](https://programminghistorian.org/en/lessons/intro-to-beautiful-soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "texts = []\n",
    "\n",
    "for story in range(1,15):\n",
    "    url = 'https://www.eslfast.com/supereasy/se/supereasy' + ('00'+str(story))[-3:] + '.htm'\n",
    "    page = urllib.request.urlopen(url).read()\n",
    "    text = BeautifulSoup(page, \"html.parser\").find(class_='MsoNormal').text\n",
    "    texts.append(text)\n",
    "    \n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that takes our learned words plus our new text and generates a list of words we need to learn and prints the words to learn, and one that adds the learned words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolearn(oldWords, newDoc):\n",
    "    words = usableWords(newDoc)\n",
    "    newWords = words-oldWords\n",
    "    for word in newWords:\n",
    "        print(word[0],'('+word[1]+')')\n",
    "    return newWords\n",
    "\n",
    "def learned(oldWords, newWords):\n",
    "    return oldWords|newWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can iterate through our texts, get the words to learn for each lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnedWords = set()\n",
    "\n",
    "for text in texts:\n",
    "    print('\\n***LESSON '+str(texts.index(text)+1)+'***')\n",
    "    textDoc = nlp(text)\n",
    "    words2learn = tolearn(learnedWords,textDoc)\n",
    "    learnedWords = learned(learnedWords,words2learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
