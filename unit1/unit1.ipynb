{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction to \n",
    "<img src=\"https://miro.medium.com/max/1200/1*HTtQseukwrBiREJf8MSVcA.jpeg\" alt=\"Spacy Logo\" style=\"width: 600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Main Documentation Page](https://spacy.io/)  \n",
    "- [How to install spaCy](https://spacy.io/usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text\n",
    "H.G. Wells, *The Invisible Man*\n",
    "<img src=\"https://www.slashfilm.com/wp/wp-content/images/invisible-man-cast-new.jpg\" alt=\"Spacy Logo\" style=\"width: 600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "book = requests.get('http://www.gutenberg.org/cache/epub/5230/pg5230.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without spaCy, Python is able to process text as a sequence of charachters (called a string).  We can slice a string, we can add strings, replace sections of a string and many other tasks.  See [w3schools string functions](https://www.w3schools.com/python/python_ref_string.asp)\n",
    "\n",
    "Common examples for working with strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ourself; everyone else is a'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slicing  [begin : end ]\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "wilde[4:-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Be a fish; everything else is already taken.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find and replace\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "wilde.replace('yourself', 'a fish').replace('everyone', 'everything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Be', 'yourself;', 'everyone', 'else', 'is', 'already', 'taken.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split\n",
    "wilde = 'Be yourself; everyone else is already taken.'\n",
    "wilde.split() #also try wilde.split(';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy takes this a step further to give the machine an understanding of text, not just as a sequence of charachters, but as natural language\n",
    "\n",
    "[A full list of current languages](https://github.com/explosion/spaCy/tree/master/spacy/lang)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "\n",
    "nlp = German()\n",
    "doc = nlp('Sei du selbst! Alle anderen sind bereits vergeben.')\n",
    "\n",
    "\n",
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "doc = nlp('Be yourself; everyone else is already taken.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The document object\n",
    "Once we have imported a language model and a text, spaCy will create what is called a document (doc) object.  \n",
    "The doc object typically contains:\n",
    "\n",
    "\n",
    "|   [attributes](https://spacy.io/api/doc#attributes) |   | \n",
    "|---|---|\n",
    "| tokens  | doc[:5]  |\n",
    "|  text  | doc.text\n",
    "| sentences  | doc.sents |\n",
    "| entities | doc.ents |\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/doc#_title).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Note the difference between working with a slice of a doc object versus a Python string**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be \n",
      "Be yourself;\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Also note how spaCy tokenization differs from Python split()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Python:\n",
      "Be\n",
      "yourself;\n",
      "everyone\n",
      "else\n",
      "is\n",
      "already\n",
      "taken.\n",
      "------\n",
      "[*] spaCy:\n",
      "Be\n",
      "yourself\n",
      ";\n",
      "everyone\n",
      "else\n",
      "is\n",
      "already\n",
      "taken\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Note the difference between working with a slice in a doc object versus a Python string \n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "display(Markdown('**Note the difference between working with a slice of a doc object versus a Python string**'))\n",
    "\n",
    "print(wilde[:3])\n",
    "print(doc[:3])\n",
    "\n",
    "display(Markdown('**Also note how spaCy tokenization differs from Python split()**'))\n",
    "print('[*] Python:')\n",
    "for token in wilde.split():\n",
    "    print(token)\n",
    "    \n",
    "print('------')    \n",
    "print('[*] spaCy:')\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Be yourself; everyone else is already taken.',\n",
       " 'tokens': [{'id': 0, 'start': 0, 'end': 2},\n",
       "  {'id': 1, 'start': 3, 'end': 11},\n",
       "  {'id': 2, 'start': 11, 'end': 12},\n",
       "  {'id': 3, 'start': 13, 'end': 21},\n",
       "  {'id': 4, 'start': 22, 'end': 26},\n",
       "  {'id': 5, 'start': 27, 'end': 29},\n",
       "  {'id': 6, 'start': 30, 'end': 37},\n",
       "  {'id': 7, 'start': 38, 'end': 43},\n",
       "  {'id': 8, 'start': 43, 'end': 44}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The to_json() method is a useful way to look at all the information contained in the doc \n",
    "doc.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Tokens\n",
    "As you can see above, the doc contains a split of the text into tokens.  Each token object has 65 attributes that can be used during analysis.  Common tasks include:\n",
    "- removing all punctuation from the text\n",
    "- counting root forms of the words (lemmata)\n",
    "- removing stopwords from the doc\n",
    "\n",
    "\n",
    "|   [attributes](https://spacy.io/api/token#attributes) |   | \n",
    "|---|---|\n",
    "| root form (lemma)  | token.lemma_  |\n",
    "| Named entity type  | token.ent_type_ |\n",
    "| token is punctuation  | token.is_punct |\n",
    "| part of speech | token.pos_ |\n",
    "| in stop words | token.is_stop |\n",
    "\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/token#_title).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be Be   Xx True\n",
      "yourself yourself   xxxx True\n",
      "; ;   ; False\n",
      "everyone everyone   xxxx True\n",
      "else else   xxxx True\n",
      "is be   xx True\n",
      "already already   xxxx True\n",
      "taken take   xxxx False\n",
      ". .   . False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text,\n",
    "         token.lemma_,\n",
    "         token.pos_,\n",
    "         token.dep_,\n",
    "         token.shape_,\n",
    "         token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/SpaCy_logo.svg/1200px-SpaCy_logo.svg.png\" alt=\"Spacy Logo\" style=\"width: 80px;\"/>  \n",
    "##  Spans\n",
    "When studying text, we are often interested in features that involve more than one token.  To do this, we can create a span.  For example, \"New York City\"\n",
    "\n",
    "Span [attributes](https://spacy.io/api/span#attributes)\n",
    "\n",
    "Full documentation can be found [here](https://spacy.io/api/span#_title). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] spaCy 5 8 New York City\n",
      "[*] string 21 34 New York City\n"
     ]
    }
   ],
   "source": [
    "text = 'I just got back from New York City.'\n",
    "nlp = English()\n",
    "doc = nlp(text)\n",
    "nyc = doc[5:8] #or doc[-4:-1]\n",
    "\n",
    "print(\n",
    "    '[*] spaCy',\n",
    "    nyc.start,\n",
    "    nyc.end,\n",
    "    doc[nyc.start:nyc.end],\n",
    ")\n",
    "print(  \n",
    "    '[*] string',\n",
    "    nyc.start_char,\n",
    "    nyc.end_char,\n",
    "    text[nyc.start_char:nyc.end_char]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: create individualized vocabularly lists \n",
    "At Haverford, we have an application called [the Bridge](https://bridge.haverford.edu/) that generates custom vocabulary lists for learning Latin and ancient Greek.  To do this, we create a list of words from texts that the student has already read and understood.  We then use the lemma of each word to compare the list of known words against words in a new text.  We can then identify which words will be new to the reader.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/J%C3%B3kai_M%C3%B3r_litogr%C3%A1fia.jpg/220px-J%C3%B3kai_M%C3%B3r_litogr%C3%A1fia.jpg'>\n",
    "\n",
    "For current purposes, let's use two texts in Hungarian. Let's say that I'm learning Hungarian and reading Mór Jókai's *The novel of the next century* (1872).  I have just finished book one and want to know what new words I will encounter when reading book two.   \n",
    "\n",
    ">*Note* I am using Python sets to find the difference between the two books. I could also find the union, the intersection and other set operations.  For more on this topic, there is an excellent tutorial from [Real Python](https://realpython.com/python-sets/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I use the requests library to get the texts from Project Gutenberg\n",
    "import requests \n",
    "vol_1 = requests.get('http://www.gutenberg.org/files/55911/55911-0.txt')\n",
    "vol_2 = requests.get('http://www.gutenberg.org/files/55912/55912-0.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16812"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.hu import Hungarian \n",
    "nlp = Hungarian()\n",
    "nlp.max_length = 1070000 # This is needed given the length of the text \n",
    "\n",
    "vol_1_doc = nlp(vol_1.text)\n",
    "vol_1_words = set([token.lemma_ for token in vol_1_doc if token.is_stop is False and token.is_punct is False])\n",
    "\n",
    "vol_2_doc = nlp(vol_2.text)\n",
    "vol_2_words = set([token.lemma_ for token in vol_2_doc if token.is_stop is False and token.is_punct is False])\n",
    "\n",
    "new_words = vol_2_words.difference(vol_1_words)\n",
    "len(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ouch, that's far too many words to learn!  Let's only count the 100 most freqent words and then create our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "from collections import Counter\n",
    "\n",
    "# Add an extension to our tokens called \"count\"\n",
    "Token.set_extension(\"count\", default=False, force=True)\n",
    "\n",
    "\n",
    "# Calculate the number of times that a lemma appears in the text\n",
    "counts = Counter([token.lemma_ for token in vol_1_doc if not token.is_punct and not token.is_stop]).most_common(100)\n",
    "counts = dict(counts)\n",
    "\n",
    "# Add the count to each token. \n",
    "vol_1_doc = nlp(vol_1.text)\n",
    "for token in vol_1_doc:\n",
    "    if token.lemma_ in counts.keys():\n",
    "        token._.count = counts[token.lemma_]\n",
    "\n",
    "# Repeat for the second text and find the difference \n",
    "counts = Counter([token.lemma_ for token in vol_2_doc if not token.is_punct and not token.is_stop]).most_common(100)\n",
    "counts = dict(counts)\n",
    "\n",
    "# I don't speak Hungarian, but these are clearly not words, let's get rid of them\n",
    "del counts['\\r\\n']\n",
    "del counts['\\r\\n\\r\\n']\n",
    "del counts['-e']\n",
    "\n",
    "vol_2_doc = nlp(vol_2.text)\n",
    "for token in vol_2_doc:\n",
    "    if token.lemma_ in counts.keys():\n",
    "        token._.count = counts[token.lemma_]\n",
    "\n",
    "# Now we find the difference between the most common words in the two texts        \n",
    "set_vol1 = set([(token.lemma_, token._.count) for token in vol_1_doc if token._.count])\n",
    "set_vol2 = set([(token.lemma_, token._.count) for token in vol_2_doc if token._.count])\n",
    "difference = set_vol2.difference(set_vol1)\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bonus cell, let's look up the definition of a word in our list\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# PyWiktionary https://pypi.org/project/pywiktionary/\n",
    "from pywiktionary.wiktionary_parser_factory import WiktionaryParserFactory\n",
    "\n",
    "parser_factory = WiktionaryParserFactory(default_language='hu')\n",
    "parser_factory_result = parser_factory.get_page('tesz')\n",
    "display(HTML(parser_factory_result['response']['query']['pages']['13301']['extract']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models \n",
    "\n",
    "What if we wanted to create a list of the 100 most freqent verbs or nouns in the text?  With the base Hungarian model, token.pos_ returns nothing. Also take a look at our lemmas. Are those really lemmas?  That model simply does not know parts of speech or Hungarian lemmata.  We need one that does. \n",
    "\n",
    "Here is a listing of the officially supported spaCy models: https://spacy.io/models\n",
    "There are currently models for :\n",
    "- English\n",
    "- German\n",
    "- French\n",
    "- Spanish\n",
    "- Portuguese\n",
    "- Italian\n",
    "- Dutch\n",
    "- Greek\n",
    "- Multi-language\n",
    "\n",
    "The spaCy documentation lists the features and capabilities of each model.  Keep in mind that there can be several models for a language.  Larger models are often slower and require more memory.   If you're not using the more advanced features of a large model, then you would probably be better off using something small.  As a general rule, it's best to start small and then move up when a larger model is needed. \n",
    "\n",
    "\n",
    "To add a spaCy supported model, simply type: \n",
    "`python -m spacy download <name of model>` `en_core_web_sm` for example`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#rather than\n",
    "#nlp = English()\n",
    "#\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "doc = nlp('Be yourself; everyone else is already taken.')\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a growing community of spaCy users.  There are dozens of spaCy-based projects in the [Universe](https://spacy.io/universe) as well as user-created language models.  If you visit [awesome-hungarian-nlp](https://github.com/oroszgy/awesome-hungarian-nlp), for example, you'll find a link to a spaCy Hungarian model [here](https://github.com/oroszgy/spacy-hungarian-models).\n",
    "\n",
    "This is a full-featured model with\n",
    "- Word vectors\n",
    "- Brown clusters\n",
    "- Token frequencies \n",
    "- Sentencizer\n",
    "- PoS Tagger\n",
    "- Lemmatizer\n",
    "- Dependency parser\n",
    "\n",
    "> If you are working locally, you'll need to install the model:  \n",
    "> `pip install https://github.com/oroszgy/spacy-hungarian-models/releases/download/hu_core_ud_lg-0.2.0/hu_core_ud_lg-0.2.0-py3-none-any.whl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hu_core_ud_lg\n",
    "\n",
    "nlp = hu_core_ud_lg.load()\n",
    "doc = nlp('A jövo század regénye.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('token: ',[token.lemma_ for token in doc])\n",
    "print('pos  : ',[token.pos_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a list of the most common new verbs we'll encounter in book 2 by adding `token.pos_=='VERB'`. Note that the large model requires 6GB of memory and you may get a MemoryError. Note that it takes longer to process than our simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "from collections import Counter\n",
    "\n",
    "nlp = hu_core_ud_lg.load()\n",
    "nlp.max_length = 1070000 \n",
    "\n",
    "# Add an extension to our tokens called \"count\"\n",
    "Token.set_extension(\"count\", default=False, force=True)\n",
    "\n",
    "\n",
    "# Calculate the number of times that a lemma appears in the text\n",
    "top100 = Counter([token.lemma_ for token in vol_1_doc if not token.is_punct and not token.is_stop]).most_common(100)\n",
    "top100 = dict(top100)\n",
    "\n",
    "# Add the count to each token. \n",
    "vol_1_doc = nlp(vol_1.text)\n",
    "\n",
    "for token in vol_1_doc:\n",
    "    if token.lemma_ in top100.keys():\n",
    "        token._.count = counts[token.lemma_]\n",
    "\n",
    "# Repeat for the second text and find the difference \n",
    "counts = Counter([token.lemma_ for token in vol_2_doc if not token.is_punct and not token.is_stop]).most_common(100)\n",
    "counts = dict(counts)\n",
    "\n",
    "vol_2_doc = nlp(vol_2.text)\n",
    "for token in vol_2_doc:\n",
    "    if token.lemma_ in top100.keys():\n",
    "        token._.count = counts[token.lemma_]\n",
    "\n",
    "# Now we find the difference between the most common words in the two texts        \n",
    "set_vol1 = set([(token.lemma_, token._.count) for token in vol_1_doc if token._.count and token.pos_=='VERB'])\n",
    "set_vol2 = set([(token.lemma_, token._.count) for token in vol_2_doc if token._.count and token.pos_=='VERB'])\n",
    "difference = set_vol2.difference(set_vol1)\n",
    "difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://spacy.io/architecture-bcdfffe5c0b9f221a2f6607f96ca0e4a.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookLoc1 = 'xmascarol1.txt'\n",
    "bookLoc2 = 'xmascarol2.txt'\n",
    "\n",
    "with open(bookLoc1, 'r') as f:\n",
    "    text1 = f.read()\n",
    "    \n",
    "with open(bookLoc2, 'r') as f:\n",
    "    text2 = f.read()\n",
    "    \n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying lemma, parts of speech and stopwords\n",
    "\n",
    "You can iterate through a spaCy document object for its tokens. The token objects allow identification of things like the word lemma, its part of speech, its shape and whether it is a common word (a stopword). Let's take the first part of the text and interate through the tokens for:\n",
    "\n",
    "1. lemma (stemmed word)\n",
    "2. part of speech (e.g., noun)\n",
    "3. syntactic dependecy (what it relates to in the sentence)\n",
    "4. shape (capitalization)\n",
    "5. stopword (whether it is a common word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(token.text,\n",
    "         token.lemma_,\n",
    "         token.pos_,\n",
    "         token.dep_,\n",
    "         token.shape_,\n",
    "         token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a span object from part of the document. Just take a slice of the entire doc object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "span = doc1[3:7]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linguistic similarity\n",
    "\n",
    "What if we want to see if texts are similar to one another? Out of the box, spaCy can compare texts based on linguistic features. Let's take the two parts of *A Christmas Carol*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, these two short texts are pretty similar. Let's load part of a series of stories that Dickens published anonymously as Boz and is compiled as *Sketches by Boz, Illustrative of Every-Day Life and Every-Day People* and test for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bozLoc = 'sketchesboz.txt'\n",
    "    \n",
    "with open(bozLoc,'r',encoding='utf8') as f:\n",
    "    bozText = f.read()\n",
    "\n",
    "bozDoc = nlp(bozText)\n",
    "\n",
    "doc1.similarity(bozDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only slightly less similar than the two parts of *A Christmas Carol*. Let's try Dickens against parts of Joyce's *Ulysses*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulyssesLoc = 'ulysses.txt'\n",
    "\n",
    "with open(ulyssesLoc, 'r',encoding='utf8') as f:\n",
    "    ulyssesText = f.read()\n",
    "\n",
    "ulyssesDoc = nlp(ulyssesText)\n",
    "\n",
    "bozDoc.similarity(ulyssesDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less similar although also very similar. What if we try against something very different, [an article about the NBA finals from fivethirtyeight.com](https://fivethirtyeight.com/features/warriors-raptors-game-2-nba-finals/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basketballLoc = 'basketball.txt'\n",
    "\n",
    "with open(basketballLoc, 'r',encoding='utf8') as f:\n",
    "    basketballText = f.read()\n",
    "\n",
    "basketballDoc = nlp(basketballText)\n",
    "\n",
    "bozDoc.similarity(basketballDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less similar still, although still pretty similar. A different model or a trained model might give more differentiating results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying entities\n",
    "\n",
    "A useful feature in spaCy is its ability to distinguish entities. Entities are properly named people or places. Out of the box, spaCy does a pretty good job of finding names, institutions and locations. We can iterate through the entities in the text just like tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in basketballDoc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems with this index, of course. \"Six dimes\" is slang for six assists, for example. If you were interested in making a gazeteer (an place index), it would be easy to do by only printing those entities that have geographical qualities. These are \"geographical or political entities\" (GPE) or \"locations\" (LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in basketballDoc.ents:\n",
    "    if ent.label_ in ['GPE','LOC']:\n",
    "        print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Creating a List of Words for Language Study\n",
    "\n",
    "1. Eliminating unwanted tokens\n",
    "2. Creating an updating set\n",
    "\n",
    "Let's say you are teaching an ESL class for beginners. There are many texts you could assign, but how can you make a vocabulary list? And once you have a list for one text, how can you make sure the words do not repeat?\n",
    "\n",
    "Let's make a set of the words we need to learn and that we have already learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2learn = set()\n",
    "learnedWords = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminating unwanted tokens\n",
    "\n",
    "We need to iterate through the list of tokens from our document and make sure that easy words or non-words do not end up in the list. The first thing to do is to eliminate tokens that should not be in the list. These include punctuation (PUNCT), proper nouns (PROPN), spaces (SPACE), numbers (NUM) and symbols (SYM). Simply test to see if the tokens part of speech is one of these parts. We also should test to see if the token is a stopword so that overly simple words do not end up on the list.\n",
    "\n",
    "Here we iterate through the first part of *A Christmas Carol* and add the lemma and its part of speech to our learning list. Doing so will mean that strings that could represent different parts of speech (e.g., \"run\") will both end up in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    if token.pos_ not in ['PUNCT','PROPN','SPACE','NUM','SYM'] and not token.is_stop:\n",
    "        words2learn.add((token.lemma_,token.pos_))\n",
    "\n",
    "for word in words2learn:\n",
    "    print(word[0],word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it might make sense to write up this part of the code as a function so we can use it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usableWords(doc):\n",
    "    w2l = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['PUNCT','PROPN','SPACE','NUM','SYM'] and not token.is_stop:\n",
    "            w2l.add((token.lemma_,token.pos_))\n",
    "    return w2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume the student learned the words from the first part of *A Christmas Carol* and wants to learn new words from the second part. We can put the learned words in that set and figure out what words need to be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnedWords = learnedWords|words2learn\n",
    "\n",
    "newWords = usableWords(doc2)\n",
    "\n",
    "words2learn.clear()\n",
    "\n",
    "print(newWords-learnedWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a set that updates\n",
    "\n",
    "It might be useful to take a corpus of texts. There is a site called ESL Fast that has short, easy texts. Using the  urllib module and the HTML parsing module BeautifulSoup, we can download these texts and make lists from them. If you need to download BeautifulSoup, do so in your terminal:\n",
    "\n",
    "pip install beautifulsoup4\n",
    "\n",
    "Let's import these modules and download the first fourteen \"supereasy\" texts on the site. If you don't know how to use BeautifulSoup, you might look at [The Programming Historian's lesson](https://programminghistorian.org/en/lessons/intro-to-beautiful-soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "texts = []\n",
    "\n",
    "for story in range(1,15):\n",
    "    url = 'https://www.eslfast.com/supereasy/se/supereasy' + ('00'+str(story))[-3:] + '.htm'\n",
    "    page = urllib.request.urlopen(url).read()\n",
    "    text = BeautifulSoup(page, \"html.parser\").find(class_='MsoNormal').text\n",
    "    texts.append(text)\n",
    "    \n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that takes our learned words plus our new text and generates a list of words we need to learn and prints the words to learn, and one that adds the learned words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolearn(oldWords, newDoc):\n",
    "    words = usableWords(newDoc)\n",
    "    newWords = words-oldWords\n",
    "    for word in newWords:\n",
    "        print(word[0],'('+word[1]+')')\n",
    "    return newWords\n",
    "\n",
    "def learned(oldWords, newWords):\n",
    "    return oldWords|newWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can iterate through our texts, get the words to learn for each lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnedWords = set()\n",
    "\n",
    "for text in texts:\n",
    "    print('\\n***LESSON '+str(texts.index(text)+1)+'***')\n",
    "    textDoc = nlp(text)\n",
    "    words2learn = tolearn(learnedWords,textDoc)\n",
    "    learnedWords = learned(learnedWords,words2learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try from your favorite books at [Gutenberg.org](gutenberg.org) by the id number in the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadText(textid, limit=9000):\n",
    "    text = str(urllib.request.urlopen('http://www.gutenberg.org/files/'+str(textid)+'/'+str(textid)+'-0.txt').read())\n",
    "    return text[:limit] #making the sample smaller will make it quicker to process\n",
    "\n",
    "def printEntities(doc):\n",
    "    labels = ['GPE','LOC', 'DATE']\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in labels:\n",
    "            print(ent.text, ent.label_)\n",
    "\n",
    "textid1 = 59859 #Candide\n",
    "textid2 = 1342 #Pride and Prejudice by Jane Austen\n",
    "\n",
    "#doc1 = nlp(loadText(textid1))\n",
    "#doc2 = nlp(loadText(textid2))\n",
    "\n",
    "#doc1.similarity(doc2)\n",
    "\n",
    "#printEntities(doc1)\n",
    "\n",
    "whitehouse = nlp('We\\'re going to protest at the White House')\n",
    "\n",
    "for ent in whitehouse.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
