{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Processing with spaCy\n",
    "\n",
    "## Installation and Basic Use\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Installation\n",
    "2. Basic uses\n",
    "3. Project: Creating a vocabulary list from words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Installing spaCy\n",
    "2. Installing models\n",
    "3. Loading spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy\n",
    "\n",
    "You should have Python 3 installed. Dependencies for spaCy may take a while to install.\n",
    "\n",
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models\n",
    "\n",
    "SpaCy relies on installed language models. Before we do anything, we have to install at least one language model. We will install the medium English (91MB) and small German models (10MB).\n",
    "\n",
    "python -m spacy download en_core_web_md\n",
    "python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading spaCy\n",
    "\n",
    "spaCy is now ready to load in Python. Let's load the English model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Uses\n",
    "\n",
    "1. Loading a text\n",
    "2. Lemma, parts of speech and stopwords\n",
    "3. Assessing similarity\n",
    "4. Working with entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading a text\n",
    "\n",
    "Feeling festive, I chose two short sections of *A Christmas Carol* by Charles Dickens to analyze with spaCy. We will open them and process them with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookLoc1 = 'C:\\\\Users\\\\seth\\\\OneDrive\\\\xmascarol1.txt'\n",
    "bookLoc2 = 'C:\\\\Users\\\\seth\\\\OneDrive\\\\xmascarol2.txt'\n",
    "\n",
    "with open(bookLoc1, 'r') as f:\n",
    "    text1 = f.read()\n",
    "    \n",
    "with open(bookLoc2, 'r') as f:\n",
    "    text2 = f.read()\n",
    "    \n",
    "doc1 = nlp(text1)\n",
    "doc2 = nlp(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying Lemma, parts of speech and stopwords\n",
    "\n",
    "You can iterate through a spaCy document object for its tokens. The token objects allow identification of things like the word lemma, its part of speech, its shape and whether it is a common word (a stopword). Let's take the first part of the text and interate through the tokens for:\n",
    "\n",
    "1. lemma (stemmed word)\n",
    "2. part of speech (e.g., noun)\n",
    "3. syntactic dependecy (what it relates to in the sentence)\n",
    "4. shape (capitalization)\n",
    "5. stopword (whether it is a common word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "п»їMarley п»їmarley NOUN nsubj x»xXxxxx False\n",
      "was be VERB ROOT xxx True\n",
      "dead dead ADJ acomp xxxx False\n",
      ", , PUNCT punct , False\n",
      "to to PART aux xx True\n",
      "begin begin VERB advcl xxxx False\n",
      "with with ADP prep xxxx True\n",
      ". . PUNCT punct . False\n",
      "There there ADV expl Xxxxx True\n",
      "is be VERB ROOT xx True\n",
      "no no DET neg xx True\n",
      "doubt doubt NOUN attr xxxx False\n",
      "whatever whatever DET attr xxxx True\n",
      "about about ADP prep xxxx True\n",
      "that that DET pobj xxxx True\n",
      ". . PUNCT punct . False\n",
      "The the DET det Xxx True\n",
      "register register NOUN nsubjpass xxxx False\n",
      "of of ADP prep xx True\n",
      "his -PRON- DET poss xxx True\n",
      "burial burial NOUN pobj xxxx False\n",
      "was be VERB auxpass xxx True\n",
      "signed sign VERB ROOT xxxx False\n",
      "by by ADP agent xx True\n",
      "the the DET det xxx True\n",
      "clergyman clergyman NOUN pobj xxxx False\n",
      ", , PUNCT punct , False\n",
      "the the DET det xxx True\n",
      "clerk clerk NOUN appos xxxx False\n",
      ", , PUNCT punct , False\n",
      "the the DET det xxx True\n",
      "undertaker undertaker NOUN conj xxxx False\n",
      ", , PUNCT punct , False\n",
      "and and CCONJ cc xxx True\n",
      "the the DET det xxx True\n",
      "chief chief ADJ amod xxxx False\n",
      "mourner mourner NOUN conj xxxx False\n",
      ". . PUNCT punct . False\n",
      "Scrooge Scrooge PROPN nsubj Xxxxx False\n",
      "signed sign VERB ROOT xxxx False\n",
      "it -PRON- PRON dobj xx True\n",
      ". . PUNCT punct . False\n",
      "And and CCONJ cc Xxx True\n",
      "Scrooge Scrooge PROPN poss Xxxxx False\n",
      "'s 's PART case 'x True\n",
      "name name NOUN nsubj xxxx True\n",
      "was be VERB ROOT xxx True\n",
      "good good ADJ acomp xxxx False\n",
      "upon upon ADP prep xxxx True\n",
      "' ' PUNCT punct ' False\n",
      "Change change NOUN pobj Xxxxx False\n",
      "for for ADP prep xxx True\n",
      "anything anything NOUN pobj xxxx True\n",
      "he -PRON- PRON nsubj xx True\n",
      "chose choose VERB relcl xxxx False\n",
      "to to PART aux xx True\n",
      "put put VERB xcomp xxx True\n",
      "his -PRON- DET poss xxx True\n",
      "hand hand NOUN dobj xxxx False\n",
      "to to ADP prep xx True\n",
      ". . PUNCT punct . False\n",
      "Old Old PROPN compound Xxx False\n",
      "Marley Marley PROPN nsubj Xxxxx False\n",
      "was be VERB ROOT xxx True\n",
      "as as ADV advmod xx True\n",
      "dead dead ADJ acomp xxxx False\n",
      "as as ADP prep xx True\n",
      "a a DET det x True\n",
      "door door NOUN compound xxxx False\n",
      "- - PUNCT punct - False\n",
      "nail nail NOUN pobj xxxx False\n",
      ". . PUNCT punct . False\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " SPACE  \n",
      "\n",
      " False\n",
      "Mind mind INTJ ROOT Xxxx False\n",
      "! ! PUNCT punct ! False\n",
      "I -PRON- PRON nsubj X True\n",
      "do do VERB aux xx True\n",
      "n't not ADV neg x'x True\n",
      "mean mean VERB ccomp xxxx False\n",
      "to to PART aux xx True\n",
      "say say VERB xcomp xxx True\n",
      "that that ADP dobj xxxx True\n",
      "I -PRON- PRON nsubj X True\n",
      "know know VERB parataxis xxxx False\n",
      ", , PUNCT punct , False\n",
      "of of ADP prep xx True\n",
      "my -PRON- DET poss xx True\n",
      "own own ADJ amod xxx True\n",
      "knowledge knowledge NOUN pobj xxxx False\n",
      ", , PUNCT punct , False\n",
      "what what PRON attr xxxx True\n",
      "there there ADV expl xxxx True\n",
      "is be VERB ROOT xx True\n",
      "particularly particularly ADV advmod xxxx False\n",
      "dead dead ADJ acomp xxxx False\n",
      "about about ADP prep xxxx True\n",
      "a a DET det x True\n",
      "door door NOUN compound xxxx False\n",
      "- - PUNCT punct - False\n",
      "nail nail NOUN pobj xxxx False\n",
      ". . PUNCT punct . False\n",
      "I -PRON- PRON nsubjpass X True\n",
      "might may VERB aux xxxx True\n",
      "have have VERB aux xxxx True\n",
      "been be VERB auxpass xxxx True\n",
      "inclined incline VERB ROOT xxxx False\n",
      ", , PUNCT punct , False\n",
      "myself -PRON- PRON npadvmod xxxx True\n",
      ", , PUNCT punct , False\n",
      "to to PART aux xx True\n",
      "regard regard VERB advcl xxxx False\n",
      "a a DET det x True\n",
      "coffin coffin NOUN compound xxxx False\n",
      "- - PUNCT punct - False\n",
      "nail nail NOUN dobj xxxx False\n",
      "as as ADP prep xx True\n",
      "the the DET det xxx True\n",
      "deadest dead ADJ amod xxxx False\n",
      "piece piece NOUN pobj xxxx False\n",
      "of of ADP prep xx True\n",
      "ironmongery ironmongery NOUN pobj xxxx False\n",
      "in in ADP prep xx True\n",
      "the the DET det xxx True\n",
      "trade trade NOUN pobj xxxx False\n",
      ". . PUNCT punct . False\n",
      "But but CCONJ cc Xxx True\n",
      "the the DET det xxx True\n",
      "wisdom wisdom NOUN nsubj xxxx False\n",
      "of of ADP prep xx True\n",
      "our -PRON- DET poss xxx True\n",
      "ancestors ancestor NOUN pobj xxxx False\n",
      "is be VERB ROOT xx True\n",
      "in in ADP prep xx True\n",
      "the the DET det xxx True\n",
      "simile simile NOUN pobj xxxx False\n",
      "; ; PUNCT punct ; False\n",
      "and and CCONJ cc xxx True\n",
      "my -PRON- DET poss xx True\n",
      "unhallowed unhallowed ADJ amod xxxx False\n",
      "hands hand NOUN nsubj xxxx False\n",
      "shall shall VERB aux xxxx False\n",
      "not not ADV neg xxx True\n",
      "disturb disturb VERB conj xxxx False\n",
      "it -PRON- PRON dobj xx True\n",
      ", , PUNCT punct , False\n",
      "or or CCONJ cc xx True\n",
      "the the DET det xxx True\n",
      "Country Country PROPN nsubj Xxxxx False\n",
      "'s 's PART case 'x True\n",
      "done do VERB conj xxxx True\n",
      "for for ADP prep xxx True\n",
      ". . PUNCT punct . False\n",
      "You -PRON- PRON nsubj Xxx True\n",
      "will will VERB aux xxxx True\n",
      ", , PUNCT punct , False\n",
      "therefore therefore ADV advmod xxxx True\n",
      ", , PUNCT punct , False\n",
      "permit permit VERB ROOT xxxx False\n",
      "me -PRON- PRON dobj xx True\n",
      "to to PART aux xx True\n",
      "repeat repeat VERB xcomp xxxx False\n",
      ", , PUNCT punct , False\n",
      "emphatically emphatically ADV advmod xxxx False\n",
      ", , PUNCT punct , False\n",
      "that that ADP mark xxxx True\n",
      "Marley Marley PROPN nsubj Xxxxx False\n",
      "was be VERB ccomp xxx True\n",
      "as as ADV advmod xx True\n",
      "dead dead ADJ acomp xxxx False\n",
      "as as ADP prep xx True\n",
      "a a DET det x True\n",
      "door door NOUN compound xxxx False\n",
      "- - PUNCT punct - False\n",
      "nail nail NOUN pobj xxxx False\n",
      ". . PUNCT punct . False\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text,\n",
    "         token.lemma_,\n",
    "         token.pos_,\n",
    "         token.dep_,\n",
    "         token.shape_,\n",
    "         token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linguistic Similarity\n",
    "\n",
    "What if we want to see if texts are similar to one another? Out of the box, spaCy can compare texts based on linguistic features. Let's take the two parts of *A Christmas Carol*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987069344903874"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, these two short texts are pretty similar. Let's load part of a series of stories that Dickens published anonymously as Boz and is compiled as *Sketches by Boz, Illustrative of Every-Day Life and Every-Day People* and test for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9867880349002466"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bozLoc = 'C:\\\\Users\\\\seth\\\\OneDrive\\\\sketchesboz.txt'\n",
    "    \n",
    "with open(bozLoc,'r',encoding='utf8') as f:\n",
    "    bozText = f.read()\n",
    "\n",
    "bozDoc = nlp(bozText)\n",
    "\n",
    "doc1.similarity(bozDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only slightly less similar than the two parts of *A Christmas Carol*. Let's try Dickens against parts of Joyce's *Ulysses*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9843544359594985"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulyssesLoc = 'C:\\\\Users\\\\seth\\\\OneDrive\\\\ulysses.txt'\n",
    "\n",
    "with open(ulyssesLoc, 'r',encoding='utf8') as f:\n",
    "    ulyssesText = f.read()\n",
    "\n",
    "ulyssesDoc = nlp(ulyssesText)\n",
    "\n",
    "bozDoc.similarity(ulyssesDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less similar although also very similar. What if we try against something very different, [an article about the NBA finals from fivethirtyeight.com]https://fivethirtyeight.com/features/warriors-raptors-game-2-nba-finals/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9732606573565973"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basketballLoc = 'C:\\\\Users\\\\seth\\\\OneDrive\\\\basketball.txt'\n",
    "\n",
    "with open(basketballLoc, 'r',encoding='utf8') as f:\n",
    "    basketballText = f.read()\n",
    "\n",
    "basketballDoc = nlp(basketballText)\n",
    "\n",
    "bozDoc.similarity(basketballDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less similar still, although still pretty similar. A different model might give more differentiating results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying Entities\n",
    "\n",
    "A useful feature in spaCy is its ability to distinguish entities. Entities are properly named people or places. Out of the box, spaCy does a pretty good job of finding names, institutions and locations. We can iterate through the entities in the text just like tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two CARDINAL\n",
      "Kevin Durant PERSON\n",
      "Game 2 EVENT\n",
      "the NBA Finals EVENT\n",
      "Sunday DATE\n",
      "Durant PERSON\n",
      "Stephen Curry’s PERSON\n",
      "Kevon Looney PERSON\n",
      "two CARDINAL\n",
      "the first half DATE\n",
      "MVP Andre Iguodala PERSON\n",
      "Marc Gasol PERSON\n",
      "the second quarter DATE\n",
      "almost a minute TIME\n",
      "Klay Thompson — PERSON\n",
      "first ORDINAL\n",
      "half CARDINAL\n",
      "eight minutes TIME\n",
      "one CARDINAL\n",
      "Raptors ORG\n",
      "the final five minutes TIME\n",
      "2 CARDINAL\n",
      "one CARDINAL\n",
      "Oakland GPE\n",
      "Games 3 and 4 DATE\n",
      "59 CARDINAL\n",
      "half CARDINAL\n",
      "Golden State GPE\n",
      "third ORDINAL\n",
      "Mario Star PERSON\n",
      "Warriors ORG\n",
      "18 CARDINAL\n",
      "the quarter DATE\n",
      "Toronto GPE\n",
      "Thompson PERSON\n",
      "Durant PERSON\n",
      "Warriors ORG\n",
      "22 CARDINAL\n",
      "Golden State GPE\n",
      "the entire second half DATE\n",
      "second ORDINAL\n",
      "NBA Finals ORG\n",
      "half CARDINAL\n",
      "the Elias Sports EVENT\n",
      "Raptors ORG\n",
      "Golden State ORG\n",
      "Raptors ORG\n",
      "as much as 13 CARDINAL\n",
      "the closing minutes TIME\n",
      "Thompson PERSON\n",
      "Raptors ORG\n",
      "Raptors ORG\n",
      "Nick Nurse PERSON\n",
      "Curry DATE\n",
      "five CARDINAL\n",
      "Toronto GPE\n",
      "12 CARDINAL\n",
      "2 CARDINAL\n",
      "Shaun Livingston PERSON\n",
      "10 seconds TIME\n",
      "one CARDINAL\n",
      "Kawhi Leonard PERSON\n",
      "Curry NORP\n",
      "Livingston PERSON\n",
      "Raptors ORG\n",
      "Pascal Siakam PERSON\n",
      "Siakam PERSON\n",
      "Iguodala PERSON\n",
      "Iggy PERSON\n",
      "Iguodala PERSON\n",
      "Sunday DATE\n",
      "Game 6 EVENT\n",
      "Houston GPE\n",
      "Rockets-Warriors FAC\n",
      "Toronto GPE\n",
      "Houston GPE\n",
      "Thompson PERSON\n",
      "Raptors ORG\n",
      "first ORDINAL\n",
      "half CARDINAL\n",
      "first ORDINAL\n",
      "six CARDINAL\n",
      "first ORDINAL\n",
      "four minutes TIME\n",
      "the half DATE\n",
      "Klay PERSON\n",
      "the first half DATE\n",
      "Houston GPE\n",
      "a scoreless first half DATE\n",
      "33 second-half DATE\n",
      "Thompson PERSON\n",
      "Toronto GPE\n",
      "Raptors ORG\n",
      "Durant PERSON\n",
      "Houston GPE\n",
      "Rockets PRODUCT\n",
      "Toronto,3 GPE\n",
      "Warriors NORP\n",
      "Draymond Green PRODUCT\n",
      "Siakam NORP\n",
      "Kyle Lowry PERSON\n",
      "Siakam PERSON\n",
      "Game 1 EVENT\n",
      "Iguodala PERSON\n",
      "Thompson PERSON\n",
      "Kawhi PERSON\n",
      "Toronto GPE\n",
      "Leonard ORG\n",
      "34 CARDINAL\n",
      "just three CARDINAL\n",
      "Sunday DATE\n",
      "seven dimes MONEY\n",
      "’d PRODUCT\n",
      "three CARDINAL\n",
      "DeMarcus Cousins ORG\n",
      "second ORDINAL\n",
      "11 CARDINAL\n",
      "10 CARDINAL\n",
      "six CARDINAL\n",
      "just 28 minutes TIME\n",
      "Thompson PERSON\n",
      "Game 35 EVENT\n",
      "Durant PERSON\n",
      "two CARDINAL\n",
      "Thompson PERSON\n",
      "Durant PERSON\n",
      "Toronto GPE\n",
      "Curry NORP\n",
      "Golden State GPE\n",
      "Game 3 EVENT\n",
      "Wednesday DATE\n",
      "Warriors ORG\n",
      "Game 2 EVENT\n"
     ]
    }
   ],
   "source": [
    "for ent in basketballDoc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some problems with this index, of course. \"Six dimes\" is slang for six assists, for example. If you were interested in making a gazeteer (an place index), it would be easy to do by only printing those entities that have geographical qualities. These are \"geographical or political entities\" (GPE) or \"locations\" (LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oakland GPE\n",
      "Golden State GPE\n",
      "Toronto GPE\n",
      "Golden State GPE\n",
      "Toronto GPE\n",
      "Houston GPE\n",
      "Toronto GPE\n",
      "Houston GPE\n",
      "Houston GPE\n",
      "Toronto GPE\n",
      "Houston GPE\n",
      "Toronto,3 GPE\n",
      "Toronto GPE\n",
      "Toronto GPE\n",
      "Golden State GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in basketballDoc.ents:\n",
    "    if ent.label_ in ['GPE','LOC']:\n",
    "        print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Creating a List of Words for Language Study\n",
    "\n",
    "1. Eliminating unwanted tokens\n",
    "2. Creating an updating set\n",
    "\n",
    "Let's say you are teaching an ESL class for beginners. There are many texts you could assign, but how can you make a vocabulary list? And once you have a list for one text, how can you make sure the words do not repeat?\n",
    "\n",
    "Let's make a set of the words we need to learn and that we have already learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2learn = set()\n",
    "learnedWords = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminating unwanted tokens\n",
    "\n",
    "We need to iterate through the list of tokens from our document and make sure that easy words or non-words do not end up in the list. The first thing to do is to eliminate tokens that should not be in the list. These include punctuation (PUNCT), proper nouns (PROPN), spaces (SPACE), numbers (NUM) and symbols (SYM). Simply test to see if the tokens part of speech is one of these parts. We also should test to see if the token is a stopword so that overly simple words do not end up on the list.\n",
    "\n",
    "Here we iterate through the first part of *A Christmas Carol* and add the lemma and its part of speech to our learning list. Doing so will mean that strings that could represent different parts of speech (e.g., \"run\") will both end up in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simile NOUN\n",
      "ironmongery NOUN\n",
      "sign VERB\n",
      "shall VERB\n",
      "chief ADJ\n",
      "begin VERB\n",
      "dead ADJ\n",
      "repeat VERB\n",
      "change NOUN\n",
      "good ADJ\n",
      "mean VERB\n",
      "wisdom NOUN\n",
      "disturb VERB\n",
      "п»їmarley NOUN\n",
      "incline VERB\n",
      "register NOUN\n",
      "nail NOUN\n",
      "mourner NOUN\n",
      "piece NOUN\n",
      "door NOUN\n",
      "clerk NOUN\n",
      "undertaker NOUN\n",
      "regard VERB\n",
      "coffin NOUN\n",
      "know VERB\n",
      "trade NOUN\n",
      "unhallowed ADJ\n",
      "emphatically ADV\n",
      "burial NOUN\n",
      "clergyman NOUN\n",
      "permit VERB\n",
      "mind INTJ\n",
      "knowledge NOUN\n",
      "particularly ADV\n",
      "doubt NOUN\n",
      "ancestor NOUN\n",
      "choose VERB\n",
      "hand NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    if token.pos_ not in ['PUNCT','PROPN','SPACE','NUM','SYM'] and not token.is_stop:\n",
    "        words2learn.add((token.lemma_,token.pos_))\n",
    "\n",
    "for word in words2learn:\n",
    "    print(word[0],word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it might make sense to write up this part of the code as a function so we can use it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usableWords(doc):\n",
    "    w2l = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in ['PUNCT','PROPN','SPACE','NUM','SYM'] and not token.is_stop:\n",
    "            w2l.add((token.lemma_,token.pos_))\n",
    "    return w2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume the student learned the words from the first part of *A Christmas Carol* and wants to learn new words from the second part. We can put the learned words in that set and figure out what words need to be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('burial', 'NOUN'), ('mean', 'VERB'), ('know', 'VERB'), ('coffin', 'NOUN'), ('knowledge', 'NOUN'), ('door', 'NOUN'), ('sign', 'VERB'), ('clergyman', 'NOUN'), ('nail', 'NOUN'), ('doubt', 'NOUN'), ('good', 'ADJ'), ('clerk', 'NOUN'), ('begin', 'VERB'), ('ancestor', 'NOUN'), ('repeat', 'VERB'), ('chief', 'ADJ'), ('hand', 'NOUN'), ('mourner', 'NOUN'), ('п»їmarley', 'NOUN'), ('undertaker', 'NOUN'), ('simile', 'NOUN'), ('piece', 'NOUN'), ('permit', 'VERB'), ('change', 'NOUN'), ('emphatically', 'ADV'), ('shall', 'VERB'), ('regard', 'VERB'), ('choose', 'VERB'), ('register', 'NOUN'), ('particularly', 'ADV'), ('dead', 'ADJ'), ('trade', 'NOUN'), ('ironmongery', 'NOUN'), ('wisdom', 'NOUN'), ('mind', 'INTJ'), ('incline', 'VERB'), ('unhallowed', 'ADJ'), ('disturb', 'VERB')}\n",
      "***\n",
      "{('burial', 'NOUN'), ('mean', 'VERB'), ('know', 'VERB'), ('coffin', 'NOUN'), ('knowledge', 'NOUN'), ('door', 'NOUN'), ('sign', 'VERB'), ('clergyman', 'NOUN'), ('nail', 'NOUN'), ('doubt', 'NOUN'), ('good', 'ADJ'), ('clerk', 'NOUN'), ('begin', 'VERB'), ('ancestor', 'NOUN'), ('repeat', 'VERB'), ('chief', 'ADJ'), ('hand', 'NOUN'), ('mourner', 'NOUN'), ('п»їmarley', 'NOUN'), ('undertaker', 'NOUN'), ('simile', 'NOUN'), ('piece', 'NOUN'), ('permit', 'VERB'), ('change', 'NOUN'), ('emphatically', 'ADV'), ('shall', 'VERB'), ('regard', 'VERB'), ('choose', 'VERB'), ('register', 'NOUN'), ('particularly', 'ADV'), ('dead', 'ADJ'), ('trade', 'NOUN'), ('ironmongery', 'NOUN'), ('wisdom', 'NOUN'), ('mind', 'INTJ'), ('incline', 'VERB'), ('unhallowed', 'ADJ'), ('disturb', 'VERB')}\n",
      "set()\n",
      "{('wind', 'NOUN'), ('rashly', 'ADV'), ('funeral', 'NOUN'), ('administrator', 'NOUN'), ('mind', 'NOUN'), ('turn', 'VERB'), ('event', 'NOUN'), ('partner', 'NOUN'), ('go', 'VERB'), ('stroll', 'NOUN'), ('middle', 'NOUN'), ('excellent', 'ADJ'), ('aged', 'ADJ'), ('executor', 'NOUN'), ('cut', 'VERB'), ('sad', 'ADJ'), ('understand', 'VERB'), ('die', 'VERB'), ('perfectly', 'ADV'), ('son', 'NOUN'), ('sole', 'ADJ'), ('residuary', 'ADJ'), ('year', 'NOUN'), ('solemnise', 'VERB'), ('play', 'NOUN'), ('start', 'VERB'), ('legatee', 'NOUN'), ('yard', 'NOUN'), ('bargain', 'NOUN'), ('easterly', 'ADJ'), ('story', 'NOUN'), ('convinced', 'ADJ'), ('breezy', 'ADJ'), ('day', 'NOUN'), ('night', 'NOUN'), ('rampart', 'NOUN'), ('dreadfully', 'ADV'), ('gentleman', 'NOUN'), ('dark', 'NOUN'), ('distinctly', 'ADV'), ('friend', 'NOUN'), ('business', 'NOUN'), ('undoubted', 'ADJ'), ('relate', 'VERB'), ('remarkable', 'ADJ'), ('point', 'NOUN'), ('literally', 'ADV'), ('man', 'NOUN'), ('mention', 'NOUN'), ('wonderful', 'ADJ'), ('instance', 'NOUN'), ('astonish', 'VERB'), ('п»їscrooge', 'INTJ'), ('bring', 'VERB'), ('spot', 'NOUN'), ('come', 'VERB'), ('take', 'VERB'), ('assign', 'NOUN'), ('course', 'ADV'), ('weak', 'ADJ')}\n"
     ]
    }
   ],
   "source": [
    "learnedWords = learnedWords|words2learn\n",
    "\n",
    "newWords = usableWords(doc2)\n",
    "\n",
    "words2learn.clear()\n",
    "\n",
    "print(newWords-learnedWords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a set that updates\n",
    "\n",
    "It might be useful to take a corpus of texts. There is a site called ESL Fast that has short, easy texts. Using the  urllib module and the HTML parsing module BeautifulSoup, we can download these texts and make lists from them. If you need to download BeautifulSoup, do so in your terminal:\n",
    "\n",
    "pip install beautifulsoup4\n",
    "\n",
    "Let's import these modules and download the first fourteen \"supereasy\" texts on the site. If you don't know how to use BeautifulSoup, you might look at [The Programming Historian's lesson]https://programminghistorian.org/en/lessons/intro-to-beautiful-soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Billy always listens to his mother. He always does what she says. If his mother says, \"Brush your teeth,\" Billy brushes his teeth. If his mother says, \"Go to bed,\" Billy goes to bed. Billy is a very good boy. A good boy listens to his mother. His mother doesn't have to ask him again. She asks him to do something one time, and she doesn't ask again. Billy is a good boy. He does what his mother asks the first time. She doesn't have to ask again. She tells Billy, \"You are my best child.\" Of course Billy is her best child. Billy is her only child.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "texts = []\n",
    "\n",
    "for story in range(1,15):\n",
    "    url = 'https://www.eslfast.com/supereasy/se/supereasy' + ('00'+str(story))[-3:] + '.htm'\n",
    "    page = urllib.request.urlopen(url).read()\n",
    "    text = BeautifulSoup(page, \"html.parser\").find(class_='MsoNormal').text\n",
    "    texts.append(text)\n",
    "    \n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that takes our learned words plus our new text and generates a list of words we need to learn and prints the words to learn, and one that adds the learned words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolearn(oldWords, newDoc):\n",
    "    words = usableWords(newDoc)\n",
    "    newWords = words-oldWords\n",
    "    for word in newWords:\n",
    "        print(word[0],'('+word[1]+')')\n",
    "    return newWords\n",
    "\n",
    "def learned(oldWords, newWords):\n",
    "    return oldWords|newWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can iterate through our texts, get the words to learn for each lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***LESSON 1***\n",
      "say (VERB)\n",
      "good (ADJ)\n",
      "time (NOUN)\n",
      "listen (VERB)\n",
      "bed (NOUN)\n",
      "course (ADV)\n",
      "tooth (NOUN)\n",
      "ask (VERB)\n",
      "go (VERB)\n",
      "child (NOUN)\n",
      "tell (VERB)\n",
      "mother (NOUN)\n",
      "brush (VERB)\n",
      "boy (NOUN)\n",
      "\n",
      "***LESSON 2***\n",
      "want (VERB)\n",
      "live (VERB)\n",
      "hug (NOUN)\n",
      "honey (NOUN)\n",
      "give (VERB)\n",
      "father (NOUN)\n",
      "smile (VERB)\n",
      "long (ADJ)\n",
      "year (NOUN)\n",
      "daddy (NOUN)\n",
      "old (ADJ)\n",
      "grow (VERB)\n",
      "big (ADJ)\n",
      "okay (ADJ)\n",
      "yes (INTJ)\n",
      "\n",
      "***LESSON 3***\n",
      "bark (VERB)\n",
      "wake (VERB)\n",
      "dog (NOUN)\n",
      "loud (ADJ)\n",
      "look (VERB)\n",
      "hear (VERB)\n",
      "outside (ADP)\n",
      "barking (NOUN)\n",
      "brown (ADJ)\n",
      "see (VERB)\n",
      "loud (ADV)\n",
      "get (VERB)\n",
      "window (NOUN)\n",
      "walk (VERB)\n",
      "barking (ADJ)\n",
      "stop (VERB)\n",
      "open (VERB)\n",
      "\n",
      "***LESSON 4***\n",
      "envelope (NOUN)\n",
      "think (VERB)\n",
      "pencil (NOUN)\n",
      "know (VERB)\n",
      "help (VERB)\n",
      "yellow (ADJ)\n",
      "dear (ADJ)\n",
      "toaster (NOUN)\n",
      "sister (NOUN)\n",
      "note (NOUN)\n",
      "thank (VERB)\n",
      "kitchen (NOUN)\n",
      "write (VERB)\n",
      "find (VERB)\n",
      "lunch (NOUN)\n",
      "counter (NOUN)\n",
      "lose (VERB)\n",
      "teacher (NOUN)\n",
      "\n",
      "***LESSON 5***\n",
      "door (NOUN)\n",
      "happy (ADJ)\n",
      "grab (VERB)\n",
      "sit (VERB)\n",
      "pink (ADJ)\n",
      "excited (ADJ)\n",
      "sneaker (NOUN)\n",
      "horse (NOUN)\n",
      "bedroom (NOUN)\n",
      "minute (NOUN)\n",
      "seat (NOUN)\n",
      "ride (VERB)\n",
      "sock (NOUN)\n",
      "mom (NOUN)\n",
      "love (VERB)\n",
      "hat (NOUN)\n",
      "wait (VERB)\n",
      "car (NOUN)\n",
      "ready (ADJ)\n",
      "okay (INTJ)\n",
      "\n",
      "***LESSON 6***\n",
      "table (NOUN)\n",
      "corn (NOUN)\n",
      "wonder (VERB)\n",
      "pea (NOUN)\n",
      "pass (VERB)\n",
      "dinner (NOUN)\n",
      "white (ADJ)\n",
      "plate (NOUN)\n",
      "delicious (ADJ)\n",
      "fork (NOUN)\n",
      "spoon (NOUN)\n",
      "rice (NOUN)\n",
      "dad (NOUN)\n",
      "chair (NOUN)\n",
      "green (ADJ)\n",
      "silver (NOUN)\n",
      "\n",
      "***LESSON 7***\n",
      "easy (ADJ)\n",
      "song (NOUN)\n",
      "like (VERB)\n",
      "remember (VERB)\n",
      "favorite (ADJ)\n",
      "day (NOUN)\n",
      "boat (NOUN)\n",
      "row (NOUN)\n",
      "word (NOUN)\n",
      "row (VERB)\n",
      "friend (NOUN)\n",
      "school (NOUN)\n",
      "sing (VERB)\n",
      "\n",
      "***LESSON 8***\n",
      "ground (NOUN)\n",
      "laugh (VERB)\n",
      "run (VERB)\n",
      "jump (VERB)\n",
      "chase (VERB)\n",
      "fence (NOUN)\n",
      "turn (VERB)\n",
      "lie (VERB)\n",
      "\n",
      "***LESSON 9***\n",
      "red (ADJ)\n",
      "strawberry (NOUN)\n",
      "brother (NOUN)\n",
      "blueberry (NOUN)\n",
      "black (ADJ)\n",
      "eat (VERB)\n",
      "redberry (NOUN)\n",
      "confused (ADJ)\n",
      "little (ADJ)\n",
      "cherry (NOUN)\n",
      "blue (ADJ)\n",
      "berry (NOUN)\n",
      "blackberry (NOUN)\n",
      "\n",
      "***LESSON 10***\n",
      "lick (VERB)\n",
      "milk (NOUN)\n",
      "carton (NOUN)\n",
      "glass (NOUN)\n",
      "cap (NOUN)\n",
      "syrup (NOUN)\n",
      "take (VERB)\n",
      "refrigerator (NOUN)\n",
      "stir (VERB)\n",
      "pour (VERB)\n",
      "cabinet (NOUN)\n",
      "chocolate (NOUN)\n",
      "\n",
      "***LESSON 11***\n",
      "late (ADJ)\n",
      "coffee (NOUN)\n",
      "drop (NOUN)\n",
      "house (NOUN)\n",
      "job (NOUN)\n",
      "sink (NOUN)\n",
      "gray (ADJ)\n",
      "finish (VERB)\n",
      "outside (ADV)\n",
      "coat (NOUN)\n",
      "drink (VERB)\n",
      "key (NOUN)\n",
      "cup (NOUN)\n",
      "work (NOUN)\n",
      "faucet (NOUN)\n",
      "water (NOUN)\n",
      "pick (VERB)\n",
      "lock (VERB)\n",
      "\n",
      "***LESSON 12***\n",
      "late (ADV)\n",
      "set (VERB)\n",
      "hungry (ADJ)\n",
      "fish (VERB)\n",
      "fish (NOUN)\n",
      "food (NOUN)\n",
      "Fish (NOUN)\n",
      "o'clock (NOUN)\n",
      "catch (VERB)\n",
      "alarm (NOUN)\n",
      "fresh (ADJ)\n",
      "start (VERB)\n",
      "early (ADJ)\n",
      "week (NOUN)\n",
      "early (ADV)\n",
      "\n",
      "***LESSON 13***\n",
      "month (NOUN)\n",
      "contract (NOUN)\n",
      "lot (NOUN)\n",
      "cost (VERB)\n",
      "use (VERB)\n",
      "save (VERB)\n",
      "require (VERB)\n",
      "new (ADJ)\n",
      "dealer (NOUN)\n",
      "money (NOUN)\n",
      "buy (VERB)\n",
      "make (VERB)\n",
      "sign (VERB)\n",
      "pay (VERB)\n",
      "\n",
      "***LESSON 14***\n",
      "thin (ADJ)\n",
      "eater (NOUN)\n",
      "bowl (NOUN)\n",
      "cereal (NOUN)\n",
      "heavy (ADJ)\n",
      "vegetable (NOUN)\n",
      "breakfast (NOUN)\n",
      "sandwich (NOUN)\n",
      "fat (ADJ)\n",
      "light (ADJ)\n"
     ]
    }
   ],
   "source": [
    "learnedWords = set()\n",
    "\n",
    "for text in texts:\n",
    "    print('\\n***LESSON '+str(texts.index(text)+1)+'***')\n",
    "    textDoc = nlp(text)\n",
    "    words2learn = tolearn(learnedWords,textDoc)\n",
    "    learnedWords = learned(learnedWords,words2learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
