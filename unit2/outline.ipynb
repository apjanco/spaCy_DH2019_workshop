{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI to spaCy\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Basic Pipeline from TEI to Spacy with annotations on document-level\n",
    "\n",
    "    1. load 50 TEI encoded XMLs from Deutsches Textarchiv\n",
    "    2. Extract plain text and author GND\n",
    "    3. Annotate each document with it's author id\n",
    "\n",
    "2. A little more advanced Pipeline from TEI to Spacy annotations on character-level\n",
    "    1. load 50 TEI encoded XMLs from Berliner Intellektuelle\n",
    "    2. Extract initial and last version\n",
    "    3. annotate sub-tokens that have been added or deleted\n",
    "    \n",
    "3. Loading word vectors from fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tei_dataloader import dta_loader, bi_loader, wilhelmus_loader\n",
    "from tei_dataloader import extract_text_versions_from_etree as extract_versions\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "\n",
    "spec = {\"tei\":\"http://www.tei-c.org/ns/1.0\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Pipeline from TEI to Spacy with annotations on document-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Deutsches Textarchiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = []\n",
    "labels = []\n",
    "\n",
    "for dta_doc in dta_loader():\n",
    "    \n",
    "    # find out author GND from XML\n",
    "    authors = dta_doc.xpath(\".//tei:author/tei:persName\", namespaces=spec)\n",
    "    \n",
    "    author_gnd = None\n",
    "    for author in authors:\n",
    "        if \"ref\" in author.attrib:\n",
    "            author_gnd = author.attrib[\"ref\"]\n",
    "            break\n",
    "    author_gnd = \"anonymous\" if author_gnd == None else author_gnd\n",
    "    \n",
    "    # retrieve plain text\n",
    "    new_txt = []\n",
    "    for body in dta_doc.findall(\".//tei:body\", namespaces=spec):\n",
    "        new_txt.append(''.join(body.itertext()).strip())\n",
    "    txts.append(''.join(new_txt))\n",
    "    labels.append(author_gnd)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[0])\n",
    "print(txts[0][:100])\n",
    "len(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "authors, _ = zip(*Counter(labels).most_common(2))\n",
    "authors\n",
    "\n",
    "two_class_txts, two_class_labels = [], []\n",
    "for txt, label in zip(txts, labels):\n",
    "    if label in authors:\n",
    "        two_class_txts.append(txt)\n",
    "        two_class_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.tokens.Doc.set_extension('author', default=None, force=True)\n",
    "\n",
    "nlp_de = spacy.load(\"de\", disable=['parser', 'tagger', 'ner'])\n",
    "nlp_nl = spacy.load(\"nl\", disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for author,doc in zip(two_class_labels, nlp_de.pipe(two_class_txts)):\n",
    "    doc._.set(\"author\", author)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]._.author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, textcat, docs, cats):\n",
    "\n",
    "    tp = 0.0   # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 0.0   # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_textcat(docs, nlp):\n",
    "    labels = [doc._.author for doc in docs]\n",
    "    authors = list(set(labels))\n",
    "    texts, labels = docs, [{author:True if label == author else False for author in authors} for label in labels]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(texts, labels)\n",
    "\n",
    "    train_data = list(zip(X_train, [{'cats': cats} for cats in y_train]))\n",
    "\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe('textcat')\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    for author in authors:\n",
    "        textcat.add_label(author)\n",
    "\n",
    "    return textcat, train_data, (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "def train(train_data, test_data, nlp, textcat):\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "        for i in range(5):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, *test_data)\n",
    "            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "                  .format(losses['textcat'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "#textcat, train_data, test_data = create_textcat(docs, nlp_de)\n",
    "#train(train_data, test_data, nlp_de, textcat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Wilhelminus Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilhelminus5135 = 'Wilhelmus van Nassouwe\\n        Ben ick van Duytschen bloet,\\n        Den Vaderlant ghetrouwe\\n        Blijf ick tot inden doot:\\n        Een Prince van Oraengien\\n        Ben ick vrij onverveert,\\n        Den Coninck van Hispaengien\\n        Heb ick altijt gheeert.\\n      \\n      \\n        In Godes vrees te leven\\n        Heb ick altijt betracht,\\n        Daerom ben ick verdreven\\n        Om Landt om Luyd ghebracht:\\n        Maer Godt sal my regeren\\n        Als een goet Instrument,\\n        Dat ick sal wederkeeren\\n        In mijnen Regiment.\\n      \\n      \\n        Lijdt u mijn Ondersaten\\n        Die oprecht zijn van aert,\\n        Godt sal u niet verlaten,\\n        Al zijt ghy nu beswaert:\\n        Die vroom begheert te leven\\n        Bidt Godt nacht ende dach,\\n        Dat hy my cracht wil gheven\\n        Dat ick u helpen mach.\\n      \\n      \\n        Lijf en goet al te samen\\n        Heb ick u niet verschoont,\\n        Mijn Broeders hooch van Namen\\n        Hebbent u oock vertoont:\\n        Graef Adolff is ghebleven,\\n        In Vrieslandt in den Slach,\\n        Zijn Siel int eewich Leven\\n        Verwacht den Jongsten dach.\\n      \\n      \\n        Edel en Hooch gheboren\\n        Van Keyserlicken Stam:\\n        Een Vorst des Rijcks vercoren\\n        Als een vroom Christen Man,\\n        Voor Godes Woort ghepreesen,\\n        Heb ick vrij onversaecht,\\n        Als een Helt sonder vreesen\\n        Mijn Edel bloet ghewaecht.\\n      \\n      \\n        Mijn Schilt ende betrouwen\\n        Sijt ghy, o Godt mijn Heer,\\n        Op u soo wil ick bouwen\\n        Verlaet my nemmermeer:\\n        Dat ick doch vroom mach blijven\\n        U dienaer taller stondt\\n        Die Tyranny verdrijven,\\n        Die my mijn hert doorwondt.\\n      \\n      \\n        Van al die my beswaren,\\n        End mijn Vervolghers zijn,\\n        Mijn Godt wilt doch bewaren\\n        Den trouwen dienaer dijn:\\n        Dat sy my niet verrasschen\\n        In haren boosen moet,\\n        Haer handen niet en wasschen\\n        In mijn onschuldich bloet.\\n      \\n      \\n        Als David moeste vluchten\\n        Voor Saul den Tyran:\\n        Soo heb ick moeten suchten\\n        Met menich Edelman:\\n        Maer Godt heeft hem verheven,\\n        Verlost uut alder noot,\\n        Een Coninckrijck ghegheven\\n        In Israel seer groot.\\n      \\n      \\n        Nae tsuer sal ick ontfanghen:\\n        Van Godt mijn Heer dat soet,\\n        Daer na so doet verlanghen\\n        Mijn Vorstelick ghemoet,\\n        Dat is dat ick mach sterven\\n        Met eeren in dat Velt,\\n        Een eewich Rijck verwerven\\n        Als een ghetrouwe Helt.\\n      \\n      \\n        Niet doet my meer erbarmen\\n        In mijnen wederspoet,\\n        Dan datmen siet verarmen\\n        Des Conincks Landen goet,\\n        Dat u de Spaengiaerts crencken\\n        O Edel Neerlandt soet,\\n        Als ick daer aen ghedencke\\n        Mijn Edel hert dat bloet.\\n      \\n      \\n        Als een Prins op gheseten\\n        Met mijner Heyres cracht,\\n        Vanden Tyran vermeten\\n        Heb ick den Slach verwacht,\\n        Die by Maestricht begraven\\n        Bevreesde mijn ghewelt,\\n        Mijn Ruyters sachmen draven\\n        Seer moedich door dat Velt.\\n      \\n      \\n        Soo het den wille des Heeren\\n        Op die tijt had gheweest,\\n        Had ick gheern willen keeren\\n        Van u dit swaer tempeest:\\n        Maer de Heer van hier boven\\n        Die alle dinck regeert,\\n        Diemen altijt moet loven\\n        En heeftet niet begheert.\\n      \\n      \\n        Seer Prinslick was ghedreven\\n        Mijn Princelick ghemoet,\\n        Stantvastich is ghebleven\\n        Mijn hert in teghenspoet,\\n        Den Heer heb ick ghebeden\\n        Van mijnes herten gront,\\n        Dat hy mijn saeck wil reden,\\n        Mijn onschult doen bekant.\\n      \\n      \\n        Oorlof mijn arme Schapen\\n        Die zijt in grooten noot,\\n        U Herder sal niet slapen\\n        Al zijt ghy nu verstroyt:\\n        Tot Godt wilt u begheven,\\n        Sijn heylsaem Woort neemt aen,\\n        Als vrome Christen leven,\\n        Tsal hier haest zijn ghedaan.\\n      \\n      \\n        Voor Godt wil ick belijden\\n        End zijner grooter Macht,\\n        Dat ick tot gheenen tijden\\n        Den Coninck heb veracht:\\n        Dan dat ick Godt den Heere\\n        Der Hoochster Majesteyt,\\n        Heb moeten obedieren,\\n        Inder gherechticheyt.'\n",
    "print(wilhelminus5135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author(tree):\n",
    "    authors = tree.xpath(\".//author\")\n",
    "    if len(authors) == 1:\n",
    "        return authors[0].text\n",
    "    else:\n",
    "        print(\"Author tag not found\")\n",
    "        return None\n",
    "\n",
    "def get_text(tree):\n",
    "    text = tree.xpath(\".//text\")\n",
    "    if len(text) == 1:\n",
    "        return ''.join(text[0].itertext()).strip()\n",
    "    else:\n",
    "        print(\"Text tag not found\")\n",
    "        return None\n",
    "\n",
    "        \n",
    "txts = []\n",
    "labels = []\n",
    "for wilheminus_doc in wilhelmus_loader():\n",
    "    txts.append(get_text(wilheminus_doc))\n",
    "    labels.append(get_author(wilheminus_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_authors = [\"Johan Fruytiers\", \"D.V. Coornhert\", \"Philips van Marnix van Sint Aldegonde\", \"Pieter Datheen\"]\n",
    "\n",
    "for cauthor in chosen_authors:\n",
    "    print(cauthor, labels.count(cauthor))\n",
    "    \n",
    "#chosen_authors = set()\n",
    "#for author in labels:\n",
    "#    if labels.count(author) > 60 and author not in [\"Anonymous\", \"anoniem\", \"8904\", None] and \", \" not in author:\n",
    "#        chosen_authors.add(author)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []        \n",
    "\n",
    "for author,txt in zip(labels, txts):\n",
    "    if author in chosen_authors:\n",
    "        doc = nlp_nl(txt)\n",
    "        doc._.set(\"author\", author)\n",
    "        docs.append(doc)\n",
    "        \n",
    "print(\"number of documents, \", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textcat, train_data, test_data = create_textcat(docs, nlp_nl)\n",
    "print(\"len(train_data)\", len(train_data[1]))\n",
    "print(\"len(test_data)\", len(test_data[1]))\n",
    "train(train_data, test_data, nlp_nl, textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilhelminus5135_doc = nlp_nl(wilhelminus5135)\n",
    "prediction = textcat(wilhelminus5135_doc)\n",
    "prediction.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spacy annotations on character-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Token, Span\n",
    "\n",
    "Token.set_extension('initial', default=list(), force=True)\n",
    "Token.set_extension('final', default=list(), force=True)\n",
    "\n",
    "Token.set_extension(\n",
    "    'has_been_modified',\n",
    "    method=lambda token: np.sum(token._.initial ^ token._.final) != 0,\n",
    "    force=True\n",
    ")\n",
    "\n",
    "Span.set_extension(\n",
    "    'has_been_modified',\n",
    "    method=lambda span: any(t._.has_been_modified() for t in span),\n",
    "    force=True\n",
    ")\n",
    "\n",
    "nlp = spacy.load(\"de\", disable=['parser', 'tagger', 'ner'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "nlp.max_length = 1e10\n",
    "for bi_doc in bi_loader():\n",
    "    for body in bi_doc.findall(\".//tei:body\", namespaces=spec):\n",
    "        doc, in_init, in_final = extract_versions(body)\n",
    "        spacified = nlp(doc)\n",
    "        for token in spacified:\n",
    "            token._.initial = np.array(in_init[token.idx:token.idx+len(token)])\n",
    "            token._.final = np.array(in_final[token.idx:token.idx+len(token)])\n",
    "        docs.append(spacified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modified_tokens = []\n",
    "\n",
    "for doc in docs:\n",
    "    break;\n",
    "for token in doc:\n",
    "    if token._.has_been_modified():\n",
    "        \n",
    "        if len(token.__str__().strip()) > 0:\n",
    "            if np.sum(np.logical_and(~token._.initial, token._.final)) > 0:\n",
    "                modified_tokens.append({\n",
    "                    \"label\": \"ADD\",\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token.idx+len(token)\n",
    "                })\n",
    "            if np.sum(np.logical_and(token._.initial, ~token._.final)) > 0:\n",
    "                modified_tokens.append({\n",
    "                    \"label\": \"DEL\",\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token.idx+len(token)\n",
    "                })\n",
    "\n",
    "\n",
    "from spacy.displacy.render import EntityRenderer\n",
    "options={\n",
    "    'colors': {'ADD': '#2ca02c', 'DEL': '#d62728'},\n",
    "    'ents': ['ADD', 'DEL']\n",
    "}\n",
    "\n",
    "renderer = EntityRenderer(options=options)\n",
    "display(HTML(renderer.render_ents(doc.__str__(), modified_tokens, \"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading word vectors from fasttext\n",
    "In order to run the following cell, choose your model in the preferred language from https://fasttext.cc/docs/en/crawl-vectors.html#models and change the path accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%path_to_cc_XX_300_vec = \"/datasets/text/fasttext/cc.de.300.vec\"\n",
    "\n",
    "try:\n",
    "    #nlp = spacy.load(\"fasttext_model_de\")\n",
    "    pass\n",
    "except OSError:\n",
    "\n",
    "    from spacy.lang.de import German\n",
    "    nlp = German()\n",
    "\n",
    "    with open(path_to_cc_XX_300_vec, 'rb') as file_:\n",
    "        header = file_.readline()\n",
    "        nr_row, nr_dim = header.split()\n",
    "        nlp.vocab.reset_vectors(width=int(nr_dim))\n",
    "        for line in file_:\n",
    "            line = line.rstrip().decode('utf8')\n",
    "            pieces = line.rsplit(' ', int(nr_dim))\n",
    "            word = pieces[0]\n",
    "            vector = np.asarray([float(v) for v in pieces[1:]], dtype='f')\n",
    "            nlp.vocab.set_vector(word, vector)  # add the vectors to the vocab\n",
    "\n",
    "    nlp.to_disk('fasttext_model_de')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
