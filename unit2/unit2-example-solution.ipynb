{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "import standoffconverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DH application with spacy\n",
    "\n",
    "## Table of Contents\n",
    "1. Custom Spacy attributes\n",
    "\n",
    "    1. Document level attributes \n",
    "        Dataset: Wilhelmus\n",
    "    2. Document level attributes (optional)\n",
    "        Dataset: Deutsches Textarchiv\n",
    "    3. Span level attributes \n",
    "        Dataset: Berliner Intellektuelle\n",
    "    4. Token level attributes (optional)\n",
    "        Dataset Wilhelmus\n",
    "    \n",
    "2. Text categorization\n",
    "    1. Authorship attribution \n",
    "        Dataset: Wilhelmus    \n",
    "3. Attribute visualization with Displacy\n",
    "    1. Document sariants \n",
    "        Dataset: Berliner Intellektuelle\n",
    "\n",
    "3. Multi-lingual Word Representations (optional)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tei_dataloader import dta_loader, bi_loader, wilhelmus_loader\n",
    "from tei_dataloader import extract_text_versions_from_etree as extract_versions\n",
    "from textcat_utils import split_train_test, get_textcat, evaluate\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "spec = {\"tei\":\"http://www.tei-c.org/ns/1.0\"}\n",
    "\n",
    "nlp_de = spacy.load(\"de\", disable=['parser', 'tagger', 'ner'])\n",
    "nlp_nl = spacy.load(\"nl\", disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Spacy attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates the use of the custom attribute namespace `._.`. In DH, the corpora are often encoded in XML (TEI). When parsing a text into spacy, the meta data is not automatically preserved. The custom namespace is capable of representing any TEI meta data inside the Spacy objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A Document level attributes \n",
    "The functioning is demonstrated using the Wilhelmus data set from the DH2019 Authorship Attribution challenge\n",
    "(https://dh2019.adho.org/wilhelmus-challenge/).\n",
    "The task is to predict the Author of dutch songs. Thus, for each song, we would like to have an `author` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.tokens.Doc.set_extension('author', default=None, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the text of the XML files and the author from the training set of the corpus. In our simplified showcase, we only focus on the authors that are mentioned in the challenge description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_authors = [\n",
    "    \"Johan Fruytiers\",\n",
    "    \"D.V. Coornhert\",\n",
    "    \"Philips van Marnix van Sint Aldegonde\",\n",
    "    \"Pieter Datheen\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author(tree):\n",
    "    authors = tree.xpath(\".//author\")\n",
    "    if len(authors) == 1:\n",
    "        return authors[0].text\n",
    "    else:\n",
    "        print(\"Author tag not found\")\n",
    "        return None\n",
    "\n",
    "def get_text(tree):\n",
    "    text = tree.xpath(\".//text\")\n",
    "    if len(text) == 1:\n",
    "        return ''.join(text[0].itertext()).strip()\n",
    "    else:\n",
    "        print(\"Text tag not found\")\n",
    "        return None\n",
    "\n",
    "wilhelmus_docs = []\n",
    "        \n",
    "for wilheminus_xml_tree in wilhelmus_loader():\n",
    "    author_label = get_author(wilheminus_xml_tree)\n",
    "    if author_label in chosen_authors:\n",
    "        text = get_text(wilheminus_xml_tree)\n",
    "        doc = nlp_nl(text)\n",
    "        doc._.set(\"author\", author_label)\n",
    "        wilhelmus_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wilhelmus_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_doc = wilhelmus_docs[0]\n",
    "print(sample_doc._.author)\n",
    "print(sample_doc.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B Document level attributes (optional)\n",
    "This Deutsches Textarchiv is very similar to the Wilhelmus example, in this case we just use the `GND` attribute of the `author` tag if it is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta_docs = []\n",
    "\n",
    "for itree, dta_xml_tree in enumerate(dta_loader()):\n",
    "    if itree > 5: # we don't need so many files\n",
    "        break;\n",
    "        \n",
    "    authors = dta_xml_tree.xpath(\".//tei:author/tei:persName\", namespaces=spec)\n",
    "    \n",
    "    author_gnd = None\n",
    "    for author in authors:\n",
    "        if \"ref\" in author.attrib:\n",
    "            author_gnd = author.attrib[\"ref\"]\n",
    "            break\n",
    "    author_gnd = \"anonymous\" if author_gnd == None else author_gnd\n",
    "    \n",
    "    # retrieve plain text\n",
    "    text = []\n",
    "    for body in dta_xml_tree.findall(\".//tei:body\", namespaces=spec):\n",
    "        text.append(''.join(body.itertext()).strip())\n",
    "    \n",
    "    text = ''.join(text)\n",
    "    if len(text) <= nlp_de.max_length: \n",
    "\n",
    "        doc = nlp_de(text)\n",
    "    \n",
    "    doc._.set(\"author\", author_gnd)\n",
    "    dta_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = dta_docs[4]\n",
    "print(sample_doc._.author)\n",
    "print(sample_doc.text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.C Span level attributes \n",
    "In this example we consider the edition Berliner Intellektuelle that encompasses letters and manuscripts by a group of people that influenced the intellectual Berlin around 1800. The letters contain a lot of alterations that have been encoded in the TEI with `<add>` and `<del>` very accurately. We introduce four custom attributes into spacy tokens and spans that encode for the initial and \"final\" version of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token, Span\n",
    "\n",
    "Token.set_extension('initial', default=list(), force=True)\n",
    "Token.set_extension('final', default=list(), force=True)\n",
    "\n",
    "Token.set_extension(\n",
    "    'has_been_modified',\n",
    "    method=lambda token: np.sum(token._.initial ^ token._.final) != 0,\n",
    "    force=True\n",
    ")\n",
    "\n",
    "Span.set_extension(\n",
    "    'has_been_modified',\n",
    "    method=lambda span: any(t._.has_been_modified() for t in span),\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_docs = []\n",
    "\n",
    "for bi_doc in bi_loader():\n",
    "    for body in bi_doc.findall(\".//tei:body\", namespaces=spec):\n",
    "        doc, in_init, in_final = extract_versions(body)\n",
    "        if len(doc) <= nlp_de.max_length: \n",
    "            spacified = nlp_de(doc)\n",
    "            for token in spacified:\n",
    "                token._.initial = np.array(in_init[token.idx:token.idx+len(token)])\n",
    "                token._.final = np.array(in_final[token.idx:token.idx+len(token)])\n",
    "            bi_docs.append(spacified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in bi_docs[0]:\n",
    "    if token.is_alpha and token._.has_been_modified():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token._.has_been_modified()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token._.initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token._.final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.D Span level attributes (optional)\n",
    "In this example, we show how to incorporate the verse meter as custom information into lines of the songs of the wilhelmus challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.tokens.Span.set_extension('verse_meter', default=None, force=True)\n",
    "\n",
    "def get_author(tree):\n",
    "    authors = tree.xpath(\".//author\")\n",
    "    if len(authors) == 1:\n",
    "        return authors[0].text\n",
    "    else:\n",
    "        print(\"Author tag not found\")\n",
    "        return None\n",
    "        \n",
    "        \n",
    "lines = []\n",
    "for wilhelmus_xml_tree in wilhelmus_loader():\n",
    "    \n",
    "    author_label = get_author(wilhelmus_xml_tree)\n",
    "    \n",
    "    if author_label in chosen_authors:\n",
    "        lines.append([])\n",
    "        text = wilhelmus_xml_tree.xpath(\".//text\")[0]\n",
    "        plain, standoffs = standoffconverter.tree_to_standoff(text)\n",
    "        doc = nlp_nl(plain)\n",
    "\n",
    "        tokenidx2i = {t.idx+tchar:t.i for t in doc for tchar in range(len(t.text))}                \n",
    "        \n",
    "        for standoff in standoffs:\n",
    "            if standoff[\"tag\"] == \"l\":\n",
    "                lines[-1].append(doc[tokenidx2i[standoff[\"begin\"]]:tokenidx2i[standoff[\"end\"]]])\n",
    "                lines[-1][-1]._.verse_meter = None if \"met\" not in standoff[\"attrib\"] else standoff[\"attrib\"][\"met\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_example = lines[4]\n",
    "\n",
    "for line in lines_example[:20]:\n",
    "    print(line, line._.verse_meter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A Authorship attribution\n",
    "In this section, the `textcat` pipeline component is introduced to classify documents. We choose the above subset of the Wilhelmus data set for illustration purposes. And will classify the Wilhelmus using the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcat = get_textcat(nlp_nl)\n",
    "for author in set(a._.author for a in wilhelmus_docs):\n",
    "    textcat.add_label(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, test_data, nlp, textcat):\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Training the model...\")\n",
    "        print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "        for i in range(10):\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                           losses=losses)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                # evaluate on the dev data split off in load_data()\n",
    "                scores = evaluate(nlp.tokenizer, textcat, *test_data)\n",
    "            print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "                  .format(losses['textcat'], scores['textcat_p'],\n",
    "                          scores['textcat_r'], scores['textcat_f']))\n",
    "\n",
    "            \n",
    "X_train, X_test, y_train, y_test = split_train_test(wilhelmus_docs)\n",
    "\n",
    "train(\n",
    "    list(zip(X_train, [{'cats': cats} for cats in y_train])),\n",
    "    (X_test, y_test),\n",
    "    nlp_nl,\n",
    "    textcat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilhelminus5135 = 'Wilhelmus van Nassouwe\\n        Ben ick van Duytschen bloet,\\n        Den Vaderlant ghetrouwe\\n        Blijf ick tot inden doot:\\n        Een Prince van Oraengien\\n        Ben ick vrij onverveert,\\n        Den Coninck van Hispaengien\\n        Heb ick altijt gheeert.\\n      \\n      \\n        In Godes vrees te leven\\n        Heb ick altijt betracht,\\n        Daerom ben ick verdreven\\n        Om Landt om Luyd ghebracht:\\n        Maer Godt sal my regeren\\n        Als een goet Instrument,\\n        Dat ick sal wederkeeren\\n        In mijnen Regiment.\\n      \\n      \\n        Lijdt u mijn Ondersaten\\n        Die oprecht zijn van aert,\\n        Godt sal u niet verlaten,\\n        Al zijt ghy nu beswaert:\\n        Die vroom begheert te leven\\n        Bidt Godt nacht ende dach,\\n        Dat hy my cracht wil gheven\\n        Dat ick u helpen mach.\\n      \\n      \\n        Lijf en goet al te samen\\n        Heb ick u niet verschoont,\\n        Mijn Broeders hooch van Namen\\n        Hebbent u oock vertoont:\\n        Graef Adolff is ghebleven,\\n        In Vrieslandt in den Slach,\\n        Zijn Siel int eewich Leven\\n        Verwacht den Jongsten dach.\\n      \\n      \\n        Edel en Hooch gheboren\\n        Van Keyserlicken Stam:\\n        Een Vorst des Rijcks vercoren\\n        Als een vroom Christen Man,\\n        Voor Godes Woort ghepreesen,\\n        Heb ick vrij onversaecht,\\n        Als een Helt sonder vreesen\\n        Mijn Edel bloet ghewaecht.\\n      \\n      \\n        Mijn Schilt ende betrouwen\\n        Sijt ghy, o Godt mijn Heer,\\n        Op u soo wil ick bouwen\\n        Verlaet my nemmermeer:\\n        Dat ick doch vroom mach blijven\\n        U dienaer taller stondt\\n        Die Tyranny verdrijven,\\n        Die my mijn hert doorwondt.\\n      \\n      \\n        Van al die my beswaren,\\n        End mijn Vervolghers zijn,\\n        Mijn Godt wilt doch bewaren\\n        Den trouwen dienaer dijn:\\n        Dat sy my niet verrasschen\\n        In haren boosen moet,\\n        Haer handen niet en wasschen\\n        In mijn onschuldich bloet.\\n      \\n      \\n        Als David moeste vluchten\\n        Voor Saul den Tyran:\\n        Soo heb ick moeten suchten\\n        Met menich Edelman:\\n        Maer Godt heeft hem verheven,\\n        Verlost uut alder noot,\\n        Een Coninckrijck ghegheven\\n        In Israel seer groot.\\n      \\n      \\n        Nae tsuer sal ick ontfanghen:\\n        Van Godt mijn Heer dat soet,\\n        Daer na so doet verlanghen\\n        Mijn Vorstelick ghemoet,\\n        Dat is dat ick mach sterven\\n        Met eeren in dat Velt,\\n        Een eewich Rijck verwerven\\n        Als een ghetrouwe Helt.\\n      \\n      \\n        Niet doet my meer erbarmen\\n        In mijnen wederspoet,\\n        Dan datmen siet verarmen\\n        Des Conincks Landen goet,\\n        Dat u de Spaengiaerts crencken\\n        O Edel Neerlandt soet,\\n        Als ick daer aen ghedencke\\n        Mijn Edel hert dat bloet.\\n      \\n      \\n        Als een Prins op gheseten\\n        Met mijner Heyres cracht,\\n        Vanden Tyran vermeten\\n        Heb ick den Slach verwacht,\\n        Die by Maestricht begraven\\n        Bevreesde mijn ghewelt,\\n        Mijn Ruyters sachmen draven\\n        Seer moedich door dat Velt.\\n      \\n      \\n        Soo het den wille des Heeren\\n        Op die tijt had gheweest,\\n        Had ick gheern willen keeren\\n        Van u dit swaer tempeest:\\n        Maer de Heer van hier boven\\n        Die alle dinck regeert,\\n        Diemen altijt moet loven\\n        En heeftet niet begheert.\\n      \\n      \\n        Seer Prinslick was ghedreven\\n        Mijn Princelick ghemoet,\\n        Stantvastich is ghebleven\\n        Mijn hert in teghenspoet,\\n        Den Heer heb ick ghebeden\\n        Van mijnes herten gront,\\n        Dat hy mijn saeck wil reden,\\n        Mijn onschult doen bekant.\\n      \\n      \\n        Oorlof mijn arme Schapen\\n        Die zijt in grooten noot,\\n        U Herder sal niet slapen\\n        Al zijt ghy nu verstroyt:\\n        Tot Godt wilt u begheven,\\n        Sijn heylsaem Woort neemt aen,\\n        Als vrome Christen leven,\\n        Tsal hier haest zijn ghedaan.\\n      \\n      \\n        Voor Godt wil ick belijden\\n        End zijner grooter Macht,\\n        Dat ick tot gheenen tijden\\n        Den Coninck heb veracht:\\n        Dan dat ick Godt den Heere\\n        Der Hoochster Majesteyt,\\n        Heb moeten obedieren,\\n        Inder gherechticheyt.'\n",
    "print(wilhelminus5135[:200] , \"...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilhelminus5135_doc = nlp_nl(wilhelminus5135)\n",
    "prediction = textcat(wilhelminus5135_doc)\n",
    "for k,v in prediction.cats.items():\n",
    "    print(k,\"{:0.4f}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Visualization with Displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A Document variants\n",
    "We already extracted the document variants in 1.C., now we would like to visualize the document variants with displacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modified_tokens = []\n",
    "sample_doc = bi_docs[3]\n",
    "\n",
    "for token in sample_doc:\n",
    "    if token._.has_been_modified():\n",
    "        \n",
    "        if len(token.__str__().strip()) > 0:\n",
    "            if np.sum(np.logical_and(~token._.initial, token._.final)) > 0:\n",
    "                modified_tokens.append({\n",
    "                    \"label\": \"ADD\",\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token.idx+len(token)\n",
    "                })\n",
    "            if np.sum(np.logical_and(token._.initial, ~token._.final)) > 0:\n",
    "                modified_tokens.append({\n",
    "                    \"label\": \"DEL\",\n",
    "                    \"start\": token.idx,\n",
    "                    \"end\": token.idx+len(token)\n",
    "                })\n",
    "\n",
    "\n",
    "from spacy.displacy.render import EntityRenderer\n",
    "options={\n",
    "    'colors': {'ADD': '#2ca02c', 'DEL': '#d62728'},\n",
    "    'ents': ['ADD', 'DEL']\n",
    "}\n",
    "\n",
    "renderer = EntityRenderer(options=options)\n",
    "display(HTML(renderer.render_ents(doc.__str__(), modified_tokens, \"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-lingual word representations (optional)\n",
    "fasttext provides word vectors for 157 different languages (https://fasttext.cc/docs/en/crawl-vectors.html ). In this example, we show how to load vectors for the dutch model and store it as spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_cc_XX_300_vec = \"downloads/cc.nl.300.vec\"\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"fasttext_model_nl\")\n",
    "\n",
    "except OSError:\n",
    "\n",
    "    from spacy.lang.nl import Dutch\n",
    "    nlp = Dutch()\n",
    "\n",
    "    with open(path_to_cc_XX_300_vec, 'rb') as file_:\n",
    "        header = file_.readline()\n",
    "        nr_row, nr_dim = header.split()\n",
    "        nlp.vocab.reset_vectors(width=int(nr_dim))\n",
    "        for line in file_:\n",
    "            line = line.rstrip().decode('utf8')\n",
    "            pieces = line.rsplit(' ', int(nr_dim))\n",
    "            word = pieces[0]\n",
    "            vector = np.asarray([float(v) for v in pieces[1:]], dtype='f')\n",
    "            nlp.vocab.set_vector(word, vector)  # add the vectors to the vocab\n",
    "\n",
    "    nlp.to_disk('fasttext_model_nl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
