{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d33wubrfki0l68.cloudfront.net/d04566d0f6671ae94fdae6fa3f767f5a6553d335/c50f0/blog/img/spacy-pytorch-transformers.jpg)\n",
    "\n",
    "## Experiment: How does a model trained on Prodigy annotations compare to a plain spaCy model trained on the same seed terms?  \n",
    "\n",
    "**training data**\n",
    "- This task is simlar to creating the JSONL seed words, but in this case, we're going to create training data with a section of text and the indexes for the new entity in that text.\n",
    "- This is very simlar to the standoff text, so it's just a matter of parsing the text at the sentence level and noting where the new label appears in the text. \n",
    "```json\n",
    "[\n",
    "    (\n",
    "        \"Horses are too tall and they pretend to care about your feelings\",\n",
    "        {\"entities\": [(0, 6, LABEL)]},\n",
    "    ), ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dace3c5cec6248b0a9069ce3255cba77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D1\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D2\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D3\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D4\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D5\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D6\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D7\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D8\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D9\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D10\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D11\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D12\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D13\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D14\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D15\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D16\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D17\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D18\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D19\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D20\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D21\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D22\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D23\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D24\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D25\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D26\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D27\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D28\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D29\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D30\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D31\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D32\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D33\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D34\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D35\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D36\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D37\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D38\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D39\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D40\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D41\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D42\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D43\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D44\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D45\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D46\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D47\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D48\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D49\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D50\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D51\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D52\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D53\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D54\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D55\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D56\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D57\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D58\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D59\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D60\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D61\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D62\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D63\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D64\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D65\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D66\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D67\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D68\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D69\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D70\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D71\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D72\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D73\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D74\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D75\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D76\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D77\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D78\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D79\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D80\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D81\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D82\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D83\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D84\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D85\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D87\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D88\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D89\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D90\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D91\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D92\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D93\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D94\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D95\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D96\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D97\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D98\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D99\n",
      "http://www.perseus.tufts.edu/hopper/xmlchunk?doc=Perseus%3Atext%3A1999.03.0070%3Anarrative%3D100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import en_core_web_sm\n",
    "import json\n",
    "import random \n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "import standoffconverter\n",
    "from lxml import etree\n",
    "from spacy.tokens import Doc\n",
    "from spacy.util import minibatch, compounding\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "def get_tei_standoff(ref):\n",
    "    \n",
    "    url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + ref\n",
    "    print(url)\n",
    "\n",
    "    tei = urlopen(url).read()\n",
    "    tei = etree.XML(tei)\n",
    "    markup = standoffconverter.tree_to_standoff(tei)\n",
    "\n",
    "    return markup\n",
    "\n",
    "refs = pickle.load(open('refs.pickle', 'rb'))\n",
    "\n",
    "TRAIN_DATA = []\n",
    "labels = []\n",
    "\n",
    "for ref in tqdm(refs[:100]):\n",
    "    ents_dict = {}\n",
    "    ents_dict['entities'] = []\n",
    "    try:\n",
    "        plain, standoff = get_tei_standoff(ref)\n",
    "\n",
    "        for tag in standoff:\n",
    "            try:\n",
    "                if tag['attrib']['type'] == 'place':\n",
    "                    word_start = tag['begin']\n",
    "                    word_end = tag['end']\n",
    "                    type_ = \"GPE\"#tag['attrib']['type']\n",
    "\n",
    "                    labels.append(type_)\n",
    "\n",
    "                    ents_dict['entities'].append([word_start, word_end, type_])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        TRAIN_DATA.append((plain, ents_dict))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "labels = list(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/explosion/spaCy/issues/3558#issuecomment-487953653\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            try:\n",
    "                while valid_end > 1 and invalid_span_tokens.match(\n",
    "                        text[valid_end - 1]):\n",
    "                    valid_end -= 1\n",
    "            except IndexError:\n",
    "                pass\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data\n",
    "TRAIN_DATA = trim_entity_spans(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norway\n",
      "Realme of England\n",
      "Iles of Fynmarke\n",
      "Realme\n",
      "of Norwey\n",
      "Towne of Northberne\n",
      "Hans\n",
      "Realme of England\n",
      "Realme of\n",
      "England\n",
      "realme of England\n",
      "York\n",
      "Kingston\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents= []\n",
    "i = 0\n",
    "for x,y in TRAIN_DATA:\n",
    "    for entity in y[\"entities\"]:\n",
    "        ents.append(entity)\n",
    "        print(x[entity[0]:entity[1]])\n",
    "        i += 1\n",
    "        if i > 10:\n",
    "            break\n",
    "            \n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "labels = list(set(labels))\n",
    "labels\n",
    "len(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, validation_data = train_test_split(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.07, recall: 0.88, f1 0.12\n",
      "start training\n",
      "Losses {'ner': 2236.5042222969737}\n",
      "precision: 0.69, recall: 0.52, f1 0.60\n",
      "Losses {'ner': 967.4094032561841}\n",
      "precision: 0.74, recall: 0.69, f1 0.71\n",
      "Losses {'ner': 785.6584732642292}\n",
      "precision: 0.76, recall: 0.70, f1 0.73\n",
      "Losses {'ner': 677.1639693573344}\n",
      "precision: 0.72, recall: 0.78, f1 0.75\n",
      "Losses {'ner': 617.323058657443}\n",
      "precision: 0.76, recall: 0.82, f1 0.79\n",
      "Losses {'ner': 497.36219907258555}\n",
      "precision: 0.76, recall: 0.81, f1 0.78\n",
      "Losses {'ner': 503.8795911298912}\n",
      "precision: 0.70, recall: 0.89, f1 0.78\n",
      "Losses {'ner': 478.0086097237321}\n",
      "precision: 0.73, recall: 0.88, f1 0.80\n",
      "Losses {'ner': 357.9985902257516}\n",
      "precision: 0.71, recall: 0.91, f1 0.80\n",
      "Losses {'ner': 342.3348926019043}\n",
      "precision: 0.77, recall: 0.87, f1 0.82\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "optimizer = nlp.resume_training()\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "sizes = compounding(1.0, 4.0, 1.001)\n",
    "\n",
    "evaluate(nlp, validation_data)\n",
    "print(\"start training\")\n",
    "for itn in range(n_iter):\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    \n",
    "        batches = minibatch(train_data, size=sizes)\n",
    "\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "        print(\"Losses\", losses)\n",
    "    evaluate(nlp, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(nlp, validation_data):\n",
    "    texts, annotations = zip(*validation_data)\n",
    "\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    for idoc, doc in enumerate(nlp.pipe(texts)):\n",
    "        true_GPE_inds = np.zeros(len(doc.text)).astype(bool)\n",
    "        for ent in annotations[idoc][\"entities\"]:\n",
    "            true_GPE_inds[ent[0]:ent[1]] = True\n",
    "\n",
    "        pred_GPE_inds = np.zeros(len(doc.text)).astype(bool)\n",
    "        for ent in doc.ents:\n",
    "            pred_GPE_inds[doc[ent.start].idx:doc[ent.end].idx + len(doc[ent.end])] = True\n",
    "\n",
    "        tp += np.logical_and(true_GPE_inds, pred_GPE_inds).sum()\n",
    "        fn += np.logical_and(true_GPE_inds, ~pred_GPE_inds).sum()\n",
    "        fp += np.logical_and(~true_GPE_inds, pred_GPE_inds).sum()\n",
    "        tn += np.logical_and(~true_GPE_inds, ~pred_GPE_inds).sum()\n",
    "\n",
    "    precision = tp /(tp + fp) if fp > 0 or tp > 0 else 0\n",
    "    recall = tp /(tp + fn) if tp > 0 or fn > 0 else 0\n",
    "    f1_score = 2*tp / (2*tp + fp + fn) if tp > 0 or fp > 0 or fn > 0 else 0\n",
    "    print(\"precision: {:.2f}, recall: {:.2f}, f1 {:.2f}\".format(precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of the entities were in the training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The army marched from Konia to Kaiseria (Caesarea), and thence to Sivas, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).  \n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "places = pickle.load(open('places.pickle', 'rb'))\n",
    "\n",
    "#nlp = spacy.load(\"spacy_v_prodigy\")\n",
    "doc = nlp(\n",
    "    \"\"\"The army marched from Konia to Kaiseria (Caesarea), and thence to Sivas, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "counter = 0\n",
    "for ent in doc.ents:\n",
    "    if ent.text in places:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        counter += 1\n",
    "\n",
    "print(f\"{counter} of the entities were in the training data\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
