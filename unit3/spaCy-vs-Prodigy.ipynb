{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: How does a model trained on Prodigy annotations compare to a plain spaCy model trained on the same seed terms?  \n",
    "\n",
    "**training data**\n",
    "- This task is simlar to creating the JSONL seed words, but in this case, we're going to create training data with a section of text and the indexes for the new entity in that text.\n",
    "- This is very simlar to the standoff text, so it's just a matter of parsing the text at the sentence level and noting where the new label appears in the text. \n",
    "```json\n",
    "[\n",
    "    (\n",
    "        \"Horses are too tall and they pretend to care about your feelings\",\n",
    "        {\"entities\": [(0, 6, LABEL)]},\n",
    "    ), ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import en_core_web_sm\n",
    "import json\n",
    "import random \n",
    "import pickle\n",
    "import spacy\n",
    "import standoffconverter\n",
    "from lxml import etree\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "new_label = 'PLACE'\n",
    "\n",
    "def get_tei_standoff(ref):\n",
    "    \n",
    "    url = 'http://www.perseus.tufts.edu/hopper/xmlchunk?doc=' + ref\n",
    "    print(url)\n",
    "\n",
    "    tei = urlopen(url).read()\n",
    "    tei = etree.XML(tei)\n",
    "    markup = standoffconverter.tree_to_standoff(tei)\n",
    "\n",
    "    return markup\n",
    "\n",
    "refs = pickle.load(open('refs.pickle', 'rb'))\n",
    "\n",
    "if os.path.exists('training.jsonl'):\n",
    "    with open('training.jsonl','w') as f:\n",
    "        for ref in refs:\n",
    "            standoff = get_tei_standoff(ref)\n",
    "            text = standoff[0]\n",
    "            jsonl = []\n",
    "            for tag in standoff[1]:\n",
    "                try:               \n",
    "                    if tag['attrib']['type'] == 'place':\n",
    "                        word_start = tag['begin']\n",
    "                        word_end = tag['end']\n",
    "                        word_len = word_end - word_start \n",
    "                        #TODO use find() to get index for sentence end and beginning around ent\n",
    "                        ent_dict = {}\n",
    "                        ent_dict['entities'] = [(80, 80 + word_len, new_label)]\n",
    "                        row = (text[word_start - 80 : word_start + word_len + 80].replace('\\n',''), ent_dict)\n",
    "                        jsonl.append(row)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "         \n",
    "        try:\n",
    "            json.loads(f.read())\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        json.dump(jsonl, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* Training: https://spacy.io/usage/training\n",
    "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
    "\n",
    "Compatible with: spaCy v2.1.0+\n",
    "Last tested with: v2.1.0\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "# new entity label\n",
    "LABEL = \"PLACE\"\n",
    "\n",
    "model = 'en_core_web_sm'\n",
    "new_model_name = 'spacy_v_prodigy'\n",
    "output_dir = '/home/ajanco/spaCy_DH2019_workshop/unit3/spacy_v_prodigy'\n",
    "\n",
    "with open('training.jsonl','r') as f:\n",
    "    TRAIN_DATA = json.loads(f.read())\n",
    "    \n",
    "\n",
    "def main(model=None, new_model_name=\"animal\", output_dir=None, n_iter=30):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "    # Adding extraneous labels shouldn't mess anything up\n",
    "    ner.add_label(\"VEGETABLE\")\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"Do you like horses?\"\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n",
    "\n",
    "main(model= model, new_model_name=new_model_name, output_dir=output_dir )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "places = pickle.load(open('places.pickle', 'rb'))\n",
    "\n",
    "nlp = spacy.load(\"spacy_v_prodigy\")\n",
    "doc = nlp(\n",
    "    \"\"\"The army marched from Konia to Kaiseria (Caesarea), and thence to Sivas, where the feast of the Korbân (sacrifice) was celebrated. Here Mustafâ Pâshâ, the emperor's favourite, was promoted to the rank of second vezir, and called into the divân. The army then continued its march to Erzerum. Besides tiie guns provided by the commander-in-chief, there were forty large guns dragged by two thousand pairs of buftaloes. The army entered the castle of Kazmaghan, and halted under the walls of Eriviin in the year 1044 (1634).  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "counter = 0\n",
    "for ent in doc.ents:\n",
    "    if ent.text in places:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "        counter += 1\n",
    "\n",
    "print(f\"{counter} of the place entities were in the training data\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
